{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN4XReXo0zg1/mDfGgCgBEe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"82fcb45ba98f40e79d76a1af89ef3348":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5bbd3d060e74f9f860a8ee3258af50a","IPY_MODEL_c6c539f30aaf4fbc9ae1073772d93dba","IPY_MODEL_096d3737fccc4c97ac6fd71801afdeed"],"layout":"IPY_MODEL_4979b421f3af41ccb45f38d9dd2de200"}},"a5bbd3d060e74f9f860a8ee3258af50a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c96b1d1dcf48485995d67a93307bff45","placeholder":"​","style":"IPY_MODEL_86d8a4dd49c14a73b5e1aeb8d21f362f","value":"tokenizer_config.json: 100%"}},"c6c539f30aaf4fbc9ae1073772d93dba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5befa9f2e6014d9da31397e9b2f3d886","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f49e8836fd51483da3fd66f151339fcd","value":48}},"096d3737fccc4c97ac6fd71801afdeed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6612f665f3fd41e6bea3918bfa3d12a6","placeholder":"​","style":"IPY_MODEL_117d9e9d2b7d46c5a53988d3f1c11cc2","value":" 48.0/48.0 [00:00&lt;00:00, 3.05kB/s]"}},"4979b421f3af41ccb45f38d9dd2de200":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c96b1d1dcf48485995d67a93307bff45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86d8a4dd49c14a73b5e1aeb8d21f362f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5befa9f2e6014d9da31397e9b2f3d886":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f49e8836fd51483da3fd66f151339fcd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6612f665f3fd41e6bea3918bfa3d12a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"117d9e9d2b7d46c5a53988d3f1c11cc2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ac60e2419704eb7a178766aa5d90023":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f24f47c2c38a484bb6320f36b8271004","IPY_MODEL_6170d63268c642afa739840581fcf79a","IPY_MODEL_d98eca080a3449a0a022bcc798ad8fd1"],"layout":"IPY_MODEL_36cd82ff34df4be793f0a6e61aebc5c3"}},"f24f47c2c38a484bb6320f36b8271004":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28a374d2ada14466a619394206235790","placeholder":"​","style":"IPY_MODEL_d33f64fa643b4f878f75c5ffac8a1c16","value":"vocab.txt: 100%"}},"6170d63268c642afa739840581fcf79a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b47abfa1e8147e7a39704ed31d966ae","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3985feffd03844929e0bab9f1dab24d0","value":231508}},"d98eca080a3449a0a022bcc798ad8fd1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3c2318877ba47e781608b49370991cb","placeholder":"​","style":"IPY_MODEL_7f368ea709a840f2bd0edcf4f9186e84","value":" 232k/232k [00:00&lt;00:00, 9.57MB/s]"}},"36cd82ff34df4be793f0a6e61aebc5c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28a374d2ada14466a619394206235790":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d33f64fa643b4f878f75c5ffac8a1c16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b47abfa1e8147e7a39704ed31d966ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3985feffd03844929e0bab9f1dab24d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b3c2318877ba47e781608b49370991cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f368ea709a840f2bd0edcf4f9186e84":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94a5ca9c849b4e2fa4375e7192f7288f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_36965adf6eb246e2af08f36cdfbd0630","IPY_MODEL_d7b803ce3fb344cfb786fc7fe680bfd2","IPY_MODEL_491a31bff07d43b5b02d31ae1b9e4f9f"],"layout":"IPY_MODEL_828ee8b44c6b4f70b30c187f9f1b0d2b"}},"36965adf6eb246e2af08f36cdfbd0630":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_281993cc8e2344219d73d454f1526aef","placeholder":"​","style":"IPY_MODEL_3b73cd03c15647db991767101f6d6725","value":"tokenizer.json: 100%"}},"d7b803ce3fb344cfb786fc7fe680bfd2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8270c6926af440eadb1a9e70bccdd91","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6ac8f96925e4a3f884a0facf3faa657","value":466062}},"491a31bff07d43b5b02d31ae1b9e4f9f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e16acfef44b44536891d372adeac567b","placeholder":"​","style":"IPY_MODEL_54146046c5d84aa7a4f9c7cdd022f782","value":" 466k/466k [00:00&lt;00:00, 15.8MB/s]"}},"828ee8b44c6b4f70b30c187f9f1b0d2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"281993cc8e2344219d73d454f1526aef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b73cd03c15647db991767101f6d6725":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8270c6926af440eadb1a9e70bccdd91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6ac8f96925e4a3f884a0facf3faa657":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e16acfef44b44536891d372adeac567b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54146046c5d84aa7a4f9c7cdd022f782":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e3013e60b2249f9968afe5b143da9c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c9e5036a3c443868597b435f1c042ed","IPY_MODEL_1350938f1c554a8a99e922b4b179e7c3","IPY_MODEL_e5238b6a3f6c41fb80ec2ff0a0782a26"],"layout":"IPY_MODEL_2de51fb5d3554f8cb04457afa8067250"}},"7c9e5036a3c443868597b435f1c042ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fa323a19f28404b950b190ff7e149d2","placeholder":"​","style":"IPY_MODEL_5ee9dd9acb9346dba9884adcde28e2f2","value":"config.json: 100%"}},"1350938f1c554a8a99e922b4b179e7c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_383ff3f1e54f4894b5454045b1ea7ee8","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75ebe3a2d0fe4a5ca2588f24b314d746","value":570}},"e5238b6a3f6c41fb80ec2ff0a0782a26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a4fb535e8134d87bffe05f7d93bfc0a","placeholder":"​","style":"IPY_MODEL_d0397b5b84034c589695c0a5e5cd862b","value":" 570/570 [00:00&lt;00:00, 34.3kB/s]"}},"2de51fb5d3554f8cb04457afa8067250":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fa323a19f28404b950b190ff7e149d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ee9dd9acb9346dba9884adcde28e2f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"383ff3f1e54f4894b5454045b1ea7ee8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75ebe3a2d0fe4a5ca2588f24b314d746":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a4fb535e8134d87bffe05f7d93bfc0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0397b5b84034c589695c0a5e5cd862b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Transformer Architecture From Scratch."],"metadata":{"id":"tMEOAHwVOArM"}},{"cell_type":"markdown","source":["Tokenization\n","\n","1.Word-based Tokenization\n","\n","2.Subword-based Tokenization    \n","3.Character-based Tokenization\n","\n","\n"],"metadata":{"id":"nsUCZzJUOLJm"}},{"cell_type":"markdown","source":["**BPE - Byte -Pair Encoding**"],"metadata":{"id":"8mGz075njtvW"}},{"cell_type":"markdown","source":["*BPE is a subword-based tokenization technique that involves iteratively merging the most frequent pair of tokens (byte pairs) in the corpus until a predefined vocabulary size is reached.\n","example :\n"," Example of BPE:\n"," Initial vocabulary: ['l', 'o', 'w', 'e', 'r', 'n', 'w', 's', 't']\n"," Corpus: \"low lower lowest newest widest\"\n","\n"," Iteration 1: Merge most frequent pair 'e' + 'r' -> 'er'\n"," Updated vocab: ['l', 'o', 'w', 'e', 'r', 'n', 'w', 's', 't', 'er']\n","\n"," Iteration 2: Merge most frequent pair 'l' + 'o' -> 'lo'\n"," Updated vocab: ['l', 'o', 'w', 'e', 'r', 'n', 'w', 's', 't', 'er', 'lo']\n","\n"," Iteration 3: Merge most frequent pair 'lo' + 'w' -> 'low'\n"," Updated vocab: ['l', 'o', 'w', 'e', 'r', 'n', 'w', 's', 't', 'er', 'lo', 'low']\n","\n"," Final tokenization:\n"," \"low\" -> ['low']\n"," \"lower\" -> ['low', 'er']\n"," \"lowest\" -> ['low', 'est']\n"," \"newest\" -> ['n', 'ew', 'est']\n"," \"widest\" -> ['w', 'id', 'est']"],"metadata":{"id":"RJkDq8d4N_G0"}},{"cell_type":"code","source":["!pip install torchtext==0.13.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sz5Zp2n3g3a9","executionInfo":{"status":"ok","timestamp":1725389520921,"user_tz":-330,"elapsed":94967,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"15605ff4-494d-4639-9e94-4daebcad7fd8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchtext==0.13.1\n","  Downloading torchtext-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.9 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.13.1) (4.66.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.13.1) (2.32.3)\n","Collecting torch==1.12.1 (from torchtext==0.13.1)\n","  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.13.1) (1.26.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1->torchtext==0.13.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.13.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.13.1) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.13.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.13.1) (2024.7.4)\n","Downloading torchtext-0.13.1-cp310-cp310-manylinux1_x86_64.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch, torchtext\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.4.0+cu121\n","    Uninstalling torch-2.4.0+cu121:\n","      Successfully uninstalled torch-2.4.0+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.4.0+cu121 requires torch==2.4.0, but you have torch 1.12.1 which is incompatible.\n","torchvision 0.19.0+cu121 requires torch==2.4.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.12.1 torchtext-0.13.1\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"GL09hQLRgjkk","executionInfo":{"status":"ok","timestamp":1725389521522,"user_tz":-330,"elapsed":607,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}}},"outputs":[],"source":["import torch\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","# Example sentence\n","sentence = \"I love coding with PyTorch\"\n"]},{"cell_type":"code","source":["\n","# Tokenizer (word-based)\n","tokenizer = get_tokenizer(\"basic_english\")\n","\n","# Tokenize the sentence\n","tokens = tokenizer(sentence)\n","print(\"Tokens:\", tokens)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AC2-8QZ9gkck","executionInfo":{"status":"ok","timestamp":1725389679674,"user_tz":-330,"elapsed":632,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"6a937da2-9f43-45fb-9129-739135614dfe"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens: ['i', 'love', 'coding', 'with', 'pytorch']\n"]}]},{"cell_type":"code","source":["# Build Vocabulary using BPE (simulate BPE with torchtext)\n","def yield_tokens(data_iter):\n","    for text in data_iter:\n","        yield tokenizer(text)\n","\n","\n","# Vocabulary object, with a max vocab size for BPE-like behavior\n","vocab = build_vocab_from_iterator(yield_tokens([sentence]), specials=[\"<unk>\"], max_tokens=100)\n","vocab.set_default_index(vocab[\"<unk>\"])\n","\n","vocab\n"],"metadata":{"id":"U6rQFttzhAwe","executionInfo":{"status":"ok","timestamp":1725389684552,"user_tz":-330,"elapsed":1411,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"915edb76-7f59-4fde-a88c-221344ad8cd2"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Vocab()"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# Encode sentence into token IDs\n","token_ids = vocab(tokens)\n","print(\"Token IDs:\", token_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_8ZWrUAhDcB","executionInfo":{"status":"ok","timestamp":1725389753134,"user_tz":-330,"elapsed":4,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"6c547ab2-e5b1-44f7-9ef0-c2410b9ae299"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab()\n","Token IDs: [2, 3, 1, 5, 4]\n"]}]},{"cell_type":"markdown","source":["WordPice Tokenizer\n","**bold text**"],"metadata":{"id":"OycIVQl8jnOB"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n"],"metadata":{"id":"P5QGjQW-jr0i","executionInfo":{"status":"ok","timestamp":1725389877265,"user_tz":-330,"elapsed":1883,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":274,"referenced_widgets":["82fcb45ba98f40e79d76a1af89ef3348","a5bbd3d060e74f9f860a8ee3258af50a","c6c539f30aaf4fbc9ae1073772d93dba","096d3737fccc4c97ac6fd71801afdeed","4979b421f3af41ccb45f38d9dd2de200","c96b1d1dcf48485995d67a93307bff45","86d8a4dd49c14a73b5e1aeb8d21f362f","5befa9f2e6014d9da31397e9b2f3d886","f49e8836fd51483da3fd66f151339fcd","6612f665f3fd41e6bea3918bfa3d12a6","117d9e9d2b7d46c5a53988d3f1c11cc2","4ac60e2419704eb7a178766aa5d90023","f24f47c2c38a484bb6320f36b8271004","6170d63268c642afa739840581fcf79a","d98eca080a3449a0a022bcc798ad8fd1","36cd82ff34df4be793f0a6e61aebc5c3","28a374d2ada14466a619394206235790","d33f64fa643b4f878f75c5ffac8a1c16","3b47abfa1e8147e7a39704ed31d966ae","3985feffd03844929e0bab9f1dab24d0","b3c2318877ba47e781608b49370991cb","7f368ea709a840f2bd0edcf4f9186e84","94a5ca9c849b4e2fa4375e7192f7288f","36965adf6eb246e2af08f36cdfbd0630","d7b803ce3fb344cfb786fc7fe680bfd2","491a31bff07d43b5b02d31ae1b9e4f9f","828ee8b44c6b4f70b30c187f9f1b0d2b","281993cc8e2344219d73d454f1526aef","3b73cd03c15647db991767101f6d6725","f8270c6926af440eadb1a9e70bccdd91","a6ac8f96925e4a3f884a0facf3faa657","e16acfef44b44536891d372adeac567b","54146046c5d84aa7a4f9c7cdd022f782","3e3013e60b2249f9968afe5b143da9c1","7c9e5036a3c443868597b435f1c042ed","1350938f1c554a8a99e922b4b179e7c3","e5238b6a3f6c41fb80ec2ff0a0782a26","2de51fb5d3554f8cb04457afa8067250","8fa323a19f28404b950b190ff7e149d2","5ee9dd9acb9346dba9884adcde28e2f2","383ff3f1e54f4894b5454045b1ea7ee8","75ebe3a2d0fe4a5ca2588f24b314d746","5a4fb535e8134d87bffe05f7d93bfc0a","d0397b5b84034c589695c0a5e5cd862b"]},"id":"Gu9uvTFyjsZq","executionInfo":{"status":"ok","timestamp":1725389888311,"user_tz":-330,"elapsed":6319,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"6852e23b-a874-4af2-b5f3-f2fccb02f824"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82fcb45ba98f40e79d76a1af89ef3348"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ac60e2419704eb7a178766aa5d90023"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94a5ca9c849b4e2fa4375e7192f7288f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e3013e60b2249f9968afe5b143da9c1"}},"metadata":{}}]},{"cell_type":"code","source":["sentence = \"I love coding with PyTorch\"\n","\n","# Tokenize and encode the sentence\n","tokens = tokenizer.tokenize(sentence)\n","token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","print(\"Tokens:\", tokens)\n","print(\"Token IDs:\", token_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p2hHNeIjj1vJ","executionInfo":{"status":"ok","timestamp":1725389889787,"user_tz":-330,"elapsed":3,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"31372df5-8218-4e57-ee26-9e1b0a23d709"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens: ['i', 'love', 'coding', 'with', 'p', '##yt', '##or', '##ch']\n","Token IDs: [1045, 2293, 16861, 2007, 1052, 22123, 2953, 2818]\n"]}]},{"cell_type":"markdown","source":["Let's See Byte Pair Encoding (BPE) from the tiktoken library introduced by OpenAI"],"metadata":{"id":"MC-HJW-OQX1U"}},{"cell_type":"markdown","source":["This technique can improve the performance of LLMs and handle rare and out-of-vocabulary words. The big difference between TikToken BPE and sentencepiece BPE is that TikToken BPE doesn't always split words into smaller parts if the whole word is already known.\n","\n","For example, if \"hugging\" is in the vocabulary, it stays as one token instead of splitting into [\"hug\",\"ging\"]."],"metadata":{"id":"nFK4Y3BNSR5w"}},{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"id":"ASkdfFkKj9lM","executionInfo":{"status":"ok","timestamp":1725390044912,"user_tz":-330,"elapsed":4926,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"931c6ffc-59b5-43c2-ce29-f0acf6e843ff"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n","Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.7.0\n"]}]},{"cell_type":"code","source":["import tiktoken\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"khmoY9EBQsmO","executionInfo":{"status":"ok","timestamp":1725390664523,"user_tz":-330,"elapsed":2,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"c76a1ccd-49fa-41f3-bfc2-299e79c3e114"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50256"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["tokens = tokenizer.encode(sentence)\n","print(\"BPE Tokenization:\", tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZMi8jHE5QsYM","executionInfo":{"status":"ok","timestamp":1725390347879,"user_tz":-330,"elapsed":783,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"26a82b1e-2054-47e5-9d05-dd9b2b0a0076"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["BPE Tokenization: [40, 1842, 19617, 351, 9485, 15884, 354]\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":146},"id":"EMcRsk3fQsId","executionInfo":{"status":"error","timestamp":1725390626785,"user_tz":-330,"elapsed":718,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"97c3dc92-f750-496b-e5e0-2bc094e28c98"},"execution_count":18,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"object of type 'Encoding' has no len()","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-eed12912f10a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: object of type 'Encoding' has no len()"]}]},{"cell_type":"markdown","source":["# Create Embeddings of tokenized words."],"metadata":{"id":"24h0WmoZqQhT"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Assume a vocab size of 30,000 and embedding dimension of 512\n","vocab_size = tokenizer.max_token_value\n","embedding_dim = 512  # Given in research Paper\n","\n","# Create an embedding layer\n","embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n","\n","# Example token IDs from a tokenized sentence\n","# token_ids = torch.tensor([101, 2204, 4539, 1022, 2007])\n","\n","# Get the embeddings for the token IDs\n","embeddings = embedding_layer( torch.tensor(token_ids))\n","\n","print(\"Embeddings shape:\", embeddings.shape)  # Should be (5, 512) for 5 tokens\n","print(\"Embeddings:\", embeddings)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JtZbyXbgqQPT","executionInfo":{"status":"ok","timestamp":1725390708222,"user_tz":-330,"elapsed":590,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"8ae3e73e-7daa-4557-de1c-a8cb492ba3d0"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Embeddings shape: torch.Size([8, 512])\n","Embeddings: tensor([[-1.5947,  0.3241, -0.4799,  ..., -0.1052, -0.4698, -0.2768],\n","        [ 0.4734,  1.6934, -0.8733,  ...,  1.0033,  1.6622, -0.6976],\n","        [-0.3605, -0.1322, -0.6009,  ..., -0.6075, -0.0437, -0.5079],\n","        ...,\n","        [ 0.3498, -1.2330, -0.3858,  ...,  1.5729,  0.6963, -1.3328],\n","        [-0.4225, -0.2490, -0.9021,  ..., -0.3376, -0.7263,  1.8447],\n","        [-0.8291, -1.1182,  0.6144,  ...,  0.5412,  0.0358,  1.2546]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5ikF5pCnqaJT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenization to Embedding full steps"],"metadata":{"id":"Rt3GI5B8q4Yo"}},{"cell_type":"code","source":["data = [\n","    \"I love machine learning.\",\n","    \"Artificial intelligence is the future.\",\n","    \"Transformers are powerful models.\",\n","    \"Natural language processing is fascinating.\",\n","    \"Deep learning enables great advances in AI.\",\n","    \"PyTorch is a popular deep learning library.\",\n","    \"GPT models are widely used in NLP.\",\n","    \"Neural networks are the backbone of deep learning.\",\n","    \"Embedding layers convert tokens to vectors.\",\n","    \"Attention mechanisms help models focus on important parts of the input.\"\n","]\n"],"metadata":{"id":"f5JfrVxkq8ji","executionInfo":{"status":"ok","timestamp":1725390942270,"user_tz":-330,"elapsed":592,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["import string\n","\n","## Creating our own vocab\n","\n","def simple_tokenize(sentence):\n","    # Remove punctuation and lowercase the sentence\n","    sentence = sentence.translate(str.maketrans('', '', string.punctuation)).lower()\n","    print(sentence)\n","    # Split the sentence into tokens\n","    return sentence.split()\n","\n","tokenized_data = [simple_tokenize(sentence) for sentence in data]\n","\n","# Let's print the tokenized sentences\n","for i, tokens in enumerate(tokenized_data):\n","    print(f\"Sentence {i+1}: {tokens}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JrwpbUkVpRex","executionInfo":{"status":"ok","timestamp":1725390944280,"user_tz":-330,"elapsed":2,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"a4704d79-a700-4be9-ffab-cfc25ad95174"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["i love machine learning\n","artificial intelligence is the future\n","transformers are powerful models\n","natural language processing is fascinating\n","deep learning enables great advances in ai\n","pytorch is a popular deep learning library\n","gpt models are widely used in nlp\n","neural networks are the backbone of deep learning\n","embedding layers convert tokens to vectors\n","attention mechanisms help models focus on important parts of the input\n","Sentence 1: ['i', 'love', 'machine', 'learning']\n","Sentence 2: ['artificial', 'intelligence', 'is', 'the', 'future']\n","Sentence 3: ['transformers', 'are', 'powerful', 'models']\n","Sentence 4: ['natural', 'language', 'processing', 'is', 'fascinating']\n","Sentence 5: ['deep', 'learning', 'enables', 'great', 'advances', 'in', 'ai']\n","Sentence 6: ['pytorch', 'is', 'a', 'popular', 'deep', 'learning', 'library']\n","Sentence 7: ['gpt', 'models', 'are', 'widely', 'used', 'in', 'nlp']\n","Sentence 8: ['neural', 'networks', 'are', 'the', 'backbone', 'of', 'deep', 'learning']\n","Sentence 9: ['embedding', 'layers', 'convert', 'tokens', 'to', 'vectors']\n","Sentence 10: ['attention', 'mechanisms', 'help', 'models', 'focus', 'on', 'important', 'parts', 'of', 'the', 'input']\n"]}]},{"cell_type":"code","source":["from collections import defaultdict\n","\n","vocab = defaultdict(lambda: len(vocab))\n","\n","# Add a special token for unknown words\n","UNK = vocab[\"<UNK>\"]\n","PAD = vocab[\"<PAD>\"]  # Padding token\n","\n","# we add padding for those sentence having less words in sentence. It helps to maintain sentence length equal.\n","\n","\n","# Populate the vocabulary with the tokenized data\n","for tokens in tokenized_data:\n","    for token in tokens:\n","        _ = vocab[token]\n","\n","# Convert defaultdict to a regular dictionary\n","vocab = dict(vocab)\n","\n","# Print the vocabulary\n","print(\"Vocabulary:\", vocab)\n","print(\"Vocabulary Size:\", len(vocab))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OeY_xEjpSDn","executionInfo":{"status":"ok","timestamp":1725391854897,"user_tz":-330,"elapsed":642,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"82cab2d6-b375-4ba2-d9fc-7e85f4cf6389"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: {'<UNK>': 0, '<PAD>': 1, 'i': 2, 'love': 3, 'machine': 4, 'learning': 5, 'artificial': 6, 'intelligence': 7, 'is': 8, 'the': 9, 'future': 10, 'transformers': 11, 'are': 12, 'powerful': 13, 'models': 14, 'natural': 15, 'language': 16, 'processing': 17, 'fascinating': 18, 'deep': 19, 'enables': 20, 'great': 21, 'advances': 22, 'in': 23, 'ai': 24, 'pytorch': 25, 'a': 26, 'popular': 27, 'library': 28, 'gpt': 29, 'widely': 30, 'used': 31, 'nlp': 32, 'neural': 33, 'networks': 34, 'backbone': 35, 'of': 36, 'embedding': 37, 'layers': 38, 'convert': 39, 'tokens': 40, 'to': 41, 'vectors': 42, 'attention': 43, 'mechanisms': 44, 'help': 45, 'focus': 46, 'on': 47, 'important': 48, 'parts': 49, 'input': 50}\n","Vocabulary Size: 51\n"]}]},{"cell_type":"code","source":["def tokens_to_ids(tokens, vocab):\n","    return [vocab.get(token, UNK) for token in tokens]\n","max_len = max(len(tokens) for tokens in tokenized_data)\n","\n","token_ids_data = [tokens_to_ids(tokens, vocab) for tokens in tokenized_data]\n","padded_token_ids_data = [token_ids + [PAD] * (max_len - len(token_ids)) for token_ids in token_ids_data]\n","\n","\n","# Print the padded token IDs\n","for i, token_ids in enumerate(padded_token_ids_data):\n","    print(f\"Padded Token IDs for Sentence {i+1}: {token_ids}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXAVtyAeqeof","executionInfo":{"status":"ok","timestamp":1725391950257,"user_tz":-330,"elapsed":714,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"129319c5-a509-4d65-ca11-8ab75f78f483"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Padded Token IDs for Sentence 1: [2, 3, 4, 5, 1, 1, 1, 1, 1, 1, 1]\n","Padded Token IDs for Sentence 2: [6, 7, 8, 9, 10, 1, 1, 1, 1, 1, 1]\n","Padded Token IDs for Sentence 3: [11, 12, 13, 14, 1, 1, 1, 1, 1, 1, 1]\n","Padded Token IDs for Sentence 4: [15, 16, 17, 8, 18, 1, 1, 1, 1, 1, 1]\n","Padded Token IDs for Sentence 5: [19, 5, 20, 21, 22, 23, 24, 1, 1, 1, 1]\n","Padded Token IDs for Sentence 6: [25, 8, 26, 27, 19, 5, 28, 1, 1, 1, 1]\n","Padded Token IDs for Sentence 7: [29, 14, 12, 30, 31, 23, 32, 1, 1, 1, 1]\n","Padded Token IDs for Sentence 8: [33, 34, 12, 9, 35, 36, 19, 5, 1, 1, 1]\n","Padded Token IDs for Sentence 9: [37, 38, 39, 40, 41, 42, 1, 1, 1, 1, 1]\n","Padded Token IDs for Sentence 10: [43, 44, 45, 14, 46, 47, 48, 49, 36, 9, 50]\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Step 5: Create the Embedding Layer\n","vocab_size = len(vocab)\n","embedding_dim = 8\n","\n","embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n","\n","# Step 6: Convert Padded Token IDs to Embeddings\n","token_ids_tensor = [torch.tensor(token_ids) for token_ids in padded_token_ids_data]\n","\n","# Get embeddings for each sentence\n","embeddings = [embedding_layer(token_ids) for token_ids in token_ids_tensor]\n","\n","# Print the embeddings\n","for i, embedding in enumerate(embeddings):\n","    print(f\"Embeddings for Padded Sentence {i+1}:\")\n","    print(embedding)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"ahcWZgsDqfMA","executionInfo":{"status":"ok","timestamp":1725391954817,"user_tz":-330,"elapsed":633,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"f905ce17-56ce-445f-cba9-654aa2d3b2c8"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Embeddings for Padded Sentence 1:\n","tensor([[-0.9132,  0.9468,  0.0817,  0.2827,  1.0028, -1.4293, -0.6571, -0.9337],\n","        [-0.4450,  0.5968, -1.0023,  0.3321, -1.6782,  0.4033, -0.6668, -0.4853],\n","        [ 0.4007, -0.2098,  0.6176,  2.8208,  1.4950,  1.0831,  0.1060, -0.1578],\n","        [-0.6790,  1.0265, -0.1804, -0.3918, -0.4139, -0.0575,  1.2878, -0.1764],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879]],\n","       grad_fn=<EmbeddingBackward0>)\n","Embeddings for Padded Sentence 2:\n","tensor([[-0.5134,  0.3959, -0.1563, -0.8650, -1.1609,  1.2289,  0.5887,  1.7408],\n","        [ 0.3049, -1.7722,  0.9349, -1.3461, -0.7457,  0.8599, -0.9841, -0.9534],\n","        [ 0.1135,  1.5228, -0.1594, -1.2954, -1.0477, -0.9460,  1.2700,  0.4391],\n","        [ 0.6435,  0.2659,  0.0102, -1.0106, -0.5039,  0.0501,  1.2744,  0.1937],\n","        [-0.4219,  0.7941,  1.2911, -2.1278,  1.2973,  0.2272, -0.8404, -0.4347],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879]],\n","       grad_fn=<EmbeddingBackward0>)\n","Embeddings for Padded Sentence 3:\n","tensor([[-0.8901,  1.6996,  0.2268,  0.5904,  0.9306, -0.9543, -1.0933,  1.5157],\n","        [ 0.3102, -0.5014, -2.4316, -0.4517,  0.8536,  0.8375, -0.6824,  0.1370],\n","        [ 1.4109, -0.5536, -1.3810, -0.0858, -0.9979, -0.6545,  0.6780,  0.2705],\n","        [ 0.3422,  0.0507, -1.7521,  0.1651, -0.4658, -0.4958,  1.4751, -1.0444],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879]],\n","       grad_fn=<EmbeddingBackward0>)\n","Embeddings for Padded Sentence 4:\n","tensor([[ 1.4268,  1.1752,  0.2261, -1.1970, -1.8897,  0.1442, -0.2165, -1.0794],\n","        [-0.0791,  0.1310, -0.9323, -0.7215, -1.4442, -0.6779,  0.2261,  0.8565],\n","        [ 1.3955,  0.6176, -0.5243,  0.1553,  1.9868,  0.8673, -0.1096,  1.5221],\n","        [ 0.1135,  1.5228, -0.1594, -1.2954, -1.0477, -0.9460,  1.2700,  0.4391],\n","        [-0.5273,  0.3764,  0.3961,  1.5406,  1.7153,  1.5590,  0.5547, -0.5016],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879]],\n","       grad_fn=<EmbeddingBackward0>)\n","Embeddings for Padded Sentence 5:\n","tensor([[ 1.0231, -1.2435,  1.0070, -0.9727,  0.3895, -0.2137,  0.2544, -1.2021],\n","        [-0.6790,  1.0265, -0.1804, -0.3918, -0.4139, -0.0575,  1.2878, -0.1764],\n","        [-0.3583,  0.2709,  1.1814, -0.6721, -0.6396,  0.9068, -1.3723, -0.0618],\n","        [ 0.3352, -1.4421,  2.4694, -0.6889,  0.5183, -0.5018, -0.8853,  0.7303],\n","        [-2.1690,  0.0700,  1.9524, -1.0835,  0.6276, -0.3597, -2.1367, -4.0439],\n","        [-0.9911, -0.4117,  0.1673, -1.0062, -1.2862, -0.6104, -0.5148,  1.1197],\n","        [-0.7731, -0.1521, -0.1822,  0.4301,  2.9890, -0.1645,  1.4541, -0.8036],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879]],\n","       grad_fn=<EmbeddingBackward0>)\n","Embeddings for Padded Sentence 6:\n","tensor([[-3.2585e-01, -1.6574e+00,  1.3698e+00,  3.1396e-01, -1.0688e+00,\n","          9.9432e-01, -5.8025e-01, -1.1689e+00],\n","        [ 1.1355e-01,  1.5228e+00, -1.5936e-01, -1.2954e+00, -1.0477e+00,\n","         -9.4599e-01,  1.2700e+00,  4.3908e-01],\n","        [ 2.8862e-01, -1.5515e-02, -1.8470e-01,  7.8839e-01, -1.3023e+00,\n","         -4.5081e-01,  9.1181e-04,  3.7666e-01],\n","        [ 2.7672e+00, -5.7447e-01,  8.7687e-01, -1.4977e+00, -1.9669e-01,\n","         -1.0939e-01, -1.3052e+00, -2.8419e-01],\n","        [ 1.0231e+00, -1.2435e+00,  1.0070e+00, -9.7267e-01,  3.8947e-01,\n","         -2.1370e-01,  2.5444e-01, -1.2021e+00],\n","        [-6.7898e-01,  1.0265e+00, -1.8043e-01, -3.9183e-01, -4.1393e-01,\n","         -5.7471e-02,  1.2878e+00, -1.7642e-01],\n","        [ 1.6456e-01,  1.7422e+00, -1.2827e+00,  2.9872e-01, -3.4968e-01,\n","          2.7488e-01, -1.1508e+00, -9.9599e-01],\n","        [ 6.3545e-01,  4.7427e-01,  2.0477e-02, -1.0658e+00, -1.4321e+00,\n","          6.4144e-01,  3.6416e-01, -2.0879e+00],\n","        [ 6.3545e-01,  4.7427e-01,  2.0477e-02, -1.0658e+00, -1.4321e+00,\n","          6.4144e-01,  3.6416e-01, -2.0879e+00],\n","        [ 6.3545e-01,  4.7427e-01,  2.0477e-02, -1.0658e+00, -1.4321e+00,\n","          6.4144e-01,  3.6416e-01, -2.0879e+00],\n","        [ 6.3545e-01,  4.7427e-01,  2.0477e-02, -1.0658e+00, -1.4321e+00,\n","          6.4144e-01,  3.6416e-01, -2.0879e+00]], grad_fn=<EmbeddingBackward0>)\n","Embeddings for Padded Sentence 7:\n","tensor([[ 1.7465,  0.0830,  0.4236, -0.4829, -0.8554,  0.8291, -0.4741,  0.9861],\n","        [ 0.3422,  0.0507, -1.7521,  0.1651, -0.4658, -0.4958,  1.4751, -1.0444],\n","        [ 0.3102, -0.5014, -2.4316, -0.4517,  0.8536,  0.8375, -0.6824,  0.1370],\n","        [ 1.8093, -0.0363,  0.7542,  1.3956,  0.8530, -1.3427,  0.5699,  1.0129],\n","        [ 0.3099,  1.6605,  0.3242,  1.7678, -0.1368,  1.1510, -1.0316,  0.0117],\n","        [-0.9911, -0.4117,  0.1673, -1.0062, -1.2862, -0.6104, -0.5148,  1.1197],\n","        [-1.2465, -0.2687, -1.3970,  0.2173,  0.6290,  0.1946, -1.0481,  1.1870],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879]],\n","       grad_fn=<EmbeddingBackward0>)\n","Embeddings for Padded Sentence 8:\n","tensor([[-0.2906, -0.6029,  0.1191, -1.9366,  0.0723, -1.1080,  1.7659, -0.0499],\n","        [ 0.7745,  1.4340, -3.0929, -1.3111,  2.0810,  0.0488,  0.4233,  0.6552],\n","        [ 0.3102, -0.5014, -2.4316, -0.4517,  0.8536,  0.8375, -0.6824,  0.1370],\n","        [ 0.6435,  0.2659,  0.0102, -1.0106, -0.5039,  0.0501,  1.2744,  0.1937],\n","        [-2.5832,  1.3147, -0.2066, -0.8010,  0.0729, -0.3881,  0.5009,  0.7990],\n","        [-0.9428, -0.1502,  0.6136, -0.7792, -0.9481, -0.0849, -1.6487, -0.1480],\n","        [ 1.0231, -1.2435,  1.0070, -0.9727,  0.3895, -0.2137,  0.2544, -1.2021],\n","        [-0.6790,  1.0265, -0.1804, -0.3918, -0.4139, -0.0575,  1.2878, -0.1764],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879]],\n","       grad_fn=<EmbeddingBackward0>)\n","Embeddings for Padded Sentence 9:\n","tensor([[ 0.5177,  0.8231,  0.0279, -0.4265, -0.3884, -0.8163,  0.4210, -0.2357],\n","        [ 0.9931,  0.8383,  1.9297,  0.9387, -0.9742,  0.5871, -0.7905,  0.4080],\n","        [-0.3133, -1.0105, -0.0955,  0.3562,  0.8722, -0.7201, -0.6132,  1.5995],\n","        [ 0.0065,  0.5967,  1.9415, -0.7137,  1.1862,  2.2979, -1.2844, -0.2289],\n","        [-1.1608, -0.8274, -1.3177, -1.9236, -2.3082, -1.1035, -1.0209, -0.8195],\n","        [ 0.3162, -0.2223, -0.4104,  0.7420, -0.0708, -1.4515,  0.9566,  0.0873],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879],\n","        [ 0.6355,  0.4743,  0.0205, -1.0658, -1.4321,  0.6414,  0.3642, -2.0879]],\n","       grad_fn=<EmbeddingBackward0>)\n","Embeddings for Padded Sentence 10:\n","tensor([[ 0.3526,  1.0236, -0.3651, -0.1868, -0.0976, -0.9227, -0.9584,  0.8113],\n","        [ 1.2606, -1.6327,  0.8794, -1.7299,  0.5246,  0.2167,  1.3110,  0.2893],\n","        [-0.3064,  1.7574, -0.4771, -0.0840, -0.2007, -0.8721, -2.1516,  1.0290],\n","        [ 0.3422,  0.0507, -1.7521,  0.1651, -0.4658, -0.4958,  1.4751, -1.0444],\n","        [ 0.0894,  0.7863, -2.0931,  0.4861,  0.3149,  0.9341,  0.3664,  0.4853],\n","        [ 1.9944, -0.3371, -0.2575, -0.5166,  0.4149, -0.8662, -0.7002, -0.3711],\n","        [ 0.0365, -0.8615,  0.1815, -0.2293, -1.9979, -0.1396, -0.0519,  0.2771],\n","        [ 0.7118, -0.3672, -1.4103,  0.8167, -0.5153, -1.2698, -0.3469,  0.5712],\n","        [-0.9428, -0.1502,  0.6136, -0.7792, -0.9481, -0.0849, -1.6487, -0.1480],\n","        [ 0.6435,  0.2659,  0.0102, -1.0106, -0.5039,  0.0501,  1.2744,  0.1937],\n","        [ 1.1689, -0.7125, -0.2841,  0.5908,  1.8445,  0.3182, -1.3264, -0.1604]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}]},{"cell_type":"markdown","source":["# ***Positional Encoding***"],"metadata":{"id":"odNTOUY35Rb7"}},{"cell_type":"markdown","source":["Positional Encoding (PE) in Transformers\n","\n","We use positional encoding to provide information about the relative or absolute position of tokens in a sequence.\n","This is necessary because the self-attention mechanism in transformers is permutation-invariant.\n","\n","Types of Positional Encoding:\n","\n","1. Sinusoidal PE (used in original Transformer):\n","   - Uses sine and cosine functions to encode positions\n","   - Allows model to extrapolate to longer sequences\n","\n","2. Learned PE (used in BERT and many other models):\n","   - Trainable embedding for each position\n","   - Can potentially capture more complex positional relationships\n","\n","3. Rotary Position Embedding (RoPE) (used in LLaMA and Mixtral):\n","   - Applies rotation to key and query vectors in attention mechanism\n","   - Enables better relative positional understanding\n","\n","4. ALiBi (Attention with Linear Biases) (used in some recent models):\n","   - Adds a bias term to attention scores based on relative positions\n","   - Allows for better extrapolation to longer sequences\n","\n","Note: LLaMA and Mixtral specifically use RoPE, which has shown good performance in long-range dependencies."],"metadata":{"id":"BVOBAhbXYB_N"}},{"cell_type":"markdown","source":["# **Let's dive deeply into Positional encoding using Sin-cosine funtions**\n"],"metadata":{"id":"qlgJFzCG5ewm"}},{"cell_type":"code","source":["import numpy as np\n","\n","def get_positional_encoding(seq_len, d_model):\n","    \"\"\"\n","    Generate a sinusoidal positional encoding matrix.\n","\n","    :param seq_len: Length of the sequence.\n","    :param d_model: Dimension of the model (same as the embedding dimension).\n","    :return: A tensor of shape (seq_len, d_model) containing positional encodings.\n","    \"\"\"\n","    # Initialize the positional encoding matrix\n","    pos_enc = np.zeros((seq_len, d_model))\n","\n","    # Create a matrix of positions (i.e., 0, 1, 2, ..., seq_len-1)\n","    positions = np.arange(seq_len).reshape(-1, 1)\n","\n","    # Define the denominator for the sine and cosine functions\n","    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n","\n","    # Calculate the positional encodings using sine and cosine functions\n","    pos_enc[:, 0::2] = np.sin(positions * div_term)\n","    pos_enc[:, 1::2] = np.cos(positions * div_term)\n","\n","    # Convert the positional encoding matrix to a tensor\n","    return torch.tensor(pos_enc, dtype=torch.float32)"],"metadata":{"id":"dHMVBoE01Sap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assume max_len is the length of the padded sequences\n","max_len = len(token_ids_tensor[0])  # This will be the same for all sentences after padding\n","d_model = embedding_dim  # The dimensionality of the embeddings\n","\n","# Get positional encodings\n","positional_encoding = get_positional_encoding(max_len, d_model)\n","\n","print(positional_encoding)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"BJbg8_X61xLf","executionInfo":{"status":"ok","timestamp":1725298868120,"user_tz":-330,"elapsed":12,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"34defa99-850c-4100-9f9f-38d524251299"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n","          1.0000e+00,  0.0000e+00,  1.0000e+00],\n","        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n","          9.9995e-01,  1.0000e-03,  1.0000e+00],\n","        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n","          9.9980e-01,  2.0000e-03,  1.0000e+00],\n","        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9996e-02,\n","          9.9955e-01,  3.0000e-03,  1.0000e+00],\n","        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n","          9.9920e-01,  4.0000e-03,  9.9999e-01],\n","        [-9.5892e-01,  2.8366e-01,  4.7943e-01,  8.7758e-01,  4.9979e-02,\n","          9.9875e-01,  5.0000e-03,  9.9999e-01],\n","        [-2.7942e-01,  9.6017e-01,  5.6464e-01,  8.2534e-01,  5.9964e-02,\n","          9.9820e-01,  6.0000e-03,  9.9998e-01],\n","        [ 6.5699e-01,  7.5390e-01,  6.4422e-01,  7.6484e-01,  6.9943e-02,\n","          9.9755e-01,  6.9999e-03,  9.9998e-01],\n","        [ 9.8936e-01, -1.4550e-01,  7.1736e-01,  6.9671e-01,  7.9915e-02,\n","          9.9680e-01,  7.9999e-03,  9.9997e-01],\n","        [ 4.1212e-01, -9.1113e-01,  7.8333e-01,  6.2161e-01,  8.9879e-02,\n","          9.9595e-01,  8.9999e-03,  9.9996e-01],\n","        [-5.4402e-01, -8.3907e-01,  8.4147e-01,  5.4030e-01,  9.9833e-02,\n","          9.9500e-01,  9.9998e-03,  9.9995e-01]])\n"]}]},{"cell_type":"code","source":["# Add positional encoding to each embedding tensor\n","embeddings_with_pos = [embedding + positional_encoding for embedding in embeddings]\n","\n","# Print the embeddings with positional encodings\n","for i, embedding_with_pos in enumerate(embeddings_with_pos):\n","    print(f\"Embeddings with Positional Encoding for Padded Sentence {i+1}:\")\n","    print(embedding_with_pos)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"RM4ZRQPI25-Q","executionInfo":{"status":"ok","timestamp":1725298868120,"user_tz":-330,"elapsed":9,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"798206f8-3347-4890-b00b-0fb2ba3e0759"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embeddings with Positional Encoding for Padded Sentence 1:\n","tensor([[-0.1868,  2.9418, -1.9159,  0.8028, -0.1820,  0.1145, -1.6209,  1.3863],\n","        [-0.4423,  0.5764,  2.3257,  1.6117, -0.7483,  0.1450,  1.4288,  1.9461],\n","        [ 1.0055, -0.1113, -1.5196, -0.0966, -0.0895,  1.5557,  0.5989,  0.6294],\n","        [ 0.6427, -1.3467,  0.3895,  1.5610,  2.1572, -0.9829, -0.1190,  0.6251],\n","        [-1.1933,  1.9160,  1.1186,  1.2478, -1.4848,  2.3074, -0.1447,  0.8419],\n","        [-1.3954,  2.8533,  1.2086,  1.2043, -1.4749,  2.3070, -0.1437,  0.8419],\n","        [-0.7159,  3.5298,  1.2938,  1.1521, -1.4649,  2.3064, -0.1427,  0.8419],\n","        [ 0.2205,  3.3236,  1.3734,  1.0916, -1.4549,  2.3058, -0.1417,  0.8419],\n","        [ 0.5528,  2.4242,  1.4466,  1.0234, -1.4449,  2.3050, -0.1407,  0.8419],\n","        [-0.0244,  1.6585,  1.5125,  0.9483, -1.4350,  2.3042, -0.1397,  0.8419],\n","        [-0.9805,  1.7306,  1.5707,  0.8670, -1.4250,  2.3032, -0.1387,  0.8419]],\n","       grad_fn=<AddBackward0>)\n","Embeddings with Positional Encoding for Padded Sentence 2:\n","tensor([[ 0.1751,  1.6651,  0.0554,  1.8044,  2.1849,  0.7508, -0.7731,  0.9948],\n","        [ 0.2545,  0.5504,  1.9148,  0.1506, -0.5290,  0.7817,  0.2342,  0.0640],\n","        [ 1.8479, -1.5572, -0.3030,  0.5454,  0.2968,  1.5857,  0.7810, -0.0482],\n","        [ 0.6140, -1.0126,  0.8531,  0.0858, -0.8043, -0.3236,  0.4346, -1.0731],\n","        [ 0.7108, -0.9380,  0.0537,  0.9069,  0.5289,  1.6174, -1.4750,  1.5862],\n","        [-1.3954,  2.8533,  1.2086,  1.2043, -1.4749,  2.3070, -0.1437,  0.8419],\n","        [-0.7159,  3.5298,  1.2938,  1.1521, -1.4649,  2.3064, -0.1427,  0.8419],\n","        [ 0.2205,  3.3236,  1.3734,  1.0916, -1.4549,  2.3058, -0.1417,  0.8419],\n","        [ 0.5528,  2.4242,  1.4466,  1.0234, -1.4449,  2.3050, -0.1407,  0.8419],\n","        [-0.0244,  1.6585,  1.5125,  0.9483, -1.4350,  2.3042, -0.1397,  0.8419],\n","        [-0.9805,  1.7306,  1.5707,  0.8670, -1.4250,  2.3032, -0.1387,  0.8419]],\n","       grad_fn=<AddBackward0>)\n","Embeddings with Positional Encoding for Padded Sentence 3:\n","tensor([[-0.1915, -0.9713, -0.8564,  1.0187, -0.8644,  1.7442,  0.4870,  1.6726],\n","        [-1.7486,  1.1922,  1.5541,  1.5181, -1.2031, -0.3445, -0.0289,  1.1109],\n","        [-0.5895, -2.5838, -0.0149, -0.6698, -1.8259,  1.1678,  1.1047, -0.0160],\n","        [ 1.0797, -2.4065,  0.2628,  1.5552,  1.1028,  1.5766, -1.6729,  1.7220],\n","        [-1.1933,  1.9160,  1.1186,  1.2478, -1.4848,  2.3074, -0.1447,  0.8419],\n","        [-1.3954,  2.8533,  1.2086,  1.2043, -1.4749,  2.3070, -0.1437,  0.8419],\n","        [-0.7159,  3.5298,  1.2938,  1.1521, -1.4649,  2.3064, -0.1427,  0.8419],\n","        [ 0.2205,  3.3236,  1.3734,  1.0916, -1.4549,  2.3058, -0.1417,  0.8419],\n","        [ 0.5528,  2.4242,  1.4466,  1.0234, -1.4449,  2.3050, -0.1407,  0.8419],\n","        [-0.0244,  1.6585,  1.5125,  0.9483, -1.4350,  2.3042, -0.1397,  0.8419],\n","        [-0.9805,  1.7306,  1.5707,  0.8670, -1.4250,  2.3032, -0.1387,  0.8419]],\n","       grad_fn=<AddBackward0>)\n","Embeddings with Positional Encoding for Padded Sentence 4:\n","tensor([[-0.3876, -0.4936,  0.3814,  1.0513, -0.9165,  2.3127,  0.8038,  0.6462],\n","        [-0.5773,  1.4830, -1.2888,  2.6300,  0.7181,  0.7799,  0.1662,  1.4944],\n","        [ 0.4206, -0.7652, -0.4916,  1.4279, -0.5776,  0.8560, -0.5821,  0.9983],\n","        [ 1.0797, -2.1310, -0.2061,  0.5206,  0.3068,  1.5855,  0.7820, -0.0482],\n","        [-1.6756, -1.7756,  1.7871,  2.1127,  0.4297,  0.1992, -1.0221,  1.7402],\n","        [-1.3954,  2.8533,  1.2086,  1.2043, -1.4749,  2.3070, -0.1437,  0.8419],\n","        [-0.7159,  3.5298,  1.2938,  1.1521, -1.4649,  2.3064, -0.1427,  0.8419],\n","        [ 0.2205,  3.3236,  1.3734,  1.0916, -1.4549,  2.3058, -0.1417,  0.8419],\n","        [ 0.5528,  2.4242,  1.4466,  1.0234, -1.4449,  2.3050, -0.1407,  0.8419],\n","        [-0.0244,  1.6585,  1.5125,  0.9483, -1.4350,  2.3042, -0.1397,  0.8419],\n","        [-0.9805,  1.7306,  1.5707,  0.8670, -1.4250,  2.3032, -0.1387,  0.8419]],\n","       grad_fn=<AddBackward0>)\n","Embeddings with Positional Encoding for Padded Sentence 5:\n","tensor([[ 0.1815,  0.6861, -2.2326,  1.4115, -0.0826,  1.1202,  0.8904,  3.6753],\n","        [ 1.3431,  0.1836,  0.1938,  1.6007,  2.1372, -0.9825, -0.1210,  0.6251],\n","        [-1.1869,  0.5049, -0.9236, -0.2482, -0.7318, -0.0523, -1.7906,  2.7972],\n","        [ 0.4445, -1.5266,  0.0221,  1.2184, -0.7506,  0.2714,  0.4885,  0.4435],\n","        [-1.6310, -0.7440, -0.4392,  1.2529,  1.4324, -0.1950,  1.0173,  0.7871],\n","        [-1.6011, -0.0559,  0.9857,  1.6946,  1.0256,  1.2079, -2.3934,  2.6707],\n","        [-1.2587,  0.0623, -1.2343, -0.0627,  1.3237,  1.8224,  0.1911,  0.7863],\n","        [ 0.2205,  3.3236,  1.3734,  1.0916, -1.4549,  2.3058, -0.1417,  0.8419],\n","        [ 0.5528,  2.4242,  1.4466,  1.0234, -1.4449,  2.3050, -0.1407,  0.8419],\n","        [-0.0244,  1.6585,  1.5125,  0.9483, -1.4350,  2.3042, -0.1397,  0.8419],\n","        [-0.9805,  1.7306,  1.5707,  0.8670, -1.4250,  2.3032, -0.1387,  0.8419]],\n","       grad_fn=<AddBackward0>)\n","Embeddings with Positional Encoding for Padded Sentence 6:\n","tensor([[-0.5451,  0.7423,  0.1407,  1.1208,  0.7060,  1.4941, -0.7096,  1.6531],\n","        [ 1.7801, -0.6008, -0.4018,  0.5603,  0.2868,  1.5859,  0.7800, -0.0482],\n","        [ 2.3285, -1.6762,  0.3313,  1.2691, -1.4944,  2.3825, -1.0952,  2.2586],\n","        [-0.9412, -0.5593, -0.7450, -0.0680, -0.7128,  1.0315, -0.5431,  1.1983],\n","        [-0.5753, -0.9675, -1.8432,  1.3325, -0.0427,  1.1194,  0.8944,  3.6753],\n","        [-0.4573, -0.0731,  0.5734,  1.4833,  2.1772, -0.9837, -0.1170,  0.6250],\n","        [ 0.7336,  1.0710,  0.3619, -0.4867, -0.7760,  1.2015,  0.1585,  1.0162],\n","        [ 0.2205,  3.3236,  1.3734,  1.0916, -1.4549,  2.3058, -0.1417,  0.8419],\n","        [ 0.5528,  2.4242,  1.4466,  1.0234, -1.4449,  2.3050, -0.1407,  0.8419],\n","        [-0.0244,  1.6585,  1.5125,  0.9483, -1.4350,  2.3042, -0.1397,  0.8419],\n","        [-0.9805,  1.7306,  1.5707,  0.8670, -1.4250,  2.3032, -0.1387,  0.8419]],\n","       grad_fn=<AddBackward0>)\n","Embeddings with Positional Encoding for Padded Sentence 7:\n","tensor([[-0.0421,  0.8813, -0.2971,  1.2145, -0.8878, -0.6054, -1.4356,  1.3294],\n","        [ 1.7800, -0.8762,  0.0671,  1.5949,  1.0828,  1.5770, -1.6749,  1.7220],\n","        [-1.6808,  0.2358,  1.6529,  1.5031, -1.1931, -0.3446, -0.0279,  1.1109],\n","        [ 0.1245,  0.6220, -0.4264,  1.1846,  0.4135,  2.2550,  0.2367, -0.3629],\n","        [ 0.3615, -0.3036,  2.2295,  0.2920,  0.8031,  2.9464,  0.8647,  1.8063],\n","        [-1.6011, -0.0559,  0.9857,  1.6946,  1.0256,  1.2079, -2.3934,  2.6707],\n","        [-1.0116,  1.7932,  1.5549,  0.6331, -1.7246,  1.8282, -2.9469,  1.8295],\n","        [ 0.2205,  3.3236,  1.3734,  1.0916, -1.4549,  2.3058, -0.1417,  0.8419],\n","        [ 0.5528,  2.4242,  1.4466,  1.0234, -1.4449,  2.3050, -0.1407,  0.8419],\n","        [-0.0244,  1.6585,  1.5125,  0.9483, -1.4350,  2.3042, -0.1397,  0.8419],\n","        [-0.9805,  1.7306,  1.5707,  0.8670, -1.4250,  2.3032, -0.1387,  0.8419]],\n","       grad_fn=<AddBackward0>)\n","Embeddings with Positional Encoding for Padded Sentence 8:\n","tensor([[-0.2193, -0.7807, -1.4544,  1.2357, -0.6891,  1.2915, -1.6332,  0.1290],\n","        [ 1.1255,  1.1778, -1.0376,  3.1362,  0.5170,  0.4981, -2.4309,  0.7068],\n","        [-1.6808,  0.2358,  1.6529,  1.5031, -1.1931, -0.3446, -0.0279,  1.1109],\n","        [ 0.6140, -1.0126,  0.8531,  0.0858, -0.8043, -0.3236,  0.4346, -1.0731],\n","        [-2.2986,  0.2386,  0.4739, -0.9934, -0.5802,  1.6122, -0.8047,  2.3222],\n","        [-0.9725,  0.7258,  0.1492,  0.3924, -0.5233,  1.7679, -0.6019,  0.9451],\n","        [-0.0979,  0.6463, -1.6680,  1.2368, -0.0227,  1.1184,  0.8964,  3.6753],\n","        [ 1.1586,  0.3972,  0.7382,  1.3705,  2.1972, -0.9849, -0.1150,  0.6250],\n","        [ 0.5528,  2.4242,  1.4466,  1.0234, -1.4449,  2.3050, -0.1407,  0.8419],\n","        [-0.0244,  1.6585,  1.5125,  0.9483, -1.4350,  2.3042, -0.1397,  0.8419],\n","        [-0.9805,  1.7306,  1.5707,  0.8670, -1.4250,  2.3032, -0.1387,  0.8419]],\n","       grad_fn=<AddBackward0>)\n","Embeddings with Positional Encoding for Padded Sentence 9:\n","tensor([[ 0.6417,  0.9711,  0.3360,  1.9716,  1.3277, -0.8154,  0.7003,  1.7867],\n","        [ 0.1311,  2.6526, -0.2302,  2.0592,  0.4802,  0.8792, -0.3986,  1.3058],\n","        [ 1.2506, -0.4734,  1.5059,  2.2505,  1.3708,  1.3595, -0.6565,  1.1306],\n","        [-0.9899,  0.2244,  0.5135,  3.2170,  0.3277,  1.9554,  0.6662,  0.4733],\n","        [-0.9706, -2.0576,  0.3466,  2.4092, -0.8045, -0.6108,  0.8437,  2.0237],\n","        [ 0.6670, -0.0529, -0.4814,  1.6127,  1.5364,  0.3287,  0.4895,  1.6454],\n","        [-0.7159,  3.5298,  1.2938,  1.1521, -1.4649,  2.3064, -0.1427,  0.8419],\n","        [ 0.2205,  3.3236,  1.3734,  1.0916, -1.4549,  2.3058, -0.1417,  0.8419],\n","        [ 0.5528,  2.4242,  1.4466,  1.0234, -1.4449,  2.3050, -0.1407,  0.8419],\n","        [-0.0244,  1.6585,  1.5125,  0.9483, -1.4350,  2.3042, -0.1397,  0.8419],\n","        [-0.9805,  1.7306,  1.5707,  0.8670, -1.4250,  2.3032, -0.1387,  0.8419]],\n","       grad_fn=<AddBackward0>)\n","Embeddings with Positional Encoding for Padded Sentence 10:\n","tensor([[-1.8893e+00, -5.3017e-01, -1.1736e-01,  9.8170e-01, -4.2851e-02,\n","          1.4526e+00,  1.1603e+00, -3.1517e-01],\n","        [ 2.3854e+00, -4.1609e-01, -7.4654e-01,  1.7537e-01, -2.2777e-01,\n","          1.2750e+00,  3.9680e-01,  2.5366e+00],\n","        [ 7.0224e-01,  1.1880e+00,  9.2685e-01,  7.3307e-01, -5.5978e-01,\n","         -1.2324e-01,  7.8094e-01,  2.0914e+00],\n","        [ 1.0797e+00, -2.4065e+00,  2.6284e-01,  1.5552e+00,  1.1028e+00,\n","          1.5766e+00, -1.6729e+00,  1.7220e+00],\n","        [-1.1615e+00, -2.5035e-01,  2.0841e-01,  1.3315e+00,  7.3242e-01,\n","          1.9861e+00, -1.2752e-01,  1.0050e-01],\n","        [-8.1644e-01, -1.7818e-01,  5.9269e-01,  1.1447e+00,  9.8905e-01,\n","          2.0290e+00,  1.2414e+00,  9.2119e-02],\n","        [-3.2675e-01,  1.5816e+00,  3.3505e-02,  1.2358e+00,  3.6845e-01,\n","          7.2989e-01, -1.7786e-03,  4.1479e-01],\n","        [ 6.0680e-01,  1.8370e+00, -1.1757e+00,  9.9572e-01, -5.9642e-01,\n","          5.4484e-01, -7.2762e-01,  8.2293e-01],\n","        [ 9.7580e-01,  2.9667e-01,  3.8711e-01,  2.1154e-01, -4.9336e-01,\n","          1.7659e+00, -5.9894e-01,  9.4512e-01],\n","        [ 8.8501e-01, -9.3372e-01,  1.3409e+00, -2.4793e-01, -7.4446e-01,\n","         -3.2722e-01,  4.4057e-01, -1.0731e+00],\n","        [-2.2806e+00, -2.8696e+00, -3.8948e-01,  1.6003e-01, -4.9226e-02,\n","          2.3601e+00,  9.7885e-01,  1.8715e+00]], grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"k4uQCdEV4p51"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Positional Encoding - Learned Positional Embeddings**"],"metadata":{"id":"RCzoyoM95sYE"}},{"cell_type":"markdown","source":["How It Works\n","1.Initialize Positional Embeddings: Create a separate embedding layer for positions, similar to how token embeddings are created.\n","\n","2.Add Positional Embeddings to Token Embeddings: During the forward pass, add the positional embeddings to the token embeddings.\n","\n","3.Train the Model: The positional embeddings are updated through backpropagation along with other model parameters.\n","\n","\n"],"metadata":{"id":"7nxE4ZCl59X9"}},{"cell_type":"code","source":["# Step 7: Learnable Positional Encoding\n","class LearnablePositionalEncoding(nn.Module):\n","    def __init__(self, max_seq_len, d_model):\n","        super(LearnablePositionalEncoding, self).__init__()\n","        self.pos_embedding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))\n","\n","    def forward(self, x):\n","        # Add the positional embeddings to the input embeddings\n","        return x + self.pos_embedding[:, :x.size(1), :]"],"metadata":{"id":"Jf9Q5M5F56ah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the learnable positional encoding\n","learnable_pos_encoding = LearnablePositionalEncoding(max_len, embedding_dim)\n","\n","# Add the learnable positional encodings to the embeddings\n","embeddings_with_pos = [learnable_pos_encoding(embedding.unsqueeze(0)) for embedding in embeddings]\n","\n","# Print the embeddings with learnable positional encodings\n","for i, embedding_with_pos in enumerate(embeddings_with_pos):\n","    print(f\"Embeddings with Learnable Positional Encoding for Padded Sentence {i+1}:\")\n","    print(embedding_with_pos.squeeze(0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"vxQAE7Bb6vRo","executionInfo":{"status":"ok","timestamp":1725298868558,"user_tz":-330,"elapsed":444,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"95b51afc-0a64-40e8-9f37-522f00c73eb5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embeddings with Learnable Positional Encoding for Padded Sentence 1:\n","tensor([[-0.1868,  1.9418, -1.9159, -0.1972, -0.1820, -0.8855, -1.6209,  0.3863],\n","        [-1.2838,  0.0361,  2.2258,  0.6167, -0.7583, -0.8549,  1.4278,  0.9461],\n","        [ 0.0962,  0.3048, -1.7183, -1.0766, -0.1095,  0.5559,  0.5969, -0.3706],\n","        [ 0.5016, -0.3567,  0.0940,  0.6057,  2.1272, -1.9824, -0.1220, -0.3749],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581]],\n","       grad_fn=<SqueezeBackward1>)\n","Embeddings with Learnable Positional Encoding for Padded Sentence 2:\n","tensor([[ 0.1751,  0.6651,  0.0554,  0.8044,  2.1849, -0.2492, -0.7731, -0.0052],\n","        [-0.5870,  0.0101,  1.8150, -0.8444, -0.5390, -0.2182,  0.2332, -0.9360],\n","        [ 0.9386, -1.1411, -0.5017, -0.4347,  0.2768,  0.5859,  0.7790, -1.0482],\n","        [ 0.4729, -0.0226,  0.5576, -0.8695, -0.8343, -1.3232,  0.4316, -2.0731],\n","        [ 1.4676, -0.2844, -0.3357, -0.0142,  0.4889,  0.6182, -1.4790,  0.5862],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581]],\n","       grad_fn=<SqueezeBackward1>)\n","Embeddings with Learnable Positional Encoding for Padded Sentence 3:\n","tensor([[-0.1915, -1.9713, -0.8564,  0.0187, -0.8644,  0.7442,  0.4870,  0.6726],\n","        [-2.5901,  0.6519,  1.4543,  0.5231, -1.2131, -1.3444, -0.0299,  0.1109],\n","        [-1.4988, -2.1676, -0.2136, -1.6499, -1.8459,  0.1680,  1.1027, -1.0160],\n","        [ 0.9386, -1.4165, -0.0327,  0.5999,  1.0728,  0.5771, -1.6759,  0.7220],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581]],\n","       grad_fn=<SqueezeBackward1>)\n","Embeddings with Learnable Positional Encoding for Padded Sentence 4:\n","tensor([[-3.8762e-01, -1.4936e+00,  3.8137e-01,  5.1280e-02, -9.1653e-01,\n","          1.3127e+00,  8.0382e-01, -3.5382e-01],\n","        [-1.4187e+00,  9.4266e-01, -1.3886e+00,  1.6350e+00,  7.0809e-01,\n","         -2.2002e-01,  1.6517e-01,  4.9445e-01],\n","        [-4.8872e-01, -3.4907e-01, -6.9030e-01,  4.4782e-01, -5.9760e-01,\n","         -1.4383e-01, -5.8414e-01, -1.6932e-03],\n","        [ 9.3862e-01, -1.1411e+00, -5.0166e-01, -4.3470e-01,  2.7681e-01,\n","          5.8592e-01,  7.7905e-01, -1.0482e+00],\n","        [-9.1877e-01, -1.1220e+00,  1.3977e+00,  1.1916e+00,  3.8969e-01,\n","         -8.0005e-01, -1.0261e+00,  7.4022e-01],\n","        [-4.3652e-01,  2.5697e+00,  7.2920e-01,  3.2673e-01, -1.5248e+00,\n","          1.3082e+00, -1.4869e-01, -1.5807e-01],\n","        [-4.3652e-01,  2.5697e+00,  7.2920e-01,  3.2673e-01, -1.5248e+00,\n","          1.3082e+00, -1.4869e-01, -1.5807e-01],\n","        [-4.3652e-01,  2.5697e+00,  7.2920e-01,  3.2673e-01, -1.5248e+00,\n","          1.3082e+00, -1.4869e-01, -1.5807e-01],\n","        [-4.3652e-01,  2.5697e+00,  7.2920e-01,  3.2673e-01, -1.5248e+00,\n","          1.3082e+00, -1.4869e-01, -1.5807e-01],\n","        [-4.3652e-01,  2.5697e+00,  7.2920e-01,  3.2673e-01, -1.5248e+00,\n","          1.3082e+00, -1.4869e-01, -1.5807e-01],\n","        [-4.3652e-01,  2.5697e+00,  7.2920e-01,  3.2673e-01, -1.5248e+00,\n","          1.3082e+00, -1.4869e-01, -1.5807e-01]], grad_fn=<SqueezeBackward1>)\n","Embeddings with Learnable Positional Encoding for Padded Sentence 5:\n","tensor([[ 0.1815, -0.3139, -2.2326,  0.4115, -0.0826,  0.1202,  0.8904,  2.6753],\n","        [ 0.5016, -0.3567,  0.0940,  0.6057,  2.1272, -1.9824, -0.1220, -0.3749],\n","        [-2.0962,  0.9211, -1.1223, -1.2283, -0.7518, -1.0521, -1.7926,  1.7972],\n","        [ 0.3034, -0.5366, -0.2735,  0.2631, -0.7806, -0.7281,  0.4855, -0.5565],\n","        [-0.8742, -0.0904, -0.8286,  0.3318,  1.3924, -1.1942,  1.0133, -0.2129],\n","        [-0.6421, -0.3395,  0.5063,  0.8171,  0.9756,  0.2092, -2.3984,  1.6707],\n","        [-0.9793, -0.8979, -1.7989, -0.8880,  1.2637,  0.8242,  0.1851, -0.2137],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581]],\n","       grad_fn=<SqueezeBackward1>)\n","Embeddings with Learnable Positional Encoding for Padded Sentence 6:\n","tensor([[-0.5451, -0.2577,  0.1407,  0.1208,  0.7060,  0.4941, -0.7096,  0.6531],\n","        [ 0.9386, -1.1411, -0.5017, -0.4347,  0.2768,  0.5859,  0.7790, -1.0482],\n","        [ 1.4192, -1.2600,  0.1326,  0.2890, -1.5143,  1.3827, -1.0972,  1.2586],\n","        [-1.0823,  0.4307, -1.0405, -1.0234, -0.7428,  0.0320, -0.5461,  0.1983],\n","        [ 0.1815, -0.3139, -2.2326,  0.4115, -0.0826,  0.1202,  0.8904,  2.6753],\n","        [ 0.5016, -0.3567,  0.0940,  0.6057,  2.1272, -1.9824, -0.1220, -0.3749],\n","        [ 1.0130,  0.1108, -0.2027, -1.3120, -0.8360,  0.2033,  0.1525,  0.0162],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581]],\n","       grad_fn=<SqueezeBackward1>)\n","Embeddings with Learnable Positional Encoding for Padded Sentence 7:\n","tensor([[-0.0421, -0.1187, -0.2971,  0.2145, -0.8878, -1.6054, -1.4356,  0.3294],\n","        [ 0.9386, -1.4165, -0.0327,  0.5999,  1.0728,  0.5771, -1.6759,  0.7220],\n","        [-2.5901,  0.6519,  1.4543,  0.5231, -1.2131, -1.3444, -0.0299,  0.1109],\n","        [-0.0166,  1.6120, -0.7219,  0.2292,  0.3835,  1.2555,  0.2337, -1.3629],\n","        [ 1.1183,  0.3501,  1.8401, -0.6290,  0.7631,  1.9472,  0.8607,  0.8063],\n","        [-0.6421, -0.3395,  0.5063,  0.8171,  0.9756,  0.2092, -2.3984,  1.6707],\n","        [-0.7322,  0.8330,  0.9903, -0.1922, -1.7846,  0.8300, -2.9529,  0.8295],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581]],\n","       grad_fn=<SqueezeBackward1>)\n","Embeddings with Learnable Positional Encoding for Padded Sentence 8:\n","tensor([[-0.2193, -1.7807, -1.4544,  0.2357, -0.6891,  0.2915, -1.6332, -0.8710],\n","        [ 0.2840,  0.6375, -1.1374,  2.1412,  0.5070, -0.5019, -2.4319, -0.2932],\n","        [-2.5901,  0.6519,  1.4543,  0.5231, -1.2131, -1.3444, -0.0299,  0.1109],\n","        [ 0.4729, -0.0226,  0.5576, -0.8695, -0.8343, -1.3232,  0.4316, -2.0731],\n","        [-1.5418,  0.8922,  0.0845, -1.9144, -0.6202,  0.6130, -0.8087,  1.3223],\n","        [-0.0136,  0.4422, -0.3302, -0.4852, -0.5733,  0.7691, -0.6069, -0.0548],\n","        [ 0.1815, -0.3139, -2.2326,  0.4115, -0.0826,  0.1202,  0.8904,  2.6753],\n","        [ 0.5016, -0.3567,  0.0940,  0.6057,  2.1272, -1.9824, -0.1220, -0.3749],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581]],\n","       grad_fn=<SqueezeBackward1>)\n","Embeddings with Learnable Positional Encoding for Padded Sentence 9:\n","tensor([[ 0.6417, -0.0289,  0.3360,  0.9716,  1.3277, -1.8154,  0.7003,  0.7867],\n","        [-0.7104,  2.1123, -0.3301,  1.0642,  0.4702, -0.1208, -0.3996,  0.3058],\n","        [ 0.3414, -0.0572,  1.3073,  1.2704,  1.3508,  0.3597, -0.6585,  0.1306],\n","        [-1.1310,  1.2144,  0.2179,  2.2617,  0.2977,  0.9559,  0.6632, -0.5267],\n","        [-0.2138, -1.4040, -0.0428,  1.4882, -0.8445, -1.6100,  0.8397,  1.0237],\n","        [ 1.6259, -0.3365, -0.9608,  0.7351,  1.4864, -0.6700,  0.4845,  0.6454],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581],\n","        [-0.4365,  2.5697,  0.7292,  0.3267, -1.5248,  1.3082, -0.1487, -0.1581]],\n","       grad_fn=<SqueezeBackward1>)\n","Embeddings with Learnable Positional Encoding for Padded Sentence 10:\n","tensor([[-1.8893, -1.5302, -0.1174, -0.0183, -0.0429,  0.4526,  1.1603, -1.3152],\n","        [ 1.5439, -0.9564, -0.8464, -0.8196, -0.2378,  0.2751,  0.3958,  1.5366],\n","        [-0.2071,  1.6042,  0.7282, -0.2470, -0.5798, -1.1230,  0.7789,  1.0914],\n","        [ 0.9386, -1.4165, -0.0327,  0.5999,  1.0728,  0.5771, -1.6759,  0.7220],\n","        [-0.4047,  0.4033, -0.1810,  0.4105,  0.6924,  0.9869, -0.1315, -0.8995],\n","        [ 0.1425, -0.4618,  0.1133,  0.2671,  0.9391,  1.0303,  1.2364, -0.9079],\n","        [-0.0473,  0.6214, -0.5311,  0.4105,  0.3085, -0.2683, -0.0078, -0.5852],\n","        [-0.0502,  1.0831, -1.8199,  0.2309, -0.6664, -0.4527, -0.7346, -0.1770],\n","        [-0.0136,  0.4422, -0.3302, -0.4852, -0.5733,  0.7691, -0.6069, -0.0548],\n","        [ 0.4729, -0.0226,  0.5576, -0.8695, -0.8343, -1.3232,  0.4316, -2.0731],\n","        [-1.7366, -2.0305, -1.2310, -0.3803, -0.1491,  1.3651,  0.9689,  0.8715]],\n","       grad_fn=<SqueezeBackward1>)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zeB-FLgw8lZS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RoPE - (Rotational Positional Encoding)"],"metadata":{"id":"b-3-CQvFIBiJ"}},{"cell_type":"markdown","source":["Mathematical Formulation\n","Given an embedding vector\n","𝑥\n","x for a token at position\n","𝑝\n","p, RoPE applies a rotation matrix\n","𝑅\n","(\n","𝑝\n",")\n","R(p) to generate the positional encoding.\n","\n","For each pair of dimensions\n","𝑖\n","i and\n","𝑖\n","+\n","1\n","i+1 in the embedding:\n","\n","𝑥\n","′\n","[\n","𝑖\n","]\n","=\n","𝑥\n","[\n","𝑖\n","]\n","cos\n","⁡\n","(\n","𝜃\n","𝑝\n",")\n","−\n","𝑥\n","[\n","𝑖\n","+\n","1\n","]\n","sin\n","⁡\n","(\n","𝜃\n","𝑝\n",")\n","\n","\n","𝑥\n","′\n","[\n","𝑖\n","+\n","1\n","]\n","=\n","𝑥\n","[\n","𝑖\n","]\n","sin\n","⁡\n","(\n","𝜃\n","𝑝\n",")\n","+\n","𝑥\n","[\n","𝑖\n","+\n","1\n","]\n","cos\n","⁡\n","(\n","𝜃\n","𝑝\n",")\n","\n","\n","\n","\n","Where\n","𝜃\n","𝑝\n","θ\n","p\n","​\n","  is the angle determined by the position\n","𝑝\n","p."],"metadata":{"id":"VPHhGGJlJKm1"}},{"cell_type":"code","source":["import math\n","\n","class RotationalPositionalEncoding(nn.Module):\n","    def __init__(self, d_model):\n","        super(RotationalPositionalEncoding, self).__init__()\n","        self.d_model = d_model\n","        assert d_model % 2 == 0, \"Embedding dimension must be even for RoPE.\"\n","\n","    def forward(self, x):\n","        seq_len = x.size(1)\n","        pos = torch.arange(seq_len, dtype=torch.float32, device=x.device)\n","        dim = torch.arange(self.d_model // 2, dtype=torch.float32, device=x.device)\n","\n","        # Compute the angles for the rotations\n","        inv_freq = 1.0 / (10000 ** (2 * dim / self.d_model))\n","        sinusoid_inp = torch.einsum(\"i,j->ij\", pos, inv_freq)\n","\n","        sin = sinusoid_inp.sin()\n","        cos = sinusoid_inp.cos()\n","\n","        # Apply rotation\n","        x_1, x_2 = x[..., 0::2], x[..., 1::2]\n","        x_rot = torch.cat([x_1 * cos - x_2 * sin, x_1 * sin + x_2 * cos], dim=-1)\n","\n","        return x_rot"],"metadata":{"id":"EC6Wf48qILur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Initialize RoPE\n","rot_pos_encoding = RotationalPositionalEncoding(embedding_dim)\n","\n","# Apply RoPE to the embeddings\n","rotated_embeddings = [rot_pos_encoding(embedding.unsqueeze(0)) for embedding in embeddings]\n","\n","# Print the result\n","print(\"Embeddings with Rotational Positional Encoding:\\n\", rotated_embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2tKO7KIJIodo","executionInfo":{"status":"ok","timestamp":1725298869018,"user_tz":-330,"elapsed":467,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"0ab5d355-5df7-4a93-dc8e-9bf01de92b9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embeddings with Rotational Positional Encoding:\n"," [tensor([[[-0.1868, -1.9159, -0.1820, -1.6209,  1.9418, -0.1972, -0.8855,\n","           0.3863],\n","         [-0.7240,  2.1531, -0.7497,  1.4268, -1.0608,  0.8358, -0.8624,\n","           0.9475],\n","         [-0.3172, -1.4701, -0.1206,  0.5976, -0.0394, -1.3965,  0.5536,\n","          -0.3694],\n","         [-0.4463, -0.0892,  2.1857, -0.1209,  0.4240,  0.6064, -1.9177,\n","          -0.3753],\n","         [ 2.2301,  0.5444, -1.5759, -0.1481, -1.3493,  0.5849,  1.2462,\n","          -0.1587],\n","         [ 2.3403,  0.4833, -1.5883, -0.1479,  1.1475,  0.6363,  1.2304,\n","          -0.1588],\n","         [ 0.2989,  0.4173, -1.6005, -0.1477,  2.5893,  0.6814,  1.2145,\n","          -0.1590],\n","         [-2.0173,  0.3472, -1.6126, -0.1476,  1.6505,  0.7197,  1.1984,\n","          -0.1591],\n","         [-2.4788,  0.2737, -1.6245, -0.1474, -0.8058,  0.7507,  1.1822,\n","          -0.1593],\n","         [-0.6613,  0.1973, -1.6362, -0.1473, -2.5212,  0.7743,  1.1659,\n","          -0.1594],\n","         [ 1.7642,  0.1191, -1.6478, -0.1471, -1.9187,  0.7901,  1.1495,\n","          -0.1595]]], grad_fn=<CatBackward0>), tensor([[[ 0.1751,  0.0554,  2.1849, -0.7731,  0.6651,  0.8044, -0.2492,\n","          -0.0052],\n","         [-0.3256,  1.8902, -0.5368,  0.2342, -0.4885, -0.6589, -0.2236,\n","          -0.9358],\n","         [ 0.6470, -0.4053,  0.2650,  0.7811,  1.3283, -0.5257,  0.5913,\n","          -1.0466],\n","         [-0.4650,  0.7896, -0.7943,  0.4378,  0.0891, -0.6659, -1.3476,\n","          -2.0718],\n","         [-1.1745, -0.3037,  0.4638, -1.4813, -0.9248, -0.1438,  0.6372,\n","           0.5803],\n","         [ 2.3403,  0.4833, -1.5883, -0.1479,  1.1475,  0.6363,  1.2304,\n","          -0.1588],\n","         [ 0.2989,  0.4173, -1.6005, -0.1477,  2.5893,  0.6814,  1.2145,\n","          -0.1590],\n","         [-2.0173,  0.3472, -1.6126, -0.1476,  1.6505,  0.7197,  1.1984,\n","          -0.1591],\n","         [-2.4788,  0.2737, -1.6245, -0.1474, -0.8058,  0.7507,  1.1822,\n","          -0.1593],\n","         [-0.6613,  0.1973, -1.6362, -0.1473, -2.5212,  0.7743,  1.1659,\n","          -0.1594],\n","         [ 1.7642,  0.1191, -1.6478, -0.1471, -1.9187,  0.7901,  1.1495,\n","          -0.1595]]], grad_fn=<CatBackward0>), tensor([[[-0.1915, -0.8564, -0.8644,  0.4870, -1.9713,  0.0187,  0.7442,\n","           0.6726],\n","         [-1.9480,  1.3948, -1.1996, -0.0301, -1.8272,  0.6656, -1.3565,\n","           0.1108],\n","         [ 2.5947,  0.1185, -1.8489,  1.1047, -0.4608, -1.6594,  0.1310,\n","          -1.0138],\n","         [-0.7293, -0.2085,  1.0550, -1.6781,  1.5348,  0.5634,  0.6090,\n","           0.7170],\n","         [ 2.2301,  0.5444, -1.5759, -0.1481, -1.3493,  0.5849,  1.2462,\n","          -0.1587],\n","         [ 2.3403,  0.4833, -1.5883, -0.1479,  1.1475,  0.6363,  1.2304,\n","          -0.1588],\n","         [ 0.2989,  0.4173, -1.6005, -0.1477,  2.5893,  0.6814,  1.2145,\n","          -0.1590],\n","         [-2.0173,  0.3472, -1.6126, -0.1476,  1.6505,  0.7197,  1.1984,\n","          -0.1591],\n","         [-2.4788,  0.2737, -1.6245, -0.1474, -0.8058,  0.7507,  1.1822,\n","          -0.1593],\n","         [-0.6613,  0.1973, -1.6362, -0.1473, -2.5212,  0.7743,  1.1659,\n","          -0.1594],\n","         [ 1.7642,  0.1191, -1.6478, -0.1471, -1.9187,  0.7901,  1.1495,\n","          -0.1595]]], grad_fn=<CatBackward0>), tensor([[[-0.3876,  0.3814, -0.9165,  0.8038, -1.4936,  0.0513,  1.3127,\n","          -0.3538],\n","         [-1.5598, -1.5449,  0.7103,  0.1647, -0.6845,  1.4882, -0.2129,\n","           0.4946],\n","         [ 0.5208, -0.7655, -0.5946, -0.5841, -0.2991,  0.3018, -0.1558,\n","          -0.0029],\n","         [-0.7682, -0.3508,  0.2591,  0.7822,  1.2621, -0.5635,  0.5940,\n","          -1.0458],\n","         [-0.2486,  0.8233,  0.4214, -1.0291,  1.4287,  1.6418, -0.7838,\n","           0.7361],\n","         [ 2.3403,  0.4833, -1.5883, -0.1479,  1.1475,  0.6363,  1.2304,\n","          -0.1588],\n","         [ 0.2989,  0.4173, -1.6005, -0.1477,  2.5893,  0.6814,  1.2145,\n","          -0.1590],\n","         [-2.0173,  0.3472, -1.6126, -0.1476,  1.6505,  0.7197,  1.1984,\n","          -0.1591],\n","         [-2.4788,  0.2737, -1.6245, -0.1474, -0.8058,  0.7507,  1.1822,\n","          -0.1593],\n","         [-0.6613,  0.1973, -1.6362, -0.1473, -2.5212,  0.7743,  1.1659,\n","          -0.1594],\n","         [ 1.7642,  0.1191, -1.6478, -0.1471, -1.9187,  0.7901,  1.1495,\n","          -0.1595]]], grad_fn=<CatBackward0>), tensor([[[ 0.1815, -2.2326, -0.0826,  0.8904, -0.3139,  0.4115,  0.1202,\n","           2.6753],\n","         [ 0.5712,  0.0330,  2.1469, -0.1216,  0.2293,  0.6120, -1.9610,\n","          -0.3751],\n","         [ 0.0348, -0.8559, -0.7306, -1.7962, -2.2894, -1.4267, -1.0669,\n","           1.7936],\n","         [-0.2246, -0.3390, -0.7585,  0.4872,  0.5740,  0.1705, -0.7512,\n","          -0.5551],\n","         [ 0.5030, -0.8924,  1.4390,  1.0141,  0.7207, -0.0171, -1.1376,\n","          -0.2089],\n","         [-0.5077,  0.0526,  0.9640, -2.4067,  0.5195,  0.9598,  0.2577,\n","           1.6587],\n","         [-1.1912, -0.9833,  1.2120,  0.1864, -0.5885, -1.7486,  0.8985,\n","          -0.2126],\n","         [-2.0173,  0.3472, -1.6126, -0.1476,  1.6505,  0.7197,  1.1984,\n","          -0.1591],\n","         [-2.4788,  0.2737, -1.6245, -0.1474, -0.8058,  0.7507,  1.1822,\n","          -0.1593],\n","         [-0.6613,  0.1973, -1.6362, -0.1473, -2.5212,  0.7743,  1.1659,\n","          -0.1594],\n","         [ 1.7642,  0.1191, -1.6478, -0.1471, -1.9187,  0.7901,  1.1495,\n","          -0.1595]]], grad_fn=<CatBackward0>), tensor([[[-0.5451,  0.1407,  0.7060, -0.7096, -0.2577,  0.1208,  0.4941,\n","           0.6531],\n","         [ 1.4673, -0.4558,  0.2709,  0.7801,  0.1733, -0.4826,  0.5887,\n","          -1.0474],\n","         [ 0.5552,  0.0726, -1.5417, -1.0997,  1.8148,  0.3096,  1.3521,\n","           1.2564],\n","         [ 1.0107, -0.6916, -0.7434, -0.5467, -0.5791, -1.2851,  0.0097,\n","           0.1967],\n","         [-0.3562, -2.2166, -0.0874,  0.8796,  0.0678, -0.4904,  0.1168,\n","           2.6788],\n","         [-0.1998, -0.2079,  2.2236, -0.1201, -0.5822,  0.5766, -1.8736,\n","          -0.3755],\n","         [ 1.0036,  0.5735, -0.8466,  0.1524, -0.1766, -1.1973,  0.1529,\n","           0.0171],\n","         [-2.0173,  0.3472, -1.6126, -0.1476,  1.6505,  0.7197,  1.1984,\n","          -0.1591],\n","         [-2.4788,  0.2737, -1.6245, -0.1474, -0.8058,  0.7507,  1.1822,\n","          -0.1593],\n","         [-0.6613,  0.1973, -1.6362, -0.1473, -2.5212,  0.7743,  1.1659,\n","          -0.1594],\n","         [ 1.7642,  0.1191, -1.6478, -0.1471, -1.9187,  0.7901,  1.1495,\n","          -0.1595]]], grad_fn=<CatBackward0>), tensor([[[-0.0421, -0.2971, -0.8878, -1.4356, -0.1187,  0.2145, -1.6054,\n","           0.3294],\n","         [ 1.6990, -0.0924,  1.0670, -1.6766,  0.0244,  0.5936,  0.5878,\n","           0.7204],\n","         [ 0.4851,  1.3214, -1.1860, -0.0302, -2.6264,  0.8016, -1.3684,\n","           0.1108],\n","         [-0.2110, -0.7574,  0.3457,  0.2378, -1.5982,  0.0056,  1.2664,\n","          -1.3622],\n","         [-0.4661,  1.9398,  0.6846,  0.8574, -1.0752,  0.1372,  1.9762,\n","           0.8097],\n","         [-0.5077,  0.0526,  0.9640, -2.4067,  0.5195,  0.9598,  0.2577,\n","           1.6587],\n","         [-0.4703,  0.9258, -1.8312, -2.9578,  1.0045,  0.4005,  0.7215,\n","           0.8118],\n","         [-2.0173,  0.3472, -1.6126, -0.1476,  1.6505,  0.7197,  1.1984,\n","          -0.1591],\n","         [-2.4788,  0.2737, -1.6245, -0.1474, -0.8058,  0.7507,  1.1822,\n","          -0.1593],\n","         [-0.6613,  0.1973, -1.6362, -0.1473, -2.5212,  0.7743,  1.1659,\n","          -0.1594],\n","         [ 1.7642,  0.1191, -1.6478, -0.1471, -1.9187,  0.7901,  1.1495,\n","          -0.1595]]], grad_fn=<CatBackward0>), tensor([[[-0.2193, -1.4544, -0.6891, -1.6332, -1.7807,  0.2357,  0.2915,\n","          -0.8710],\n","         [-0.3829, -1.3455,  0.5120, -2.4316,  0.5834,  2.0169, -0.4968,\n","          -0.2957],\n","         [ 0.4851,  1.3214, -1.1860, -0.0302, -2.6264,  0.8016, -1.3684,\n","           0.1108],\n","         [-0.4650,  0.7896, -0.7943,  0.4378,  0.0891, -0.6659, -1.3476,\n","          -2.0718],\n","         [ 1.6830,  0.8234, -0.6442, -0.8139,  0.5836, -1.7304,  0.5877,\n","           1.3190],\n","         [ 0.4202, -0.0572, -0.6110, -0.6067,  0.1384, -0.5841,  0.7395,\n","          -0.0579],\n","         [ 0.0866, -2.0750, -0.0897,  0.8743, -0.3521, -0.9210,  0.1151,\n","           2.6806],\n","         [ 0.6125, -0.3183,  2.2607, -0.1194,  0.0606,  0.5238, -1.8288,\n","          -0.3758],\n","         [-2.4788,  0.2737, -1.6245, -0.1474, -0.8058,  0.7507,  1.1822,\n","          -0.1593],\n","         [-0.6613,  0.1973, -1.6362, -0.1473, -2.5212,  0.7743,  1.1659,\n","          -0.1594],\n","         [ 1.7642,  0.1191, -1.6478, -0.1471, -1.9187,  0.7901,  1.1495,\n","          -0.1595]]], grad_fn=<CatBackward0>), tensor([[[ 0.6417,  0.3360,  1.3277,  0.7003, -0.0289,  0.9716, -1.8154,\n","           0.7867],\n","         [-2.1612, -0.4347,  0.4714, -0.3999,  0.5435,  1.0259, -0.1161,\n","           0.3054],\n","         [-0.0900,  1.0288,  1.3433, -0.6588,  0.3342,  1.5048,  0.3866,\n","           0.1293],\n","         [ 0.9483, -0.4602,  0.2689,  0.6648, -1.3618,  2.2251,  0.9644,\n","          -0.5247],\n","         [-0.9228, -0.6189, -0.7795,  0.8356,  1.0795,  1.3540, -1.6425,\n","           1.0271],\n","         [ 0.1385, -1.1956,  1.5181,  0.4812, -1.6546,  0.1844, -0.5949,\n","           0.6479],\n","         [ 0.2989,  0.4173, -1.6005, -0.1477,  2.5893,  0.6814,  1.2145,\n","          -0.1590],\n","         [-2.0173,  0.3472, -1.6126, -0.1476,  1.6505,  0.7197,  1.1984,\n","          -0.1591],\n","         [-2.4788,  0.2737, -1.6245, -0.1474, -0.8058,  0.7507,  1.1822,\n","          -0.1593],\n","         [-0.6613,  0.1973, -1.6362, -0.1473, -2.5212,  0.7743,  1.1659,\n","          -0.1594],\n","         [ 1.7642,  0.1191, -1.6478, -0.1471, -1.9187,  0.7901,  1.1495,\n","          -0.1595]]], grad_fn=<CatBackward0>), tensor([[[-1.8893, -0.1174, -0.0429,  1.1603, -1.5302, -0.0183,  0.4526,\n","          -1.3152],\n","         [ 1.6389, -0.7603, -0.2405,  0.3943,  0.7824, -0.9000,  0.2727,\n","           1.5370],\n","         [-1.3725,  0.7627, -0.5572,  0.7768, -0.8558, -0.0974, -1.1344,\n","           1.0929],\n","         [-0.7293, -0.2085,  1.0550, -1.6781,  1.5348,  0.5634,  0.6090,\n","           0.7170],\n","         [ 0.5698, -0.3266,  0.6524, -0.1279,  0.0427,  0.3076,  1.0138,\n","          -0.9000],\n","         [-0.4024, -0.0287,  0.8864,  1.2409, -0.2676,  0.2887,  1.0759,\n","          -0.9017],\n","         [ 0.1282, -0.6701,  0.3240, -0.0043,  0.6099,  0.0389, -0.2493,\n","          -0.5852],\n","         [-0.7494, -1.5407, -0.6331, -0.7334,  0.7836, -0.9958, -0.4982,\n","          -0.1822],\n","         [-0.4355,  0.1180, -0.6329, -0.6065, -0.0777, -0.5749,  0.7209,\n","          -0.0597],\n","         [-0.4216,  1.0277, -0.7120,  0.4502,  0.2155, -0.1037, -1.3928,\n","          -2.0691],\n","         [ 0.3525, -0.3451, -0.2846,  0.9601,  2.6485, -1.2413,  1.3434,\n","           0.8812]]], grad_fn=<CatBackward0>)]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"a5cuTdCLItdy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Self-Attention"],"metadata":{"id":"ZbdbsSyWLWMp"}},{"cell_type":"markdown","source":["Self-Attention in Transformers\n","\n","Self-attention, also known as intra-attention, is a key component of the Transformer architecture. It allows the model to weigh the importance of different parts of the input sequence when processing each element. Here's why it's crucial:\n","\n","1. Contextual understanding: Self-attention enables each word to attend to all other words in the sequence, capturing long-range dependencies and contextual information.\n","\n","2. Parallelization: Unlike RNNs, self-attention can be computed in parallel for all positions, making it more efficient for training on modern hardware.\n","\n","3. No sequential bottleneck: It doesn't suffer from the sequential nature of RNNs, allowing better handling of long-range dependencies.\n","\n","4. Position-aware: When combined with positional encoding, it can take into account the relative or absolute positions of sequence elements.\n","\n","5. Interpretability: The attention weights can be visualized to understand which parts of the input the model focuses on for each output.\n","\n","In Transformers, self-attention is used in both the encoder and decoder, allowing the model to process input and generate output by considering the entire context of the sequence.\n"],"metadata":{"id":"h_ijiyMfLawc"}},{"cell_type":"code","source":["\n","# Step 7: Implement Self-Attention\n","class SelfAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(SelfAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n","\n","        self.depth = d_model // num_heads\n","\n","        self.w_q = nn.Linear(d_model, d_model)\n","        self.w_k = nn.Linear(d_model, d_model)\n","        self.w_v = nn.Linear(d_model, d_model)\n","        self.w_o = nn.Linear(d_model, d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        x = x.view(batch_size, -1, self.num_heads, self.depth)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        print(batch_size)\n","        print(x.shape)\n","        q = self.w_q(x)\n","        k = self.w_k(x)\n","        v = self.w_v(x)\n","\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        # Scaled dot-product attention\n","        scores = torch.matmul(q, k.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n","        attention_weights = nn.functional.softmax(scores, dim=-1)\n","\n","        out = torch.matmul(attention_weights, v)\n","        out = out.permute(0, 2, 1, 3).contiguous()\n","        out = out.view(batch_size, -1, self.d_model)\n","\n","        return self.w_o(out)\n","\n","# Example usage of Self-Attention\n","d_model = embedding_dim\n","num_heads = 2\n","\n","self_attention = SelfAttention(d_model, num_heads)\n","attention_output = [self_attention(embeddings_with_pos[0].unsqueeze(0))]\n","\n","# Print the attention output\n","print(\"Self-Attention Output:\\n\", attention_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1IuPcBVcLZZz","executionInfo":{"status":"ok","timestamp":1725298869019,"user_tz":-330,"elapsed":26,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"11b03808-bc33-4f37-90f1-9c30bf6d54f3","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","torch.Size([1, 1, 11, 8])\n","Self-Attention Output:\n"," [tensor([[[ 0.2800, -0.3912,  0.3615,  0.2537, -0.0485, -0.3278, -0.4037,\n","           0.1846],\n","         [ 0.4607, -0.5139,  0.1797,  0.2159, -0.0106, -0.4759, -0.6212,\n","           0.1922],\n","         [ 0.3931, -0.4884,  0.2413,  0.2587, -0.0412, -0.4232, -0.5381,\n","           0.2028],\n","         [ 0.3274, -0.4345,  0.2283,  0.2571, -0.1562, -0.3656, -0.4148,\n","           0.0668],\n","         [ 0.4038, -0.4634,  0.2993,  0.2249,  0.0493, -0.4266, -0.5735,\n","           0.2641],\n","         [ 0.4038, -0.4634,  0.2993,  0.2249,  0.0493, -0.4266, -0.5735,\n","           0.2641],\n","         [ 0.4038, -0.4634,  0.2993,  0.2249,  0.0493, -0.4266, -0.5735,\n","           0.2641],\n","         [ 0.4038, -0.4634,  0.2993,  0.2249,  0.0493, -0.4266, -0.5735,\n","           0.2641],\n","         [ 0.4038, -0.4634,  0.2993,  0.2249,  0.0493, -0.4266, -0.5735,\n","           0.2641],\n","         [ 0.4038, -0.4634,  0.2993,  0.2249,  0.0493, -0.4266, -0.5735,\n","           0.2641],\n","         [ 0.4038, -0.4634,  0.2993,  0.2249,  0.0493, -0.4266, -0.5735,\n","           0.2641]]], grad_fn=<ViewBackward0>)]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xRUYqLFaLvVa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Grouped Multi-query Attention"],"metadata":{"id":"Zlq8CLnqQKzs"}},{"cell_type":"markdown","source":["Grouped Multi-query Attention (GMQA)\n","\n","Grouped Multi-query Attention (GMQA) is an optimization technique for transformer models that aims to reduce computational complexity and memory usage while maintaining model performance. It was introduced as an alternative to the standard multi-head attention mechanism.\n","\n","Key features of GMQA:\n","\n","1. Reduced parameter count: Instead of having separate key and value projections for each attention head, GMQA groups multiple query heads to share the same key and value projections.\n","\n","2. Improved efficiency: By reducing the number of key and value projections, GMQA decreases the computational cost and memory requirements of the attention mechanism.\n","\n","3. Scalability: GMQA allows for a higher number of attention heads without significantly increasing the model size, potentially leading to improved model capacity and performance.\n","\n","4. Flexibility: The number of groups can be adjusted to balance between computational efficiency and model expressiveness.\n","\n","GMQA has shown promising results in various natural language processing tasks, offering a good trade-off between model size, computational efficiency, and performance.\n"],"metadata":{"id":"zWIFhXI_ZV3M"}},{"cell_type":"code","source":["class GroupedMultiQueryAttention(nn.Module):\n","    def __init__(self, d_model, num_heads, num_groups):\n","        super(GroupedMultiQueryAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.num_groups = num_groups\n","        self.d_model = d_model\n","        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n","        assert num_heads % num_groups == 0, \"num_heads must be divisible by num_groups.\"\n","\n","        self.depth = d_model // num_heads\n","        self.group_depth = d_model // num_groups\n","\n","        self.w_q = nn.Linear(d_model, d_model)\n","        self.w_k = nn.Linear(d_model, self.group_depth)\n","        self.w_v = nn.Linear(d_model, self.group_depth)\n","        self.w_o = nn.Linear(d_model, d_model)\n","\n","    def split_heads(self, x, batch_size, num_heads):\n","        x = x.view(batch_size, -1, num_heads, self.depth)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","\n","        q = self.w_q(x)\n","        k = self.w_k(x)\n","        v = self.w_v(x)\n","\n","        q = self.split_heads(q, batch_size, self.num_heads)\n","\n","        # Split into groups first\n","        k = self.split_heads(k, batch_size, self.num_groups)\n","        v = self.split_heads(v, batch_size, self.num_groups)\n","\n","        # Expand k and v to match the number of heads\n","        k = k.repeat(1, self.num_heads // self.num_groups, 1, 1)\n","        v = v.repeat(1, self.num_heads // self.num_groups, 1, 1)\n","\n","        # Scaled dot-product attention\n","        scores = torch.matmul(q, k.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n","        attention_weights = nn.functional.softmax(scores, dim=-1)\n","\n","        out = torch.matmul(attention_weights, v)\n","        out = out.permute(0, 2, 1, 3).contiguous()\n","        out = out.view(batch_size, -1, self.d_model)\n","\n","        return self.w_o(out)\n","\n"],"metadata":{"id":"eCXuZjVIQPRI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage of Self-Attention\n","d_model = embedding_dim\n","num_heads = 4\n","num_groups = 2\n","\n","grp_attention = GroupedMultiQueryAttention(d_model, num_heads,num_groups)\n","attention_output = [grp_attention(embeddings_with_pos[0].unsqueeze(0)) ]\n","\n","# Print the attention output\n","print(\"grp-Attention Output:\\n\", attention_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HwslFf__QetZ","executionInfo":{"status":"ok","timestamp":1725298869020,"user_tz":-330,"elapsed":23,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"c8ad151f-0f44-4f6f-f463-d64a3113b80f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["grp-Attention Output:\n"," [tensor([[[-0.0975,  0.4815, -0.2181, -0.0645,  0.6781, -0.1469,  0.2447,\n","           0.5530],\n","         [-0.2061,  0.4858, -0.2481, -0.1751,  0.7303, -0.1108,  0.2562,\n","           0.5559],\n","         [-0.1628,  0.3477, -0.1717, -0.1807,  0.6270, -0.0950,  0.3227,\n","           0.4950],\n","         [-0.1191,  0.6156, -0.2846, -0.0753,  0.7080, -0.1475,  0.1373,\n","           0.5293],\n","         [-0.1412,  0.3695, -0.1708, -0.1479,  0.6505, -0.1287,  0.2982,\n","           0.5093],\n","         [-0.1412,  0.3695, -0.1708, -0.1479,  0.6505, -0.1287,  0.2982,\n","           0.5093],\n","         [-0.1412,  0.3695, -0.1708, -0.1479,  0.6505, -0.1287,  0.2982,\n","           0.5093],\n","         [-0.1412,  0.3695, -0.1708, -0.1479,  0.6505, -0.1287,  0.2982,\n","           0.5093],\n","         [-0.1412,  0.3695, -0.1708, -0.1479,  0.6505, -0.1287,  0.2982,\n","           0.5093],\n","         [-0.1412,  0.3695, -0.1708, -0.1479,  0.6505, -0.1287,  0.2982,\n","           0.5093],\n","         [-0.1412,  0.3695, -0.1708, -0.1479,  0.6505, -0.1287,  0.2982,\n","           0.5093]]], grad_fn=<ViewBackward0>)]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"boZLOAm5SDMZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feed-Forward Networks (FFN) in Transformers"],"metadata":{"id":"1p5_wtkDaLJl"}},{"cell_type":"markdown","source":["Feed-Forward Networks (FFN) in Transformers\n","\n","Feed-Forward Networks (FFN) are a crucial component of the Transformer architecture. They are applied after the self-attention mechanism in each encoder and decoder layer. The FFN helps in introducing non-linearity and increasing the model's capacity to learn complex patterns.\n","\n","Architecture of FFN:\n","1. Input layer: Takes in the output from the self-attention layer (dimension: d_model)\n","2. First linear transformation: Expands the input to a higher dimension (d_ff, typically 4 times d_model)\n","3. Activation function: Usually ReLU (Rectified Linear Unit)\n","4. Second linear transformation: Projects back to the original dimension (d_model)\n","5. Residual connection: The output is added to the input\n","6. Layer normalization: Applied to the sum of the residual connection\n","\n","The FFN can be represented as:\n","\n","FFN(x) = max(0, xW1 + b1)W2 + b2\n","\n","Where W1, W2 are weight matrices, and b1, b2 are bias vectors.\n","\n","This architecture allows the Transformer to process information across all positions in the sequence, complementing the global dependencies captured by the self-attention mechanism.\n"],"metadata":{"id":"ZrHpPM5MZhfI"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","# Define the Feed-Forward Network (FFN) with Layer Normalization and Residual Connection\n","class FeedForwardNetwork(nn.Module):\n","    def __init__(self, d_model, d_ff):\n","        super(FeedForwardNetwork, self).__init__()\n","        self.fc1 = nn.Linear(d_model, d_ff)\n","        self.fc2 = nn.Linear(d_ff, d_model)\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        # Apply the feed-forward network with ReLU activation\n","        residual = x\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        # Add residual connection and apply layer normalization\n","        x = self.layer_norm(x + residual)\n","        return x\n","\n","# Example usage in the Transformer block\n","d_ff = 2048  # Feed-forward hidden dimension size (usually larger than d_model)\n","\n","# Initialize the feed-forward network\n","ffn = FeedForwardNetwork(d_model=embedding_dim, d_ff=d_ff)\n","# Pass the output from attention through the feed-forward network\n","ffn_output = [ffn(attn_out) for attn_out in attention_output]\n","\n","# Print the output of the feed-forward network\n","for i, output in enumerate(ffn_output):\n","    print(f\"Feed-Forward Output for Sentence {i+1}:\\n\", output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zfu-BBnCaM22","executionInfo":{"status":"ok","timestamp":1725298869020,"user_tz":-330,"elapsed":19,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"42023fad-e821-4229-ca19-8512ec72bf02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Feed-Forward Output for Sentence 1:\n"," tensor([[[-0.7320,  1.0657, -1.1681, -0.9003,  1.6522, -0.3548, -0.5565,\n","           0.9937],\n","         [-0.8963,  1.0240, -1.0854, -1.0707,  1.6280, -0.1372, -0.4415,\n","           0.9791],\n","         [-0.8567,  0.7859, -0.9993, -1.2850,  1.6923, -0.0752, -0.2852,\n","           1.0232],\n","         [-0.7094,  1.3196, -1.2032, -0.7750,  1.5363, -0.3225, -0.7072,\n","           0.8615],\n","         [-0.8087,  0.8230, -1.0017, -1.1716,  1.7443, -0.2256, -0.3844,\n","           1.0249],\n","         [-0.8087,  0.8230, -1.0017, -1.1716,  1.7443, -0.2256, -0.3844,\n","           1.0249],\n","         [-0.8087,  0.8230, -1.0017, -1.1716,  1.7443, -0.2256, -0.3844,\n","           1.0249],\n","         [-0.8087,  0.8230, -1.0017, -1.1716,  1.7443, -0.2256, -0.3844,\n","           1.0249],\n","         [-0.8087,  0.8230, -1.0017, -1.1716,  1.7443, -0.2256, -0.3844,\n","           1.0249],\n","         [-0.8087,  0.8230, -1.0017, -1.1716,  1.7443, -0.2256, -0.3844,\n","           1.0249],\n","         [-0.8087,  0.8230, -1.0017, -1.1716,  1.7443, -0.2256, -0.3844,\n","           1.0249]]], grad_fn=<NativeLayerNormBackward0>)\n"]}]},{"cell_type":"code","source":["# SwiGLU Activation\n","class SwiGLU(nn.Module):\n","    def forward(self, x):\n","        x1, x2 = x.chunk(2, dim=-1)  # Split the input into two equal parts\n","        return F.silu(x1) * x2  # Apply Swish (SiLU) to one part and multiply with the other\n"],"metadata":{"id":"1PFKU-XQaUBB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","# Define the Feed-Forward Network (FFN) with Layer Normalization and Residual Connection\n","class FeedForwardNetworkSwiGLU(nn.Module):\n","    def __init__(self, d_model, d_ff):\n","        super(FeedForwardNetworkSwiGLU, self).__init__()\n","        self.fc1 = nn.Linear(d_model, d_ff*2)\n","        self.fc2 = nn.Linear(d_ff, d_model)\n","        self.swiGLU = SwiGLU()\n","\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        # Apply the feed-forward network with ReLU activation\n","        residual = x\n","\n","        x = self.fc1(x)\n","        x = self.swiGLU(x)\n","        x = self.fc2(x)\n","\n","        # Add residual connection and apply layer normalization\n","        x = self.layer_norm(x + residual)\n","        return x\n","\n","\n","d_ff = 2048  # Feed-forward hidden dimension size (usually larger than d_model)\n","\n","# Initialize the FFN with SwiGLU\n","ffn_swiglu = FeedForwardNetworkSwiGLU(embedding_dim, d_ff)\n","\n","# Create some example input embeddings (from the previous steps)\n","print(attention_output[0].size())\n","# Pass the input through the FFN\n","output = ffn_swiglu(attention_output[0])\n","\n","print(\"Output from Feed-Forward Network with SwiGLU:\\n\", output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8e_gSQ50b8IL","executionInfo":{"status":"ok","timestamp":1725298869021,"user_tz":-330,"elapsed":10,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"f2c08258-2989-4b5b-d620-076c5bd078ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 11, 8])\n","Output from Feed-Forward Network with SwiGLU:\n"," tensor([[[-0.9865,  0.9154, -1.0368, -0.6631,  1.5247, -1.0750,  0.2606,\n","           1.0608],\n","         [-1.1463,  0.9018, -0.9634, -0.8691,  1.5587, -0.8105,  0.3332,\n","           0.9956],\n","         [-1.1431,  0.6535, -0.8548, -1.0026,  1.5162, -0.8577,  0.6576,\n","           1.0309],\n","         [-0.9485,  1.2198, -1.1016, -0.6020,  1.5146, -0.9444, -0.0580,\n","           0.9201],\n","         [-1.0865,  0.6887, -0.8617, -0.9044,  1.5588, -0.9826,  0.5404,\n","           1.0473],\n","         [-1.0865,  0.6887, -0.8617, -0.9044,  1.5588, -0.9826,  0.5404,\n","           1.0473],\n","         [-1.0865,  0.6887, -0.8617, -0.9044,  1.5588, -0.9826,  0.5404,\n","           1.0473],\n","         [-1.0865,  0.6887, -0.8617, -0.9044,  1.5588, -0.9826,  0.5404,\n","           1.0473],\n","         [-1.0865,  0.6887, -0.8617, -0.9044,  1.5588, -0.9826,  0.5404,\n","           1.0473],\n","         [-1.0865,  0.6887, -0.8617, -0.9044,  1.5588, -0.9826,  0.5404,\n","           1.0473],\n","         [-1.0865,  0.6887, -0.8617, -0.9044,  1.5588, -0.9826,  0.5404,\n","           1.0473]]], grad_fn=<NativeLayerNormBackward0>)\n"]}]},{"cell_type":"markdown","source":["# Full Implementation till EncoderLayer"],"metadata":{"id":"vh8u1fvpR1Lt"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import string\n","from collections import defaultdict\n","\n","# Step 1: Prepare the Data\n","data = [\n","    \"I love machine learning.\",\n","    \"Artificial intelligence is the future.\",\n","    \"Transformers are powerful models.\",\n","    \"Natural language processing is fascinating.\",\n","    \"Deep learning enables great advances in AI.\",\n","    \"PyTorch is a popular deep learning library.\",\n","    \"GPT models are widely used in NLP.\",\n","    \"Neural networks are the backbone of deep learning.\",\n","    \"Embedding layers convert tokens to vectors.\",\n","    \"Attention mechanisms help models focus on important parts of the input.\"\n","]\n","\n","# Step 2: Simple Tokenization\n","def simple_tokenize(sentence):\n","    sentence = sentence.translate(str.maketrans('', '', string.punctuation)).lower()\n","    return sentence.split()\n","\n","tokenized_data = [simple_tokenize(sentence) for sentence in data]\n","\n","# Step 3: Build a Vocabulary with Padding and Unknown Tokens\n","vocab = defaultdict(lambda: len(vocab))\n","PAD = vocab[\"<PAD>\"]\n","UNK = vocab[\"<UNK>\"]\n","\n","# Populate the vocabulary with the tokenized data\n","for tokens in tokenized_data:\n","    for token in tokens:\n","        _ = vocab[token]\n","\n","# Convert defaultdict to a regular dictionary\n","vocab = dict(vocab)\n","\n","\n","\n","\n","\n","\n","\n","# Step 4: Convert Tokens to IDs and Pad Sequences\n","def tokens_to_ids(tokens, vocab):\n","    return [vocab.get(token, UNK) for token in tokens]\n","\n","max_len = max(len(tokens) for tokens in tokenized_data)\n","token_ids_data = [tokens_to_ids(tokens, vocab) for tokens in tokenized_data]\n","padded_token_ids_data = [token_ids + [PAD] * (max_len - len(token_ids)) for token_ids in token_ids_data]\n","\n","\n","\n","\n","\n","\n","\n","# Step 5: Create the Embedding Layer\n","vocab_size = len(vocab)\n","embedding_dim = 8\n","embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n","\n","# Convert Padded Token IDs to Embeddings\n","token_ids_tensor = torch.tensor(padded_token_ids_data)\n","print(token_ids_tensor)\n","\n","embeddings = embedding_layer(token_ids_tensor)\n","\n","\n","\n","\n","\n","# Step 6: Positional Encoding (Sinusoidal)\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len):\n","        super(PositionalEncoding, self).__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.pe = pe.unsqueeze(0).transpose(0, 1)\n","\n","    def forward(self, x):\n","        return x + self.pe[:x.size(0), :]\n","\n","\n","\n","\n","pos_encoder = PositionalEncoding(embedding_dim, max_len)\n","embeddings_with_pos = pos_encoder(embeddings)\n","\n","# Step 7: Self-Attention Mechanism\n","class MultiHeadSelfAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        assert d_model % num_heads == 0\n","        self.num_heads = num_heads\n","        self.depth = d_model // num_heads\n","\n","        self.w_q = nn.Linear(d_model, d_model)\n","        self.w_k = nn.Linear(d_model, d_model)\n","        self.w_v = nn.Linear(d_model, d_model)\n","        self.w_o = nn.Linear(d_model, d_model)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        q = self.w_q(x).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n","        k = self.w_k(x).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n","        v = self.w_v(x).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n","\n","        scores = torch.matmul(q, k.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n","        attention_weights = nn.functional.softmax(scores, dim=-1)\n","        attention_output = torch.matmul(attention_weights, v)\n","\n","        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.depth)\n","        output = self.w_o(attention_output)\n","        return output\n","\n","num_heads = 4\n","attention = MultiHeadSelfAttention(embedding_dim, num_heads)\n","attention_output = attention(embeddings_with_pos)\n","\n","\n","\n","\n","\n","\n","# Step 8: Feed-Forward Network with SwiGLU\n","class SwiGLU(nn.Module):\n","    def forward(self, x):\n","        return x * torch.nn.functional.silu(x)\n","\n","class FeedForwardNetworkSwiGLU(nn.Module):\n","    def __init__(self, d_model, d_ff):\n","        super(FeedForwardNetworkSwiGLU, self).__init__()\n","        self.fc1 = nn.Linear(d_model, d_ff * 2)\n","        self.fc2 = nn.Linear(d_ff, d_model)\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        residual = x\n","        x = self.fc1(x)\n","\n","        # SwiGLU: Split the output into two parts\n","        x1, x2 = x.chunk(2, dim=-1)\n","\n","        # Apply SiLU activation function to one part\n","        x = x1 * torch.nn.functional.silu(x2)\n","\n","        # Apply the second linear transformation\n","        x = self.fc2(x)\n","\n","        # Add the residual connection and apply layer normalization\n","        x = self.layer_norm(x + residual)\n","        return x\n","\n","d_ff = 32\n","ffn_swiglu = FeedForwardNetworkSwiGLU(embedding_dim, d_ff)\n","ffn_output = ffn_swiglu(attention_output)\n","\n","# Step 9: Combine into an Encoder Layer\n","class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attention = MultiHeadSelfAttention(d_model, num_heads)\n","        self.ffn = FeedForwardNetworkSwiGLU(d_model, d_ff)\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        # Apply self-attention and add residual connection\n","        attn_output = self.self_attention(x)\n","        x = self.layer_norm(x + attn_output)\n","\n","        # Apply feed-forward network and add residual connection\n","        ffn_output = self.ffn(x)\n","        x = self.layer_norm(x + ffn_output)\n","        return x\n","\n","# Instantiate and apply the encoder layer\n","encoder_layer = EncoderLayer(embedding_dim, num_heads, d_ff)\n","encoder_output = encoder_layer(embeddings_with_pos)\n","\n","# Print the final encoder output\n","print(\"Final Encoder Output:\\n\", encoder_output)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"gkydf_s0cQWh","executionInfo":{"status":"ok","timestamp":1725305645166,"user_tz":-330,"elapsed":443,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"24cd5447-cb5d-45ba-ad19-b1f2a50d5ff2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 2,  3,  4,  5,  0,  0,  0,  0,  0,  0,  0],\n","        [ 6,  7,  8,  9, 10,  0,  0,  0,  0,  0,  0],\n","        [11, 12, 13, 14,  0,  0,  0,  0,  0,  0,  0],\n","        [15, 16, 17,  8, 18,  0,  0,  0,  0,  0,  0],\n","        [19,  5, 20, 21, 22, 23, 24,  0,  0,  0,  0],\n","        [25,  8, 26, 27, 19,  5, 28,  0,  0,  0,  0],\n","        [29, 14, 12, 30, 31, 23, 32,  0,  0,  0,  0],\n","        [33, 34, 12,  9, 35, 36, 19,  5,  0,  0,  0],\n","        [37, 38, 39, 40, 41, 42,  0,  0,  0,  0,  0],\n","        [43, 44, 45, 14, 46, 47, 48, 49, 36,  9, 50]])\n","Final Encoder Output:\n"," tensor([[[-0.3615,  2.0050, -0.6459, -1.4127,  0.7535, -0.4096, -0.5554,\n","           0.6265],\n","         [-1.1762, -0.4951, -0.9052,  0.9795, -1.2819,  0.8444,  0.7284,\n","           1.3061],\n","         [-1.1402,  0.9215,  0.0189,  1.5088, -0.6020,  0.1075, -1.6047,\n","           0.7903],\n","         [ 0.1036,  0.5697, -1.5493,  0.8538, -1.4637,  1.3849,  0.5352,\n","          -0.4343],\n","         [-1.2121,  1.6602, -0.1858, -0.9283,  0.1794,  1.4658, -0.8199,\n","          -0.1593],\n","         [-1.2121,  1.6602, -0.1858, -0.9283,  0.1794,  1.4658, -0.8199,\n","          -0.1593],\n","         [-1.2121,  1.6602, -0.1858, -0.9283,  0.1794,  1.4658, -0.8199,\n","          -0.1593],\n","         [-1.2121,  1.6602, -0.1858, -0.9283,  0.1794,  1.4658, -0.8199,\n","          -0.1593],\n","         [-1.2121,  1.6602, -0.1858, -0.9283,  0.1794,  1.4658, -0.8199,\n","          -0.1593],\n","         [-1.2121,  1.6602, -0.1858, -0.9283,  0.1794,  1.4658, -0.8199,\n","          -0.1593],\n","         [-1.2121,  1.6602, -0.1858, -0.9283,  0.1794,  1.4658, -0.8199,\n","          -0.1593]],\n","\n","        [[-1.4457, -0.5629,  0.9634,  1.7139, -0.3783,  0.2440, -1.0985,\n","           0.5641],\n","         [ 0.6430,  0.7532, -0.9358, -0.0711, -1.1791,  1.6642, -1.3306,\n","           0.4562],\n","         [ 0.8417, -1.1282, -0.9219, -1.0129, -0.9170,  0.9345,  1.0992,\n","           1.1046],\n","         [-0.7785,  0.3878, -0.7109,  1.6004, -0.8793, -0.4258, -0.8003,\n","           1.6068],\n","         [-1.1966, -1.0952,  0.1442, -0.4166, -0.8783,  1.1520,  0.6696,\n","           1.6209],\n","         [-0.2593,  1.3888, -0.1048, -1.1345, -0.0221,  1.7287, -1.2687,\n","          -0.3281],\n","         [-0.2593,  1.3888, -0.1048, -1.1345, -0.0221,  1.7287, -1.2687,\n","          -0.3281],\n","         [-0.2593,  1.3888, -0.1048, -1.1345, -0.0221,  1.7287, -1.2687,\n","          -0.3281],\n","         [-0.2593,  1.3888, -0.1048, -1.1345, -0.0221,  1.7287, -1.2687,\n","          -0.3281],\n","         [-0.2593,  1.3888, -0.1048, -1.1345, -0.0221,  1.7287, -1.2687,\n","          -0.3281],\n","         [-0.2593,  1.3888, -0.1048, -1.1345, -0.0221,  1.7287, -1.2687,\n","          -0.3281]],\n","\n","        [[-0.0873,  0.7041, -0.2794,  0.5878,  0.7987, -0.4892, -2.2656,\n","           1.0309],\n","         [-0.4434, -0.0925,  2.3164, -0.9279, -0.0747, -0.7749,  0.6920,\n","          -0.6950],\n","         [-0.1543,  1.0193, -1.4254, -0.5882,  0.7246,  1.7684, -0.6306,\n","          -0.7139],\n","         [-0.3625, -0.6953,  0.8797,  1.9630, -1.3993, -0.8307,  0.2954,\n","           0.1498],\n","         [ 0.0721,  0.1816,  0.3772, -1.1848, -0.0746,  2.2109, -1.1587,\n","          -0.4236],\n","         [ 0.0721,  0.1816,  0.3772, -1.1848, -0.0746,  2.2109, -1.1587,\n","          -0.4236],\n","         [ 0.0721,  0.1816,  0.3772, -1.1848, -0.0746,  2.2109, -1.1587,\n","          -0.4236],\n","         [ 0.0721,  0.1816,  0.3772, -1.1848, -0.0746,  2.2109, -1.1587,\n","          -0.4236],\n","         [ 0.0721,  0.1816,  0.3772, -1.1848, -0.0746,  2.2109, -1.1587,\n","          -0.4236],\n","         [ 0.0721,  0.1816,  0.3772, -1.1848, -0.0746,  2.2109, -1.1587,\n","          -0.4236],\n","         [ 0.0721,  0.1816,  0.3772, -1.1848, -0.0746,  2.2109, -1.1587,\n","          -0.4236]],\n","\n","        [[ 0.3598, -1.8021,  0.7796,  1.1645, -0.7809, -0.9903,  0.2722,\n","           0.9972],\n","         [ 1.0211, -1.4854,  0.7323,  0.4202, -1.7938, -0.1459,  0.5370,\n","           0.7146],\n","         [-1.3711,  0.7700, -0.0568,  0.2123, -0.2470,  0.9703, -1.6285,\n","           1.3507],\n","         [ 0.4805, -1.9236, -0.4372, -0.5737, -0.6150,  0.8812,  1.0774,\n","           1.1105],\n","         [-0.2043, -1.5158, -0.4853, -0.1662,  0.4019,  0.2540, -0.5021,\n","           2.2179],\n","         [-0.8125, -0.5375,  0.7090, -0.7993,  0.2810,  2.1269, -1.1313,\n","           0.1638],\n","         [-0.8125, -0.5375,  0.7090, -0.7993,  0.2810,  2.1269, -1.1313,\n","           0.1638],\n","         [-0.8125, -0.5375,  0.7090, -0.7993,  0.2810,  2.1269, -1.1313,\n","           0.1638],\n","         [-0.8125, -0.5375,  0.7090, -0.7993,  0.2810,  2.1269, -1.1313,\n","           0.1638],\n","         [-0.8125, -0.5375,  0.7090, -0.7993,  0.2810,  2.1269, -1.1313,\n","           0.1638],\n","         [-0.8125, -0.5375,  0.7090, -0.7993,  0.2810,  2.1269, -1.1313,\n","           0.1638]],\n","\n","        [[-1.9289, -0.5345,  0.8045,  1.4444, -0.2792, -0.0344, -0.4577,\n","           0.9857],\n","         [-0.3292, -1.2087, -0.6540,  1.6503, -1.2466,  1.1394,  0.6536,\n","          -0.0048],\n","         [-0.7838,  1.2257, -1.0372,  1.9819, -0.6566,  0.1810, -0.4796,\n","          -0.4313],\n","         [ 0.1629, -1.2581,  0.4064,  1.9182, -0.2677, -0.8480, -0.9919,\n","           0.8783],\n","         [ 0.0765, -1.9774,  1.2973,  0.6982,  0.8243,  0.4560, -0.8885,\n","          -0.4865],\n","         [ 0.0523, -0.2734,  0.0277, -0.5119, -0.1656, -0.4397, -1.1586,\n","           2.4691],\n","         [-1.3005, -1.2360, -0.9720,  1.6110,  0.4479,  0.4769,  0.8985,\n","           0.0742],\n","         [-1.8988,  0.2814,  0.6798, -0.4559,  0.4401,  1.5595, -0.9495,\n","           0.3434],\n","         [-1.8988,  0.2814,  0.6798, -0.4559,  0.4401,  1.5595, -0.9495,\n","           0.3434],\n","         [-1.8988,  0.2814,  0.6798, -0.4559,  0.4401,  1.5595, -0.9495,\n","           0.3434],\n","         [-1.8988,  0.2814,  0.6798, -0.4559,  0.4401,  1.5595, -0.9495,\n","           0.3434]],\n","\n","        [[-2.1421,  1.2202, -0.2270,  0.3847,  0.4969, -0.4704, -0.3321,\n","           1.0699],\n","         [-0.7713, -1.1212, -0.4571, -0.8256, -0.5853,  0.9433,  1.2398,\n","           1.5773],\n","         [-1.4179,  0.5258,  0.4159, -0.8811, -0.5420,  1.9363,  0.5815,\n","          -0.6185],\n","         [-0.7443, -0.2462,  0.2211, -0.8091,  0.7103,  0.5112, -1.5321,\n","           1.8891],\n","         [-2.2466,  0.2874,  0.7192,  1.0446, -0.1534, -0.0271, -0.5759,\n","           0.9518],\n","         [-1.1432, -0.0874, -0.8089,  1.4937, -1.3728,  1.2777,  0.5170,\n","           0.1239],\n","         [-2.0510, -0.4523,  0.6215,  0.4428, -0.6048,  1.3559, -0.1877,\n","           0.8757],\n","         [-1.9636,  1.0393,  0.4798, -0.6305,  0.4742,  1.1652, -0.8715,\n","           0.3071],\n","         [-1.9636,  1.0393,  0.4798, -0.6305,  0.4742,  1.1652, -0.8715,\n","           0.3071],\n","         [-1.9636,  1.0393,  0.4798, -0.6305,  0.4742,  1.1652, -0.8715,\n","           0.3071],\n","         [-1.9636,  1.0393,  0.4798, -0.6305,  0.4742,  1.1652, -0.8715,\n","           0.3071]],\n","\n","        [[-1.2960,  0.5168, -0.0450,  0.9196, -0.2120, -1.4643, -0.1494,\n","           1.7303],\n","         [-1.2540,  0.5402,  1.1270,  1.5370, -1.1325, -1.0672, -0.0343,\n","           0.2839],\n","         [-1.4477,  1.4339,  1.5487, -0.8768,  0.2685, -0.7725, -0.0631,\n","          -0.0911],\n","         [-1.3549,  1.7737, -0.3593, -0.3124, -0.0293,  0.5733, -1.2455,\n","           0.9543],\n","         [-0.2757, -0.1355,  1.4865, -2.1803,  0.1831,  0.4383, -0.3058,\n","           0.7894],\n","         [ 0.2110,  1.4680, -0.0982, -1.2280, -0.0335, -0.5287, -1.3060,\n","           1.5154],\n","         [-1.0780,  0.3916,  0.2731, -1.5695, -0.0481,  1.7687, -0.5695,\n","           0.8317],\n","         [-1.2117,  1.6850,  0.4853, -1.1036,  0.2669,  0.9367, -1.1336,\n","           0.0750],\n","         [-1.2117,  1.6850,  0.4853, -1.1036,  0.2669,  0.9367, -1.1336,\n","           0.0750],\n","         [-1.2117,  1.6850,  0.4853, -1.1036,  0.2669,  0.9367, -1.1336,\n","           0.0750],\n","         [-1.2117,  1.6850,  0.4853, -1.1036,  0.2669,  0.9367, -1.1336,\n","           0.0750]],\n","\n","        [[-0.5079, -1.1402,  0.1762,  1.9766,  0.9005, -0.6856, -1.0539,\n","           0.3344],\n","         [-0.0241, -0.4871,  0.5688,  1.8085,  0.4930, -0.9531, -1.7100,\n","           0.3039],\n","         [-0.5288,  1.3412,  1.8625, -1.0656, -0.1133, -0.9936,  0.0577,\n","          -0.5600],\n","         [-0.8598,  0.7824, -0.1365,  1.3628, -0.9680, -0.7259, -0.9845,\n","           1.5296],\n","         [-0.0295, -1.0979,  0.7466,  2.1841, -0.8859, -0.2568,  0.1150,\n","          -0.7757],\n","         [ 1.5295, -0.3146, -0.8283, -0.8998,  0.4762,  1.4495, -0.0978,\n","          -1.3148],\n","         [-1.0154,  1.0554,  1.3073,  0.8550, -1.0236, -0.5191, -1.2994,\n","           0.6397],\n","         [ 1.4237,  0.3260, -0.7441,  0.5306, -1.8820,  0.9212,  0.2001,\n","          -0.7754],\n","         [-0.2547,  1.6179,  0.5821, -1.3915, -0.0100,  1.1210, -1.2795,\n","          -0.3854],\n","         [-0.2547,  1.6179,  0.5821, -1.3915, -0.0100,  1.1210, -1.2795,\n","          -0.3854],\n","         [-0.2547,  1.6179,  0.5821, -1.3915, -0.0100,  1.1210, -1.2795,\n","          -0.3854]],\n","\n","        [[ 0.5608, -0.4846,  0.9633, -0.6100,  0.4716,  1.6392, -1.3548,\n","          -1.1856],\n","         [-0.3846,  0.0705, -0.1727,  1.3300, -1.3306,  0.3635, -1.3765,\n","           1.5004],\n","         [ 1.3116,  0.8904,  0.1651,  0.3926, -2.0936, -0.5742, -0.5883,\n","           0.4964],\n","         [-0.8383, -0.5593,  1.0034, -0.5288, -1.0687,  1.2906, -0.7994,\n","           1.5004],\n","         [-1.4896,  0.1824,  0.0127,  1.1008,  1.6963,  0.1751, -1.1711,\n","          -0.5065],\n","         [ 0.2182,  0.4452,  1.2044,  1.2274, -1.7772, -0.2392, -1.2463,\n","           0.1676],\n","         [ 0.2269,  0.3778,  0.9140, -1.6102, -0.0184,  1.7066, -1.1067,\n","          -0.4899],\n","         [ 0.2269,  0.3778,  0.9140, -1.6102, -0.0184,  1.7066, -1.1067,\n","          -0.4899],\n","         [ 0.2269,  0.3778,  0.9140, -1.6102, -0.0184,  1.7066, -1.1067,\n","          -0.4899],\n","         [ 0.2269,  0.3778,  0.9140, -1.6102, -0.0184,  1.7066, -1.1067,\n","          -0.4899],\n","         [ 0.2269,  0.3778,  0.9140, -1.6102, -0.0184,  1.7066, -1.1067,\n","          -0.4899]],\n","\n","        [[ 0.7188, -1.4221, -0.2966,  0.0178,  0.1628,  0.1006, -1.2340,\n","           1.9527],\n","         [-0.0193, -0.1290,  1.3380,  0.5940, -0.0460, -0.9807, -1.8934,\n","           1.1363],\n","         [ 1.5382, -1.6220,  0.8668,  0.4750, -1.0977,  0.2072, -0.7798,\n","           0.4124],\n","         [-0.3386, -0.9369,  1.3295,  1.6306, -1.2725, -0.9122,  0.2010,\n","           0.2992],\n","         [ 0.8316, -1.0618,  1.6522, -0.5667, -0.1584, -0.3496, -1.3824,\n","           1.0352],\n","         [ 0.1112, -0.6942,  2.0500, -1.1333,  0.5335, -0.3939, -1.0930,\n","           0.6197],\n","         [-0.4679, -1.0569,  1.3643,  0.6856, -0.3252, -0.0678, -1.5176,\n","           1.3854],\n","         [ 0.7777, -1.4545, -0.2424,  0.6952, -1.5688,  0.1173,  0.1825,\n","           1.4931],\n","         [ 1.1930, -2.2516,  0.0766, -0.3366,  0.4352,  1.0441,  0.1377,\n","          -0.2985],\n","         [-0.5837, -0.9111,  0.2366,  1.4631, -0.7612, -0.5426, -0.7066,\n","           1.8055],\n","         [ 0.1044,  0.7921, -1.2251,  0.1721,  2.0818, -0.2646, -0.6748,\n","          -0.9858]]], grad_fn=<NativeLayerNormBackward0>)\n"]}]},{"cell_type":"code","source":["## Connecting multiple encoderlayers to make Encoder\n","\n","\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_len):\n","        super(TransformerEncoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model, max_len)\n","        self.encoder_layers = nn.ModuleList(\n","            [EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n","        )\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        # Apply embedding and positional encoding\n","        x = self.embedding(x)\n","        x = self.pos_encoder(x)\n","\n","        # Pass through each encoder layer\n","        for encoder_layer in self.encoder_layers:\n","            x = encoder_layer(x)\n","\n","        # Apply final layer normalization\n","        x = self.layer_norm(x)\n","        return x"],"metadata":{"id":"BJa9pOnvPsiP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage:\n","num_layers = 6\n","encoder = TransformerEncoder(embedding_dim, num_heads, d_ff, num_layers, vocab_size, max_len)\n","\n","# Convert input data to tensor format\n","input_tensor = torch.tensor(padded_token_ids_data)\n","\n","# Pass input through the full encoder\n","encoded_output = encoder(input_tensor)\n","\n","print(\"Encoded Output:\\n\", encoded_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"FpxnMhZ1Ow-a","executionInfo":{"status":"ok","timestamp":1725305666949,"user_tz":-330,"elapsed":444,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"e71cc685-e612-4cd8-f2d4-551d5411d2f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoded Output:\n"," tensor([[[-2.5182e+00,  3.1315e-01,  1.3221e-01,  4.9132e-01,  1.0794e+00,\n","           2.3023e-01, -1.6487e-02,  2.8838e-01],\n","         [-5.0042e-01,  3.8617e-01, -1.2510e-01, -1.9037e-01, -2.1190e+00,\n","           1.4153e-01,  9.3809e-01,  1.4691e+00],\n","         [-5.1613e-01,  1.4354e+00, -4.9616e-01,  1.3677e+00, -6.6916e-01,\n","           9.4237e-01, -1.2448e+00, -8.1924e-01],\n","         [ 9.0937e-01,  7.5987e-02, -3.1832e-01,  1.1996e+00, -1.3432e+00,\n","          -1.6139e+00, -1.2916e-02,  1.1034e+00],\n","         [ 7.3825e-01,  1.0895e+00, -1.0489e+00,  8.0652e-01, -1.9039e+00,\n","          -6.4912e-01,  5.2264e-01,  4.4491e-01],\n","         [ 7.3825e-01,  1.0895e+00, -1.0489e+00,  8.0652e-01, -1.9039e+00,\n","          -6.4912e-01,  5.2264e-01,  4.4491e-01],\n","         [ 7.3825e-01,  1.0895e+00, -1.0489e+00,  8.0652e-01, -1.9039e+00,\n","          -6.4912e-01,  5.2264e-01,  4.4491e-01],\n","         [ 7.3825e-01,  1.0895e+00, -1.0489e+00,  8.0652e-01, -1.9039e+00,\n","          -6.4912e-01,  5.2264e-01,  4.4491e-01],\n","         [ 7.3825e-01,  1.0895e+00, -1.0489e+00,  8.0652e-01, -1.9039e+00,\n","          -6.4912e-01,  5.2264e-01,  4.4491e-01],\n","         [ 7.3825e-01,  1.0895e+00, -1.0489e+00,  8.0652e-01, -1.9039e+00,\n","          -6.4912e-01,  5.2264e-01,  4.4491e-01],\n","         [ 7.3825e-01,  1.0895e+00, -1.0489e+00,  8.0652e-01, -1.9039e+00,\n","          -6.4912e-01,  5.2264e-01,  4.4491e-01]],\n","\n","        [[-7.2010e-03, -6.6570e-01, -4.5459e-01,  1.8260e+00, -1.8125e+00,\n","           1.3492e-02,  3.0045e-01,  8.0001e-01],\n","         [ 1.5217e+00,  9.3806e-01,  3.3771e-01, -9.4020e-01, -1.5294e+00,\n","          -4.7331e-01, -7.1241e-01,  8.5787e-01],\n","         [ 5.1345e-01,  9.0304e-01, -1.1681e+00, -1.3131e+00,  9.4173e-02,\n","          -6.0857e-01, -2.6083e-01,  1.8399e+00],\n","         [-1.8705e-01,  1.9391e+00, -5.3330e-01, -4.6427e-05, -1.6045e+00,\n","          -6.3799e-01,  5.6450e-02,  9.6733e-01],\n","         [ 2.1223e-01,  1.7591e+00,  1.1791e+00,  9.7361e-02, -2.3244e-01,\n","          -1.3423e+00, -1.1568e+00, -5.1631e-01],\n","         [ 1.3674e+00,  5.7300e-01, -1.3889e+00,  2.7628e-01, -1.6193e+00,\n","          -4.8993e-01,  8.7889e-01,  4.0245e-01],\n","         [ 1.3674e+00,  5.7300e-01, -1.3889e+00,  2.7628e-01, -1.6193e+00,\n","          -4.8993e-01,  8.7889e-01,  4.0245e-01],\n","         [ 1.3674e+00,  5.7300e-01, -1.3889e+00,  2.7628e-01, -1.6193e+00,\n","          -4.8993e-01,  8.7889e-01,  4.0245e-01],\n","         [ 1.3674e+00,  5.7300e-01, -1.3889e+00,  2.7628e-01, -1.6193e+00,\n","          -4.8993e-01,  8.7889e-01,  4.0245e-01],\n","         [ 1.3674e+00,  5.7300e-01, -1.3889e+00,  2.7628e-01, -1.6193e+00,\n","          -4.8993e-01,  8.7889e-01,  4.0245e-01],\n","         [ 1.3674e+00,  5.7300e-01, -1.3889e+00,  2.7628e-01, -1.6193e+00,\n","          -4.8993e-01,  8.7889e-01,  4.0245e-01]],\n","\n","        [[ 2.3900e+00, -3.6692e-01, -5.8988e-01, -1.1699e+00, -6.1470e-01,\n","           3.5181e-02,  9.6017e-02,  2.2018e-01],\n","         [ 1.2897e+00, -5.9932e-01, -1.5149e+00, -7.3332e-01,  5.1229e-01,\n","          -7.7458e-01,  1.4695e+00,  3.5063e-01],\n","         [ 1.2926e+00,  4.9685e-01,  9.1259e-01, -3.1214e-01,  3.2302e-01,\n","          -7.5630e-01,  1.5337e-01, -2.1100e+00],\n","         [ 1.2737e+00,  1.5749e-01,  7.8190e-02, -1.3232e+00, -1.0752e+00,\n","          -1.1484e+00,  1.1699e+00,  8.6753e-01],\n","         [ 1.3127e+00, -6.7927e-02, -1.1082e+00, -5.4032e-02, -1.4316e+00,\n","          -5.0607e-01,  1.6398e+00,  2.1535e-01],\n","         [ 1.3127e+00, -6.7927e-02, -1.1082e+00, -5.4032e-02, -1.4316e+00,\n","          -5.0607e-01,  1.6398e+00,  2.1535e-01],\n","         [ 1.3127e+00, -6.7927e-02, -1.1082e+00, -5.4032e-02, -1.4316e+00,\n","          -5.0607e-01,  1.6398e+00,  2.1535e-01],\n","         [ 1.3127e+00, -6.7927e-02, -1.1082e+00, -5.4032e-02, -1.4316e+00,\n","          -5.0607e-01,  1.6398e+00,  2.1535e-01],\n","         [ 1.3127e+00, -6.7927e-02, -1.1082e+00, -5.4032e-02, -1.4316e+00,\n","          -5.0607e-01,  1.6398e+00,  2.1535e-01],\n","         [ 1.3127e+00, -6.7927e-02, -1.1082e+00, -5.4032e-02, -1.4316e+00,\n","          -5.0607e-01,  1.6398e+00,  2.1535e-01],\n","         [ 1.3127e+00, -6.7927e-02, -1.1082e+00, -5.4032e-02, -1.4316e+00,\n","          -5.0607e-01,  1.6398e+00,  2.1535e-01]],\n","\n","        [[-2.2042e-01, -5.2007e-01, -6.7642e-01,  2.0218e+00,  6.1127e-01,\n","           2.2603e-01,  1.9289e-01, -1.6351e+00],\n","         [ 5.7565e-01,  1.8434e+00, -8.9668e-01, -1.4410e+00,  1.4186e-01,\n","           7.3971e-01, -9.0513e-01, -5.7796e-02],\n","         [-1.3750e-01,  2.7667e-01, -1.5267e+00, -9.2780e-01, -7.9054e-01,\n","           1.7820e+00,  5.2793e-01,  7.9598e-01],\n","         [ 2.1889e-01,  1.1715e-01, -1.0478e+00, -1.2726e+00,  2.0149e-01,\n","          -6.0031e-01,  1.9651e-01,  2.1866e+00],\n","         [-1.5967e-01, -6.0303e-01, -5.1486e-01,  1.4887e+00,  7.5998e-01,\n","          -1.9552e+00,  8.4210e-01,  1.4195e-01],\n","         [ 1.0476e+00, -4.7421e-01, -9.3119e-01,  8.3680e-01, -1.6781e+00,\n","          -5.0337e-01,  1.3952e+00,  3.0726e-01],\n","         [ 1.0476e+00, -4.7421e-01, -9.3119e-01,  8.3680e-01, -1.6781e+00,\n","          -5.0337e-01,  1.3952e+00,  3.0726e-01],\n","         [ 1.0476e+00, -4.7421e-01, -9.3119e-01,  8.3680e-01, -1.6781e+00,\n","          -5.0337e-01,  1.3952e+00,  3.0726e-01],\n","         [ 1.0476e+00, -4.7421e-01, -9.3119e-01,  8.3680e-01, -1.6781e+00,\n","          -5.0337e-01,  1.3952e+00,  3.0726e-01],\n","         [ 1.0476e+00, -4.7421e-01, -9.3119e-01,  8.3680e-01, -1.6781e+00,\n","          -5.0337e-01,  1.3952e+00,  3.0726e-01],\n","         [ 1.0476e+00, -4.7421e-01, -9.3119e-01,  8.3680e-01, -1.6781e+00,\n","          -5.0337e-01,  1.3952e+00,  3.0726e-01]],\n","\n","        [[-3.4879e-01, -1.3851e-01, -4.5973e-01,  1.6206e+00, -1.5501e+00,\n","          -9.5863e-01,  8.2878e-01,  1.0064e+00],\n","         [ 6.0984e-01, -1.0534e+00, -3.2023e-03,  1.4169e+00, -8.9986e-01,\n","          -1.5732e+00,  5.3114e-01,  9.7169e-01],\n","         [ 1.4725e+00, -4.7257e-01, -1.1953e+00,  5.4587e-01,  3.1268e-01,\n","           1.5182e-01, -1.7168e+00,  9.0179e-01],\n","         [-3.6792e-01, -6.1990e-01, -6.1012e-01,  2.1666e+00, -7.8096e-01,\n","           1.0508e+00, -2.0991e-03, -8.3643e-01],\n","         [-1.8358e+00, -2.3049e-01,  1.1906e+00, -1.8970e-01, -4.0298e-02,\n","           9.9508e-01,  1.0862e+00, -9.7552e-01],\n","         [ 1.0954e+00, -5.2157e-01, -1.3219e+00,  1.2174e+00, -6.9913e-01,\n","           1.2727e+00,  4.7004e-02, -1.0899e+00],\n","         [-6.0807e-01,  2.6981e-01, -1.5567e+00, -3.0296e-01,  2.1491e+00,\n","          -4.1604e-01,  4.9965e-01, -3.4784e-02],\n","         [ 2.3552e-01, -9.5860e-02, -9.9182e-01,  1.0988e+00, -1.8537e+00,\n","          -2.6185e-01,  1.4309e+00,  4.3808e-01],\n","         [ 2.3552e-01, -9.5860e-02, -9.9182e-01,  1.0988e+00, -1.8537e+00,\n","          -2.6185e-01,  1.4309e+00,  4.3808e-01],\n","         [ 2.3552e-01, -9.5860e-02, -9.9182e-01,  1.0988e+00, -1.8537e+00,\n","          -2.6185e-01,  1.4309e+00,  4.3808e-01],\n","         [ 2.3552e-01, -9.5860e-02, -9.9182e-01,  1.0988e+00, -1.8537e+00,\n","          -2.6185e-01,  1.4309e+00,  4.3808e-01]],\n","\n","        [[ 1.3010e+00, -5.9591e-01, -5.0469e-01,  1.8002e+00, -1.2771e+00,\n","          -1.7430e-01, -8.4235e-01,  2.9317e-01],\n","         [-4.9556e-01,  1.0240e+00, -1.1222e+00, -9.4017e-01,  1.7574e-01,\n","          -4.0234e-01, -3.0724e-01,  2.0677e+00],\n","         [ 1.2354e+00,  2.6464e-01, -6.2063e-01, -3.5141e-01, -5.5924e-01,\n","          -1.3735e+00, -4.6153e-01,  1.8662e+00],\n","         [ 1.1423e-01,  1.7673e+00, -1.1175e+00,  8.4228e-01, -7.5306e-01,\n","           6.6519e-01, -1.3687e+00, -1.4983e-01],\n","         [-3.1480e-01,  9.0759e-01, -6.2957e-01,  1.5203e+00, -1.5204e+00,\n","          -1.0775e+00,  1.8557e-01,  9.2870e-01],\n","         [ 5.8650e-01, -6.2795e-01, -2.8127e-01,  1.5448e+00, -8.7105e-01,\n","          -1.6163e+00,  7.3960e-02,  1.1914e+00],\n","         [ 1.1988e+00,  8.8195e-01, -1.2114e-01,  1.5233e+00, -8.7844e-01,\n","          -1.3464e+00, -8.2126e-01, -4.3679e-01],\n","         [ 1.6822e-01,  5.3624e-01, -1.1466e+00,  1.2594e+00, -1.9089e+00,\n","          -3.1823e-01,  8.5298e-01,  5.5696e-01],\n","         [ 1.6822e-01,  5.3624e-01, -1.1466e+00,  1.2594e+00, -1.9089e+00,\n","          -3.1823e-01,  8.5298e-01,  5.5696e-01],\n","         [ 1.6822e-01,  5.3624e-01, -1.1466e+00,  1.2594e+00, -1.9089e+00,\n","          -3.1823e-01,  8.5298e-01,  5.5696e-01],\n","         [ 1.6822e-01,  5.3624e-01, -1.1466e+00,  1.2594e+00, -1.9089e+00,\n","          -3.1823e-01,  8.5298e-01,  5.5696e-01]],\n","\n","        [[ 3.0492e-01,  7.8615e-01, -3.8146e-01, -1.1840e+00,  2.0337e-01,\n","          -1.6343e+00,  1.7342e-01,  1.7319e+00],\n","         [ 5.8657e-01,  1.1854e+00,  1.5369e-01, -5.3625e-01, -1.6745e+00,\n","          -1.2219e+00,  2.4997e-01,  1.2570e+00],\n","         [ 6.2753e-01, -4.3920e-01, -2.1988e+00,  4.0569e-01,  1.3358e-01,\n","          -5.6604e-01,  1.0007e+00,  1.0365e+00],\n","         [ 5.5724e-01,  1.1329e+00, -5.3286e-01,  6.8042e-02, -2.2078e+00,\n","           5.6450e-01, -4.3790e-01,  8.5593e-01],\n","         [ 1.0173e-01,  2.0696e+00,  1.1274e-01,  3.9015e-02,  1.6234e-01,\n","          -1.8622e+00, -2.6705e-01, -3.5616e-01],\n","         [ 1.0531e+00,  2.9626e-01, -1.3510e+00,  1.3192e+00, -1.0559e+00,\n","           1.0897e+00, -5.7156e-01, -7.7989e-01],\n","         [ 4.7161e-01,  1.0679e+00, -1.0534e+00, -1.4488e-01, -1.1163e+00,\n","           2.7974e-01, -1.1769e+00,  1.6723e+00],\n","         [ 4.5613e-01,  9.8171e-01, -9.0107e-01,  1.0541e+00, -2.0119e+00,\n","          -5.8257e-01,  5.8623e-01,  4.1739e-01],\n","         [ 4.5613e-01,  9.8171e-01, -9.0107e-01,  1.0541e+00, -2.0119e+00,\n","          -5.8257e-01,  5.8623e-01,  4.1739e-01],\n","         [ 4.5613e-01,  9.8171e-01, -9.0107e-01,  1.0541e+00, -2.0119e+00,\n","          -5.8257e-01,  5.8623e-01,  4.1739e-01],\n","         [ 4.5613e-01,  9.8171e-01, -9.0107e-01,  1.0541e+00, -2.0119e+00,\n","          -5.8257e-01,  5.8623e-01,  4.1739e-01]],\n","\n","        [[ 5.7134e-01,  1.0811e+00,  1.8966e-01, -1.9174e+00, -3.6030e-02,\n","          -1.3004e+00,  4.7835e-01,  9.3342e-01],\n","         [ 1.9006e+00,  1.1544e+00,  9.5320e-02, -1.0749e-01, -1.0152e+00,\n","          -4.5312e-02, -1.1273e+00, -8.5499e-01],\n","         [ 1.4382e+00, -6.9370e-01, -1.8382e+00, -2.7414e-01,  1.4128e-01,\n","          -5.5188e-01,  1.1016e+00,  6.7683e-01],\n","         [-5.3951e-01,  1.9618e+00, -1.6013e-01, -4.9210e-02, -1.6825e+00,\n","          -5.6934e-01,  2.5720e-01,  7.8163e-01],\n","         [ 6.0065e-01,  7.3930e-01,  9.3587e-01, -2.0126e+00, -1.1608e+00,\n","          -3.0378e-01,  5.5115e-01,  6.5020e-01],\n","         [-4.6778e-01,  2.8363e-01, -3.1309e-01, -1.0467e+00, -1.3956e+00,\n","           2.1626e-01,  7.2730e-01,  1.9959e+00],\n","         [ 1.0670e+00,  8.5062e-01, -5.0091e-01,  7.7710e-01, -1.9909e+00,\n","          -1.0005e+00,  4.0497e-01,  3.9266e-01],\n","         [ 1.1893e+00, -7.4639e-01, -1.8078e-01,  8.8048e-01, -1.0729e+00,\n","          -1.6353e+00,  4.9179e-01,  1.0738e+00],\n","         [ 1.1162e+00,  4.4553e-01, -1.0555e+00,  4.1477e-01, -1.9634e+00,\n","          -3.3290e-01,  1.1107e+00,  2.6467e-01],\n","         [ 1.1162e+00,  4.4553e-01, -1.0555e+00,  4.1477e-01, -1.9634e+00,\n","          -3.3290e-01,  1.1107e+00,  2.6467e-01],\n","         [ 1.1162e+00,  4.4553e-01, -1.0555e+00,  4.1477e-01, -1.9634e+00,\n","          -3.3290e-01,  1.1107e+00,  2.6467e-01]],\n","\n","        [[ 1.3668e+00, -2.1642e-02, -1.7710e+00, -1.0035e+00,  1.0228e+00,\n","          -3.4816e-01, -1.4011e-01,  8.9486e-01],\n","         [ 1.3323e-01, -2.4055e-01,  3.9227e-01, -8.3203e-01, -4.6618e-01,\n","          -1.7320e+00,  1.5883e+00,  1.1570e+00],\n","         [ 1.4567e+00,  4.5027e-01, -5.1189e-01, -6.0286e-01, -2.0829e+00,\n","           3.4062e-01,  7.4330e-01,  2.0668e-01],\n","         [ 3.2346e-01,  5.0859e-01, -4.5283e-01, -1.9896e+00, -4.9264e-01,\n","           4.4210e-01, -7.9465e-02,  1.7403e+00],\n","         [ 1.9153e+00,  7.5546e-01, -9.8405e-01, -7.7955e-01, -8.0991e-01,\n","          -1.0312e+00,  3.4627e-01,  5.8768e-01],\n","         [ 5.2849e-01,  7.7117e-01,  8.3065e-01, -1.5372e+00, -1.2192e-01,\n","          -1.0055e+00, -9.3791e-01,  1.4722e+00],\n","         [ 1.4867e+00, -2.2751e-01, -1.0644e+00, -2.4532e-01, -1.4148e+00,\n","          -3.9352e-01,  1.5043e+00,  3.5457e-01],\n","         [ 1.4867e+00, -2.2751e-01, -1.0644e+00, -2.4532e-01, -1.4148e+00,\n","          -3.9352e-01,  1.5043e+00,  3.5457e-01],\n","         [ 1.4867e+00, -2.2751e-01, -1.0644e+00, -2.4532e-01, -1.4148e+00,\n","          -3.9352e-01,  1.5043e+00,  3.5457e-01],\n","         [ 1.4867e+00, -2.2751e-01, -1.0644e+00, -2.4532e-01, -1.4148e+00,\n","          -3.9352e-01,  1.5043e+00,  3.5457e-01],\n","         [ 1.4867e+00, -2.2751e-01, -1.0644e+00, -2.4532e-01, -1.4148e+00,\n","          -3.9352e-01,  1.5043e+00,  3.5457e-01]],\n","\n","        [[-1.3376e+00, -5.8550e-01, -4.4963e-01, -9.1127e-01, -3.1388e-01,\n","           1.7103e+00,  8.2141e-01,  1.0662e+00],\n","         [ 2.0418e+00, -2.5218e-01,  3.3714e-01, -4.5415e-01,  7.0674e-01,\n","          -1.4431e+00, -5.8804e-03, -9.3037e-01],\n","         [ 2.0523e+00, -7.8475e-01, -4.4414e-01, -1.3441e+00, -5.4740e-01,\n","          -1.8328e-01,  4.6438e-01,  7.8696e-01],\n","         [ 1.1989e+00, -4.6557e-01,  3.1204e-01, -1.4355e+00, -9.7493e-01,\n","          -8.6264e-01,  1.0373e+00,  1.1904e+00],\n","         [ 1.2589e-02,  6.9463e-02, -1.1041e+00, -9.1595e-01,  1.4176e+00,\n","          -3.2892e-01,  1.7397e+00, -8.9035e-01],\n","         [ 2.2210e+00, -7.0379e-01, -1.6134e-01,  3.1037e-01, -1.0413e+00,\n","          -1.0832e+00,  2.0655e-02,  4.3761e-01],\n","         [ 1.9778e+00, -2.7510e-01, -1.2882e+00, -7.2053e-01, -6.0899e-01,\n","          -5.0099e-01,  3.8430e-01,  1.0316e+00],\n","         [ 2.3485e-01, -6.1229e-01, -8.6232e-01,  1.1658e+00, -1.4321e+00,\n","          -4.8664e-01,  1.7690e+00,  2.2374e-01],\n","         [-4.3609e-01, -4.9411e-01, -2.6363e-01, -1.4036e+00, -7.8662e-01,\n","           4.2959e-01,  1.0539e+00,  1.9006e+00],\n","         [-1.1183e+00,  1.0919e+00,  1.4987e-01, -6.4533e-02, -1.6955e+00,\n","          -3.7014e-01,  5.0050e-01,  1.5061e+00],\n","         [-6.7119e-01,  1.1998e+00,  8.9244e-01, -1.5028e+00, -1.6772e-01,\n","           1.6840e-01, -1.1832e+00,  1.2643e+00]]],\n","       grad_fn=<NativeLayerNormBackward0>)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"cc-ktye-Pnxd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Masked-MultiHeadAttention And DecoderLayer"],"metadata":{"id":"FOoPIddCSLAI"}},{"cell_type":"code","source":["### Masked Muti head attention\n","\n","\n","class MaskedMultiHeadSelfAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MaskedMultiHeadSelfAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.depth = d_model // num_heads\n","\n","        self.w_q = nn.Linear(d_model, d_model)\n","        self.w_k = nn.Linear(d_model, d_model)\n","        self.w_v = nn.Linear(d_model, d_model)\n","        self.w_o = nn.Linear(d_model, d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        x = x.view(batch_size, -1, self.num_heads, self.depth)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, x, kv=None, mask=None):\n","        batch_size = x.size(0)\n","\n","        # If kv is None, we're performing self-attention\n","        if kv is None:\n","            kv = x\n","\n","        q = self.w_q(x)  # Query\n","        k = self.w_k(kv)  # Key\n","        v = self.w_v(kv)  # Value\n","\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        # Scaled dot-product attention\n","        scores = torch.matmul(q, k.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n","\n","        if mask is not None:\n","            scores = scores.masked_fill(mask == 0, float('-inf'))\n","\n","        attention_weights = nn.functional.softmax(scores, dim=-1)\n","        attention_output = torch.matmul(attention_weights, v)\n","\n","        # Concatenate heads\n","        attention_output = attention_output.permute(0, 2, 1, 3).contiguous()\n","        attention_output = attention_output.view(batch_size, -1, self.num_heads * self.depth)\n","\n","        # Final linear layer\n","        output = self.w_o(attention_output)\n","        return output\n"],"metadata":{"id":"RHPdlLosPnkK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff):\n","        super(DecoderLayer, self).__init__()\n","        self.masked_self_attention = MaskedMultiHeadSelfAttention(d_model, num_heads)\n","        self.encoder_attention = MultiHeadSelfAttention(d_model, num_heads)\n","        self.ffn = FeedForwardNetworkSwiGLU(d_model, d_ff)\n","        self.layer_norm_1 = nn.LayerNorm(d_model)\n","        self.layer_norm_2 = nn.LayerNorm(d_model)\n","        self.layer_norm_3 = nn.LayerNorm(d_model)\n","\n","    def forward(self, x, encoder_output, mask=None):\n","        # Masked self-attention\n","        residual = x\n","        x = self.masked_self_attention(x, x, mask)\n","        x = self.layer_norm_1(x + residual)\n","\n","        # Encoder-decoder attention\n","        residual = x\n","        x = self.encoder_attention(x, encoder_output)\n","        x = self.layer_norm_2(x + residual)\n","\n","        # Feed-forward network\n","        residual = x\n","        x = self.ffn(x)\n","        x = self.layer_norm_3(x + residual)\n","\n","        return x\n"],"metadata":{"id":"5IYEEi2vOzkw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Decoder\n","class TransformerDecoder(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_len):\n","        super(TransformerDecoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model, max_len)\n","        self.decoder_layers = nn.ModuleList(\n","            [DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n","        )\n","        self.layer_norm = nn.LayerNorm(d_model)\n","        self.output_layer = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, x, encoder_output, mask=None):\n","        # Apply embedding and positional encoding\n","        x = self.embedding(x)\n","        x = self.pos_encoder(x)\n","\n","        # Pass through each decoder layer\n","        for decoder_layer in self.decoder_layers:\n","            x = decoder_layer(x, encoder_output, mask)\n","\n","        # Apply final layer normalization and output layer\n","        x = self.layer_norm(x)\n","        output = self.output_layer(x)\n","        return output\n"],"metadata":{"id":"w0ie1Ud4PLE6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Example usage:\n","num_layers = 6\n","decoder = TransformerDecoder(embedding_dim, num_heads, d_ff, num_layers, vocab_size, max_len)\n","\n","# Create some example target input (e.g., shifted right target sequence for training)\n","target_input_tensor = torch.tensor(padded_token_ids_data)\n","\n","# Pass target input and encoder output through the decoder\n","decoder_output = decoder(target_input_tensor, encoded_output)\n","\n","print(\"Decoder Output:\\n\", decoder_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"-94hK13WP1FZ","executionInfo":{"status":"ok","timestamp":1725305952491,"user_tz":-330,"elapsed":422,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"cb05c3ff-a218-46e8-c0d9-4733a8dfd0ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoder Output:\n"," tensor([[[-0.9084, -0.3984,  0.3829,  ..., -0.3335,  0.2679, -0.6934],\n","         [ 0.0345,  1.5045,  0.5387,  ...,  0.3364,  0.5554, -0.5450],\n","         [ 0.7700,  0.8883,  0.2676,  ...,  0.6521, -0.4089,  0.3865],\n","         ...,\n","         [ 0.2712,  1.4132,  0.3575,  ..., -0.1094,  0.3532, -0.5686],\n","         [ 0.2712,  1.4132,  0.3575,  ..., -0.1094,  0.3532, -0.5686],\n","         [ 0.2712,  1.4132,  0.3575,  ..., -0.1094,  0.3532, -0.5686]],\n","\n","        [[-0.1309,  1.1669,  0.3967,  ..., -0.2471,  0.2829,  0.0891],\n","         [-0.0665,  0.8651,  0.2818,  ...,  0.8058, -0.3688,  0.2312],\n","         [ 0.9910,  0.2022, -0.3763,  ..., -0.5187, -1.0557,  0.4144],\n","         ...,\n","         [ 0.4884,  0.9727, -0.2119,  ..., -0.6482,  0.0232,  0.0171],\n","         [ 0.4884,  0.9727, -0.2119,  ..., -0.6482,  0.0232,  0.0171],\n","         [ 0.4884,  0.9727, -0.2119,  ..., -0.6482,  0.0232,  0.0171]],\n","\n","        [[ 1.0777,  0.1037, -0.2088,  ..., -0.4967, -0.7784,  0.6806],\n","         [ 0.9500,  0.0276, -0.6714,  ..., -0.7748, -0.2710,  0.0874],\n","         [ 0.5933, -1.1870, -0.7691,  ..., -0.7348, -0.6887,  0.2014],\n","         ...,\n","         [ 0.7009,  0.2190, -0.4158,  ..., -0.6973, -0.2375,  0.4420],\n","         [ 0.7009,  0.2190, -0.4158,  ..., -0.6973, -0.2375,  0.4420],\n","         [ 0.7009,  0.2190, -0.4158,  ..., -0.6973, -0.2375,  0.4420]],\n","\n","        ...,\n","\n","        [[ 0.8817,  0.7722,  0.1673,  ...,  0.1416, -0.1608,  0.0407],\n","         [ 0.0561,  1.3190,  0.5092,  ...,  0.5880,  0.1677, -0.3632],\n","         [ 0.7928,  0.5827, -0.5745,  ..., -0.6958,  0.1695, -0.8323],\n","         ...,\n","         [ 0.2358,  1.1602,  0.0222,  ..., -0.4975,  0.1975, -0.1486],\n","         [ 0.2358,  1.1602,  0.0222,  ..., -0.4975,  0.1975, -0.1486],\n","         [ 0.2358,  1.1602,  0.0222,  ..., -0.4975,  0.1975, -0.1486]],\n","\n","        [[ 0.4557,  0.0680, -0.0780,  ..., -0.3659, -1.1240,  1.2175],\n","         [ 0.5375, -0.0079,  0.0516,  ..., -0.7003, -0.5265,  0.7057],\n","         [ 0.0119,  0.7879,  0.6010,  ...,  0.5840, -0.1022,  0.9169],\n","         ...,\n","         [ 0.6194,  0.3707, -0.5275,  ..., -0.5829, -0.1274,  0.3547],\n","         [ 0.6194,  0.3707, -0.5275,  ..., -0.5829, -0.1274,  0.3547],\n","         [ 0.6194,  0.3707, -0.5275,  ..., -0.5829, -0.1274,  0.3547]],\n","\n","        [[ 0.6007, -0.4274, -0.2091,  ..., -0.3285, -0.3112,  0.8090],\n","         [ 0.4498, -0.3474, -0.0582,  ..., -0.3810,  0.0903,  0.3252],\n","         [-0.1849,  0.5214, -0.1968,  ...,  0.7858,  0.3911, -0.2724],\n","         ...,\n","         [ 0.2373,  1.0719, -0.3590,  ..., -0.3941,  0.7324, -0.6326],\n","         [ 1.0541,  0.1520, -0.3382,  ..., -0.4407, -0.3152,  0.3270],\n","         [ 0.4425,  0.7103, -0.4568,  ..., -0.6580,  0.5895, -1.0329]]],\n","       grad_fn=<ViewBackward0>)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"YT4KPqCqP5S3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bringing all Features together to build Transformer - Encode and Decoder"],"metadata":{"id":"zyQoTE8cSZwK"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import string\n","from collections import defaultdict\n","\n","# Define SwiGLU activation function\n","class SwiGLU(nn.Module):\n","    def forward(self, x):\n","        x1, x2 = x.chunk(2, dim=-1)  # Split the input into two equal parts\n","        return F.silu(x1) * x2  # Apply Swish (SiLU) to one part and multiply with the other\n","\n","# Define Rotatory Positional Encoding\n","class RotatoryPositionalEncoding(nn.Module):\n","    def __init__(self, max_seq_len, d_model):\n","        super(RotatoryPositionalEncoding, self).__init__()\n","        self.d_model = d_model\n","        self.max_seq_len = max_seq_len\n","\n","        # Create a tensor of shape (max_seq_len, d_model)\n","        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n","        self.register_buffer('pe', torch.zeros(max_seq_len, d_model))\n","        self.pe[:, 0::2] = torch.sin(position * div_term)\n","        self.pe[:, 1::2] = torch.cos(position * div_term)\n","\n","    def forward(self, x):\n","        return x + self.pe[:x.size(1)]\n","\n","# Define Multi-Head Self Attention with Masking\n","class MaskedMultiHeadSelfAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MaskedMultiHeadSelfAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.depth = d_model // num_heads\n","\n","        self.w_q = nn.Linear(d_model, d_model)\n","        self.w_k = nn.Linear(d_model, d_model)\n","        self.w_v = nn.Linear(d_model, d_model)\n","        self.w_o = nn.Linear(d_model, d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        x = x.view(batch_size, -1, self.num_heads, self.depth)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, x, mask=None):\n","        batch_size = x.size(0)\n","\n","        q = self.split_heads(self.w_q(x), batch_size)\n","        k = self.split_heads(self.w_k(x), batch_size)\n","        v = self.split_heads(self.w_v(x), batch_size)\n","\n","        scores = torch.matmul(q, k.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n","\n","        if mask is not None:\n","            scores += mask  # Apply mask to scores\n","\n","        attention_weights = F.softmax(scores, dim=-1)\n","        output = torch.matmul(attention_weights, v)\n","        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.num_heads * self.depth)\n","        return self.w_o(output)\n","\n","# Define Feed-Forward Network with SwiGLU\n","class FeedForwardNetworkSwiGLU(nn.Module):\n","    def __init__(self, d_model, d_ff):\n","        super(FeedForwardNetworkSwiGLU, self).__init__()\n","        self.fc1 = nn.Linear(d_model, d_ff*2)\n","        self.fc2 = nn.Linear(d_ff, d_model)\n","        self.swiGLU = SwiGLU()\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        residual = x\n","        x = self.fc1(x)\n","        x = self.swiGLU(x)\n","        x = self.fc2(x)\n","        x = self.layer_norm(x + residual)\n","        return x\n","\n","# Define Transformer Encoder\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len):\n","        super(TransformerEncoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoding = RotatoryPositionalEncoding(max_seq_len, d_model)\n","        self.encoder_layers = nn.ModuleList([\n","            nn.ModuleList([\n","                MultiHeadSelfAttention(d_model, num_heads),\n","                FeedForwardNetworkSwiGLU(d_model, d_ff)\n","            ]) for _ in range(num_layers)\n","        ])\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = self.pos_encoding(x)\n","        for self_attn, ffn in self.encoder_layers:\n","            x = self_attn(x)\n","            x = ffn(x)\n","        return self.layer_norm(x)\n","\n","# Define Transformer Decoder\n","class TransformerDecoder(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len):\n","        super(TransformerDecoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoding = RotatoryPositionalEncoding(max_seq_len, d_model)\n","        self.decoder_layers = nn.ModuleList([\n","            nn.ModuleList([\n","                MaskedMultiHeadSelfAttention(d_model, num_heads),  # Masked MHA\n","                MultiHeadSelfAttention(d_model, num_heads),  # Cross-attention\n","                FeedForwardNetworkSwiGLU(d_model, d_ff)\n","            ]) for _ in range(num_layers)\n","        ])\n","        self.layer_norm = nn.LayerNorm(d_model)\n","        self.fc_out = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, x, encoder_output):\n","        x = self.embedding(x)\n","        x = self.pos_encoding(x)\n","        for self_attn, cross_attn, ffn in self.decoder_layers:\n","            seq_len = x.size(1)\n","            mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(x.device)\n","            x = self.layer_norm(x + self_attn(x, mask))\n","            x = self.layer_norm(x + cross_attn(x, encoder_output))\n","            x = self.layer_norm(x + ffn(x))\n","        return self.fc_out(x)\n"],"metadata":{"id":"3ujBdIfxSi30"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Fake dataset\n","data = [\n","    \"I love machine learning.\",\n","    \"Artificial intelligence is the future.\",\n","    \"Transformers are powerful models.\",\n","    \"Natural language processing is fascinating.\",\n","    \"Deep learning enables great advances in AI.\",\n","    \"PyTorch is a popular deep learning library.\",\n","    \"GPT models are widely used in NLP.\",\n","    \"Neural networks are the backbone of deep learning.\",\n","    \"Embedding layers convert tokens to vectors.\",\n","    \"Attention mechanisms help models focus on important parts of the input.\"\n","]\n","\n","# Tokenize data\n","def simple_tokenize(sentence):\n","    sentence = sentence.translate(str.maketrans('', '', string.punctuation)).lower()\n","    return sentence.split()\n","\n","tokenized_data = [simple_tokenize(sentence) for sentence in data]\n","\n","# Build vocabulary with padding and unknown tokens\n","vocab = defaultdict(lambda: len(vocab))\n","PAD = vocab[\"<PAD>\"]\n","UNK = vocab[\"<UNK>\"]\n","\n","for tokens in tokenized_data:\n","    for token in tokens:\n","        _ = vocab[token]\n","\n","vocab = dict(vocab)\n","\n","def tokens_to_ids(tokens, vocab):\n","    return [vocab.get(token, UNK) for token in tokens]\n","\n","max_len = max(len(tokens) for tokens in tokenized_data)\n","\n","token_ids_data = [tokens_to_ids(tokens, vocab) for tokens in tokenized_data]\n","padded_token_ids_data = [token_ids + [PAD] * (max_len - len(token_ids)) for token_ids in token_ids_data]\n","\n","# Model parameters\n","d_model = 8\n","num_heads = 4\n","d_ff = 32\n","num_layers = 2\n","vocab_size = len(vocab)\n","max_seq_len = max_len\n","\n","# Initialize models\n","encoder = TransformerEncoder(d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len)\n","decoder = TransformerDecoder(d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len)\n","\n","# Example input sequences\n","batch_size = 2\n","src = torch.tensor(padded_token_ids_data[:batch_size])\n","tgt = torch.tensor(padded_token_ids_data[batch_size:batch_size*2])\n","\n","# Forward pass through the encoder and decoder\n","encoder_output = encoder(src)\n","decoder_output = decoder(tgt, encoder_output)\n","\n","# Print the results\n","print(\"Source sequences (src):\")\n","print(src)\n","print(\"\\nTarget sequences (tgt):\")\n","print(tgt)\n","print(\"\\nEncoder output:\")\n","print(encoder_output)\n","print(\"\\nDecoder output:\")\n","print(decoder_output)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"FmU9PitXT9HR","executionInfo":{"status":"ok","timestamp":1725308480949,"user_tz":-330,"elapsed":674,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"c9b28707-60d4-40ec-ca14-76079d801c09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Source sequences (src):\n","tensor([[ 2,  3,  4,  5,  0,  0,  0,  0,  0,  0,  0],\n","        [ 6,  7,  8,  9, 10,  0,  0,  0,  0,  0,  0]])\n","\n","Target sequences (tgt):\n","tensor([[11, 12, 13, 14,  0,  0,  0,  0,  0,  0,  0],\n","        [15, 16, 17,  8, 18,  0,  0,  0,  0,  0,  0]])\n","\n","Encoder output:\n","tensor([[[-0.3966, -1.9151,  1.0020,  0.6535,  1.4238, -0.8403,  0.0998,\n","          -0.0271],\n","         [-0.3966, -1.9151,  1.0020,  0.6535,  1.4238, -0.8403,  0.0998,\n","          -0.0271],\n","         [-0.3966, -1.9151,  1.0020,  0.6535,  1.4238, -0.8403,  0.0998,\n","          -0.0271],\n","         [-0.3966, -1.9151,  1.0020,  0.6535,  1.4238, -0.8403,  0.0998,\n","          -0.0271],\n","         [-0.3966, -1.9151,  1.0020,  0.6535,  1.4238, -0.8403,  0.0998,\n","          -0.0271],\n","         [-0.3966, -1.9151,  1.0020,  0.6535,  1.4238, -0.8403,  0.0998,\n","          -0.0271],\n","         [-0.3966, -1.9151,  1.0020,  0.6535,  1.4238, -0.8403,  0.0998,\n","          -0.0271],\n","         [-0.3966, -1.9151,  1.0020,  0.6535,  1.4238, -0.8403,  0.0998,\n","          -0.0271],\n","         [-0.3966, -1.9151,  1.0020,  0.6535,  1.4238, -0.8403,  0.0998,\n","          -0.0271],\n","         [-0.3966, -1.9151,  1.0020,  0.6535,  1.4238, -0.8403,  0.0998,\n","          -0.0271],\n","         [-0.3966, -1.9151,  1.0020,  0.6535,  1.4238, -0.8403,  0.0998,\n","          -0.0271]],\n","\n","        [[-0.4442, -1.7192,  0.9415,  0.6753,  1.3347, -1.2591,  0.3512,\n","           0.1198],\n","         [-0.4441, -1.7194,  0.9415,  0.6755,  1.3346, -1.2589,  0.3510,\n","           0.1198],\n","         [-0.4444, -1.7189,  0.9410,  0.6750,  1.3351, -1.2595,  0.3514,\n","           0.1204],\n","         [-0.4445, -1.7186,  0.9407,  0.6747,  1.3353, -1.2600,  0.3516,\n","           0.1208],\n","         [-0.4444, -1.7189,  0.9410,  0.6750,  1.3351, -1.2595,  0.3513,\n","           0.1204],\n","         [-0.4447, -1.7183,  0.9403,  0.6745,  1.3356, -1.2602,  0.3518,\n","           0.1211],\n","         [-0.4446, -1.7184,  0.9405,  0.6746,  1.3354, -1.2601,  0.3518,\n","           0.1209],\n","         [-0.4445, -1.7186,  0.9407,  0.6748,  1.3353, -1.2599,  0.3516,\n","           0.1206],\n","         [-0.4445, -1.7187,  0.9408,  0.6748,  1.3352, -1.2598,  0.3515,\n","           0.1206],\n","         [-0.4445, -1.7186,  0.9406,  0.6747,  1.3354, -1.2599,  0.3516,\n","           0.1208],\n","         [-0.4446, -1.7184,  0.9404,  0.6746,  1.3355, -1.2601,  0.3517,\n","           0.1210]]], grad_fn=<NativeLayerNormBackward0>)\n","\n","Decoder output:\n","tensor([[[-0.2849,  0.6406, -0.4078,  ...,  0.1697,  0.7326,  0.0300],\n","         [-0.4168,  0.5269, -0.9587,  ..., -0.0640, -0.4902,  0.7602],\n","         [-0.4983, -0.2829, -0.3271,  ..., -0.7997,  0.3962, -0.0035],\n","         ...,\n","         [-0.2241, -0.7925, -0.0208,  ...,  1.0662,  0.2852,  0.1916],\n","         [-0.0079, -0.6488,  0.4480,  ...,  0.9965,  0.1921, -0.1248],\n","         [-0.0259, -0.2342,  0.4885,  ...,  0.7647,  0.3911, -0.3897]],\n","\n","        [[-0.9399, -0.3293, -1.0277,  ..., -0.1837,  1.2563,  0.3892],\n","         [-0.7275, -0.6903, -0.7311,  ...,  0.3779,  0.0342,  0.8343],\n","         [-0.8375, -0.0169, -0.8513,  ..., -0.7907,  0.6021,  0.3318],\n","         ...,\n","         [-0.2385, -0.7348, -0.0108,  ...,  1.0209,  0.3164,  0.1601],\n","         [-0.0415, -0.6148,  0.4127,  ...,  0.9431,  0.2304, -0.1313],\n","         [-0.0624, -0.2309,  0.4419,  ...,  0.7037,  0.3998, -0.3804]]],\n","       grad_fn=<ViewBackward0>)\n"]}]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","class TransformerDataset(Dataset):\n","    def __init__(self, src_sequences, tgt_sequences):\n","        self.src_sequences = src_sequences\n","        self.tgt_sequences = tgt_sequences\n","\n","    def __len__(self):\n","        return len(self.src_sequences)\n","\n","    def __getitem__(self, idx):\n","        src = torch.tensor(self.src_sequences[idx])\n","        tgt = torch.tensor(self.tgt_sequences[idx])\n","        return src, tgt\n","\n","# Example dataset (using the padded sequences from before)\n","src_sequences = padded_token_ids_data[:8]  # Source sequences\n","tgt_sequences = padded_token_ids_data[1:9]  # Target sequences (shifted by 1 for simplicity)\n","\n","dataset = TransformerDataset(src_sequences, tgt_sequences)\n","data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n"],"metadata":{"id":"82uT32FiaDG-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len):\n","        super(Transformer, self).__init__()\n","        self.encoder = TransformerEncoder(d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len)\n","        self.decoder = TransformerDecoder(d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len)\n","\n","    def forward(self, src, tgt):\n","        encoder_output = self.encoder(src)\n","        decoder_output = self.decoder(tgt, encoder_output)\n","        return decoder_output\n"],"metadata":{"id":"mvMgs27WaDDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","# Initialize the transformer model\n","model = Transformer(d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD)  # Ignoring padding tokens in loss\n","\n","# Training loop\n","num_epochs = 100\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    for src, tgt in data_loader:\n","        tgt_input = tgt[:, :-1]  # All but last token (teacher forcing)\n","        tgt_output = tgt[:, 1:]  # All but first token (labels)\n","\n","        optimizer.zero_grad()\n","        output = model(src, tgt_input)\n","        loss = criterion(output.view(-1, vocab_size), tgt_output.contiguous().view(-1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(data_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"8q2qfkkjaC-P","executionInfo":{"status":"ok","timestamp":1725310978916,"user_tz":-330,"elapsed":9051,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"f9179d19-90a2-4928-f87d-5c295534610f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150, Loss: 4.0442\n","Epoch 2/150, Loss: 3.6850\n","Epoch 3/150, Loss: 3.4424\n","Epoch 4/150, Loss: 3.2484\n","Epoch 5/150, Loss: 3.0421\n","Epoch 6/150, Loss: 2.8623\n","Epoch 7/150, Loss: 2.6885\n","Epoch 8/150, Loss: 2.4912\n","Epoch 9/150, Loss: 2.2966\n","Epoch 10/150, Loss: 2.1498\n","Epoch 11/150, Loss: 1.9810\n","Epoch 12/150, Loss: 1.8318\n","Epoch 13/150, Loss: 1.6873\n","Epoch 14/150, Loss: 1.5451\n","Epoch 15/150, Loss: 1.4129\n","Epoch 16/150, Loss: 1.3051\n","Epoch 17/150, Loss: 1.1938\n","Epoch 18/150, Loss: 1.0882\n","Epoch 19/150, Loss: 0.9945\n","Epoch 20/150, Loss: 0.9219\n","Epoch 21/150, Loss: 0.8340\n","Epoch 22/150, Loss: 0.7451\n","Epoch 23/150, Loss: 0.6584\n","Epoch 24/150, Loss: 0.6055\n","Epoch 25/150, Loss: 0.5418\n","Epoch 26/150, Loss: 0.4851\n","Epoch 27/150, Loss: 0.4511\n","Epoch 28/150, Loss: 0.3996\n","Epoch 29/150, Loss: 0.3627\n","Epoch 30/150, Loss: 0.3240\n","Epoch 31/150, Loss: 0.2923\n","Epoch 32/150, Loss: 0.2656\n","Epoch 33/150, Loss: 0.2427\n","Epoch 34/150, Loss: 0.2169\n","Epoch 35/150, Loss: 0.2013\n","Epoch 36/150, Loss: 0.1905\n","Epoch 37/150, Loss: 0.1878\n","Epoch 38/150, Loss: 0.1596\n","Epoch 39/150, Loss: 0.1489\n","Epoch 40/150, Loss: 0.1359\n","Epoch 41/150, Loss: 0.1241\n","Epoch 42/150, Loss: 0.1143\n","Epoch 43/150, Loss: 0.1059\n","Epoch 44/150, Loss: 0.1006\n","Epoch 45/150, Loss: 0.0925\n","Epoch 46/150, Loss: 0.0880\n","Epoch 47/150, Loss: 0.0821\n","Epoch 48/150, Loss: 0.0777\n","Epoch 49/150, Loss: 0.0723\n","Epoch 50/150, Loss: 0.0691\n","Epoch 51/150, Loss: 0.0650\n","Epoch 52/150, Loss: 0.0611\n","Epoch 53/150, Loss: 0.0583\n","Epoch 54/150, Loss: 0.0556\n","Epoch 55/150, Loss: 0.0535\n","Epoch 56/150, Loss: 0.0506\n","Epoch 57/150, Loss: 0.0489\n","Epoch 58/150, Loss: 0.0472\n","Epoch 59/150, Loss: 0.0443\n","Epoch 60/150, Loss: 0.0436\n","Epoch 61/150, Loss: 0.0417\n","Epoch 62/150, Loss: 0.0401\n","Epoch 63/150, Loss: 0.0384\n","Epoch 64/150, Loss: 0.0374\n","Epoch 65/150, Loss: 0.0358\n","Epoch 66/150, Loss: 0.0347\n","Epoch 67/150, Loss: 0.0336\n","Epoch 68/150, Loss: 0.0327\n","Epoch 69/150, Loss: 0.0314\n","Epoch 70/150, Loss: 0.0304\n","Epoch 71/150, Loss: 0.0293\n","Epoch 72/150, Loss: 0.0285\n","Epoch 73/150, Loss: 0.0278\n","Epoch 74/150, Loss: 0.0269\n","Epoch 75/150, Loss: 0.0261\n","Epoch 76/150, Loss: 0.0252\n","Epoch 77/150, Loss: 0.0246\n","Epoch 78/150, Loss: 0.0239\n","Epoch 79/150, Loss: 0.0233\n","Epoch 80/150, Loss: 0.0225\n","Epoch 81/150, Loss: 0.0218\n","Epoch 82/150, Loss: 0.0211\n","Epoch 83/150, Loss: 0.0211\n","Epoch 84/150, Loss: 0.0207\n","Epoch 85/150, Loss: 0.0197\n","Epoch 86/150, Loss: 0.0196\n","Epoch 87/150, Loss: 0.0190\n","Epoch 88/150, Loss: 0.0185\n","Epoch 89/150, Loss: 0.0178\n","Epoch 90/150, Loss: 0.0176\n","Epoch 91/150, Loss: 0.0172\n","Epoch 92/150, Loss: 0.0168\n","Epoch 93/150, Loss: 0.0162\n","Epoch 94/150, Loss: 0.0162\n","Epoch 95/150, Loss: 0.0157\n","Epoch 96/150, Loss: 0.0153\n","Epoch 97/150, Loss: 0.0148\n","Epoch 98/150, Loss: 0.0148\n","Epoch 99/150, Loss: 0.0144\n","Epoch 100/150, Loss: 0.0141\n","Epoch 101/150, Loss: 0.0139\n","Epoch 102/150, Loss: 0.0136\n","Epoch 103/150, Loss: 0.0132\n","Epoch 104/150, Loss: 0.0131\n","Epoch 105/150, Loss: 0.0129\n","Epoch 106/150, Loss: 0.0128\n","Epoch 107/150, Loss: 0.0124\n","Epoch 108/150, Loss: 0.0122\n","Epoch 109/150, Loss: 0.0120\n","Epoch 110/150, Loss: 0.0118\n","Epoch 111/150, Loss: 0.0116\n","Epoch 112/150, Loss: 0.0113\n","Epoch 113/150, Loss: 0.0111\n","Epoch 114/150, Loss: 0.0110\n","Epoch 115/150, Loss: 0.0107\n","Epoch 116/150, Loss: 0.0103\n","Epoch 117/150, Loss: 0.0103\n","Epoch 118/150, Loss: 0.0103\n","Epoch 119/150, Loss: 0.0100\n","Epoch 120/150, Loss: 0.0098\n","Epoch 121/150, Loss: 0.0097\n","Epoch 122/150, Loss: 0.0095\n","Epoch 123/150, Loss: 0.0094\n","Epoch 124/150, Loss: 0.0093\n","Epoch 125/150, Loss: 0.0090\n","Epoch 126/150, Loss: 0.0090\n","Epoch 127/150, Loss: 0.0087\n","Epoch 128/150, Loss: 0.0086\n","Epoch 129/150, Loss: 0.0085\n","Epoch 130/150, Loss: 0.0082\n","Epoch 131/150, Loss: 0.0083\n","Epoch 132/150, Loss: 0.0082\n","Epoch 133/150, Loss: 0.0080\n","Epoch 134/150, Loss: 0.0077\n","Epoch 135/150, Loss: 0.0076\n","Epoch 136/150, Loss: 0.0077\n","Epoch 137/150, Loss: 0.0075\n","Epoch 138/150, Loss: 0.0075\n","Epoch 139/150, Loss: 0.0073\n","Epoch 140/150, Loss: 0.0072\n","Epoch 141/150, Loss: 0.0071\n","Epoch 142/150, Loss: 0.0070\n","Epoch 143/150, Loss: 0.0069\n","Epoch 144/150, Loss: 0.0067\n","Epoch 145/150, Loss: 0.0067\n","Epoch 146/150, Loss: 0.0066\n","Epoch 147/150, Loss: 0.0066\n","Epoch 148/150, Loss: 0.0065\n","Epoch 149/150, Loss: 0.0064\n","Epoch 150/150, Loss: 0.0063\n"]}]},{"cell_type":"code","source":["def evaluate(model, data_loader):\n","    model.eval()\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for src, tgt in data_loader:\n","            tgt_input = tgt[:, :-1]  # All but last token (teacher forcing)\n","            tgt_output = tgt[:, 1:]  # All but first token (labels)\n","\n","            output = model(src, tgt_input)\n","            loss = criterion(output.view(-1, vocab_size), tgt_output.contiguous().view(-1))\n","\n","            total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(data_loader)\n","    return avg_loss\n","\n","# Evaluate the model\n","validation_loss = evaluate(model, data_loader)\n","print(f\"Validation Loss: {validation_loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iwyFIR_5aCzZ","executionInfo":{"status":"ok","timestamp":1725310982604,"user_tz":-330,"elapsed":415,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"555b6293-55a0-4105-a685-1ff744bf7345"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.0063\n"]}]},{"cell_type":"code","source":["def generate_sequence(model, src, max_len=max_seq_len):\n","    model.eval()\n","    src = torch.tensor(src).unsqueeze(0)  # Add batch dimension\n","    tgt = torch.zeros((1, max_len), dtype=torch.long)  # Initialize target sequence with zeros\n","    print(\"Initial Target Sequence:\", tgt)\n","\n","    with torch.no_grad():\n","        for i in range(max_len):\n","            tgt_input = tgt[:, :i+1]  # All tokens up to position i (inclusive)\n","            print(\"Target Input Shape:\", tgt_input.shape)\n","\n","            # Generate predictions\n","            output = model(src, tgt_input)\n","            print(\"Model Output Shape:\", output.shape)\n","\n","            # Output should be of shape (batch_size, seq_len, vocab_size)\n","            if output.dim() == 3:\n","                # Handle cases where output is empty\n","                if output.size(1) > 0:\n","                    next_token = output[:, -1, :].argmax(dim=-1)  # Get the most likely next token\n","                    tgt[:, i] = next_token\n","                else:\n","                    print(\"Output sequence is empty.\")\n","                    break\n","            else:\n","                print(f\"Unexpected output shape: {output.shape}\")\n","                break\n","\n","            # Stop if padding token is generated\n","            if next_token.item() == PAD:\n","                break\n","\n","    return tgt.squeeze().tolist()\n","\n","# Generate a sequence\n","example_src = padded_token_ids_data[9]  # Use an example source sequence\n","print(f\"Example Source Sequence: {example_src}\")\n","generated_sequence = generate_sequence(model, example_src)\n","print(f\"Generated Sequence: {generated_sequence}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-U8E3uraqTb","executionInfo":{"status":"ok","timestamp":1725310984417,"user_tz":-330,"elapsed":7,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"fdf52626-35d4-4cee-c68d-a55731bc0604"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Example Source Sequence: [43, 44, 45, 14, 46, 47, 48, 49, 36, 9, 50]\n","Initial Target Sequence: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n","Target Input Shape: torch.Size([1, 1])\n","Model Output Shape: torch.Size([1, 1, 51])\n","Target Input Shape: torch.Size([1, 2])\n","Model Output Shape: torch.Size([1, 2, 51])\n","Target Input Shape: torch.Size([1, 3])\n","Model Output Shape: torch.Size([1, 3, 51])\n","Target Input Shape: torch.Size([1, 4])\n","Model Output Shape: torch.Size([1, 4, 51])\n","Target Input Shape: torch.Size([1, 5])\n","Model Output Shape: torch.Size([1, 5, 51])\n","Target Input Shape: torch.Size([1, 6])\n","Model Output Shape: torch.Size([1, 6, 51])\n","Target Input Shape: torch.Size([1, 7])\n","Model Output Shape: torch.Size([1, 7, 51])\n","Target Input Shape: torch.Size([1, 8])\n","Model Output Shape: torch.Size([1, 8, 51])\n","Target Input Shape: torch.Size([1, 9])\n","Model Output Shape: torch.Size([1, 9, 51])\n","Target Input Shape: torch.Size([1, 10])\n","Model Output Shape: torch.Size([1, 10, 51])\n","Target Input Shape: torch.Size([1, 11])\n","Model Output Shape: torch.Size([1, 11, 51])\n","Generated Sequence: [38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38]\n"]}]},{"cell_type":"code","source":["from torch.optim.lr_scheduler import StepLR\n","\n","# Define a learning rate scheduler\n","scheduler = StepLR(optimizer, step_size=5, gamma=0.7)\n","\n","# Modify the training loop to include the scheduler\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    for src, tgt in data_loader:\n","        tgt_input = tgt[:, :-1]\n","        tgt_output = tgt[:, 1:]\n","\n","        optimizer.zero_grad()\n","        output = model(src, tgt_input)\n","        loss = criterion(output.view(-1, vocab_size), tgt_output.contiguous().view(-1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(data_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n","\n","    # Update the learning rate\n","    scheduler.step()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"NenRVKpqaqi4","executionInfo":{"status":"ok","timestamp":1725310994599,"user_tz":-330,"elapsed":10188,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"49b1c55a-d18a-4595-ddc7-1015e9d02c03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150, Loss: 0.0062\n","Epoch 2/150, Loss: 0.0061\n","Epoch 3/150, Loss: 0.0061\n","Epoch 4/150, Loss: 0.0059\n","Epoch 5/150, Loss: 0.0059\n","Epoch 6/150, Loss: 0.0059\n","Epoch 7/150, Loss: 0.0058\n","Epoch 8/150, Loss: 0.0057\n","Epoch 9/150, Loss: 0.0057\n","Epoch 10/150, Loss: 0.0057\n","Epoch 11/150, Loss: 0.0056\n","Epoch 12/150, Loss: 0.0056\n","Epoch 13/150, Loss: 0.0056\n","Epoch 14/150, Loss: 0.0055\n","Epoch 15/150, Loss: 0.0055\n","Epoch 16/150, Loss: 0.0054\n","Epoch 17/150, Loss: 0.0054\n","Epoch 18/150, Loss: 0.0054\n","Epoch 19/150, Loss: 0.0054\n","Epoch 20/150, Loss: 0.0054\n","Epoch 21/150, Loss: 0.0054\n","Epoch 22/150, Loss: 0.0053\n","Epoch 23/150, Loss: 0.0053\n","Epoch 24/150, Loss: 0.0053\n","Epoch 25/150, Loss: 0.0053\n","Epoch 26/150, Loss: 0.0053\n","Epoch 27/150, Loss: 0.0053\n","Epoch 28/150, Loss: 0.0052\n","Epoch 29/150, Loss: 0.0052\n","Epoch 30/150, Loss: 0.0052\n","Epoch 31/150, Loss: 0.0052\n","Epoch 32/150, Loss: 0.0052\n","Epoch 33/150, Loss: 0.0052\n","Epoch 34/150, Loss: 0.0052\n","Epoch 35/150, Loss: 0.0052\n","Epoch 36/150, Loss: 0.0051\n","Epoch 37/150, Loss: 0.0052\n","Epoch 38/150, Loss: 0.0051\n","Epoch 39/150, Loss: 0.0052\n","Epoch 40/150, Loss: 0.0051\n","Epoch 41/150, Loss: 0.0051\n","Epoch 42/150, Loss: 0.0051\n","Epoch 43/150, Loss: 0.0051\n","Epoch 44/150, Loss: 0.0052\n","Epoch 45/150, Loss: 0.0051\n","Epoch 46/150, Loss: 0.0050\n","Epoch 47/150, Loss: 0.0051\n","Epoch 48/150, Loss: 0.0051\n","Epoch 49/150, Loss: 0.0051\n","Epoch 50/150, Loss: 0.0051\n","Epoch 51/150, Loss: 0.0051\n","Epoch 52/150, Loss: 0.0051\n","Epoch 53/150, Loss: 0.0051\n","Epoch 54/150, Loss: 0.0051\n","Epoch 55/150, Loss: 0.0051\n","Epoch 56/150, Loss: 0.0051\n","Epoch 57/150, Loss: 0.0051\n","Epoch 58/150, Loss: 0.0051\n","Epoch 59/150, Loss: 0.0050\n","Epoch 60/150, Loss: 0.0050\n","Epoch 61/150, Loss: 0.0051\n","Epoch 62/150, Loss: 0.0051\n","Epoch 63/150, Loss: 0.0050\n","Epoch 64/150, Loss: 0.0051\n","Epoch 65/150, Loss: 0.0051\n","Epoch 66/150, Loss: 0.0051\n","Epoch 67/150, Loss: 0.0051\n","Epoch 68/150, Loss: 0.0051\n","Epoch 69/150, Loss: 0.0051\n","Epoch 70/150, Loss: 0.0051\n","Epoch 71/150, Loss: 0.0050\n","Epoch 72/150, Loss: 0.0051\n","Epoch 73/150, Loss: 0.0051\n","Epoch 74/150, Loss: 0.0051\n","Epoch 75/150, Loss: 0.0050\n","Epoch 76/150, Loss: 0.0051\n","Epoch 77/150, Loss: 0.0050\n","Epoch 78/150, Loss: 0.0050\n","Epoch 79/150, Loss: 0.0051\n","Epoch 80/150, Loss: 0.0051\n","Epoch 81/150, Loss: 0.0051\n","Epoch 82/150, Loss: 0.0051\n","Epoch 83/150, Loss: 0.0051\n","Epoch 84/150, Loss: 0.0051\n","Epoch 85/150, Loss: 0.0050\n","Epoch 86/150, Loss: 0.0050\n","Epoch 87/150, Loss: 0.0051\n","Epoch 88/150, Loss: 0.0050\n","Epoch 89/150, Loss: 0.0051\n","Epoch 90/150, Loss: 0.0051\n","Epoch 91/150, Loss: 0.0051\n","Epoch 92/150, Loss: 0.0051\n","Epoch 93/150, Loss: 0.0050\n","Epoch 94/150, Loss: 0.0051\n","Epoch 95/150, Loss: 0.0051\n","Epoch 96/150, Loss: 0.0050\n","Epoch 97/150, Loss: 0.0050\n","Epoch 98/150, Loss: 0.0051\n","Epoch 99/150, Loss: 0.0050\n","Epoch 100/150, Loss: 0.0050\n","Epoch 101/150, Loss: 0.0051\n","Epoch 102/150, Loss: 0.0051\n","Epoch 103/150, Loss: 0.0051\n","Epoch 104/150, Loss: 0.0051\n","Epoch 105/150, Loss: 0.0050\n","Epoch 106/150, Loss: 0.0050\n","Epoch 107/150, Loss: 0.0051\n","Epoch 108/150, Loss: 0.0050\n","Epoch 109/150, Loss: 0.0051\n","Epoch 110/150, Loss: 0.0051\n","Epoch 111/150, Loss: 0.0051\n","Epoch 112/150, Loss: 0.0051\n","Epoch 113/150, Loss: 0.0051\n","Epoch 114/150, Loss: 0.0051\n","Epoch 115/150, Loss: 0.0050\n","Epoch 116/150, Loss: 0.0051\n","Epoch 117/150, Loss: 0.0050\n","Epoch 118/150, Loss: 0.0051\n","Epoch 119/150, Loss: 0.0051\n","Epoch 120/150, Loss: 0.0050\n","Epoch 121/150, Loss: 0.0051\n","Epoch 122/150, Loss: 0.0051\n","Epoch 123/150, Loss: 0.0050\n","Epoch 124/150, Loss: 0.0050\n","Epoch 125/150, Loss: 0.0050\n","Epoch 126/150, Loss: 0.0050\n","Epoch 127/150, Loss: 0.0051\n","Epoch 128/150, Loss: 0.0050\n","Epoch 129/150, Loss: 0.0051\n","Epoch 130/150, Loss: 0.0050\n","Epoch 131/150, Loss: 0.0051\n","Epoch 132/150, Loss: 0.0051\n","Epoch 133/150, Loss: 0.0051\n","Epoch 134/150, Loss: 0.0050\n","Epoch 135/150, Loss: 0.0051\n","Epoch 136/150, Loss: 0.0050\n","Epoch 137/150, Loss: 0.0050\n","Epoch 138/150, Loss: 0.0051\n","Epoch 139/150, Loss: 0.0051\n","Epoch 140/150, Loss: 0.0051\n","Epoch 141/150, Loss: 0.0051\n","Epoch 142/150, Loss: 0.0051\n","Epoch 143/150, Loss: 0.0050\n","Epoch 144/150, Loss: 0.0051\n","Epoch 145/150, Loss: 0.0050\n","Epoch 146/150, Loss: 0.0051\n","Epoch 147/150, Loss: 0.0051\n","Epoch 148/150, Loss: 0.0051\n","Epoch 149/150, Loss: 0.0051\n","Epoch 150/150, Loss: 0.0051\n"]}]},{"cell_type":"code","source":["# Save the model\n","torch.save(model.state_dict(), 'transformer_model.pth')\n","\n","# Load the model\n","model = Transformer(d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len)\n","model.load_state_dict(torch.load('transformer_model.pth'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qnk-sY5GdpIz","executionInfo":{"status":"ok","timestamp":1725310994599,"user_tz":-330,"elapsed":34,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"8d1b1a79-0075-4582-9659-b7bcf9ac9c71"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":150}]},{"cell_type":"code","source":["# Save model and optimizer state\n","torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","}, 'transformer_model.pth')\n"],"metadata":{"id":"JGzhLJgmch6N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the model and optimizer\n","model = TransformerEncoder(d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len)\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","# Load the model and optimizer state\n","checkpoint = torch.load('transformer_model.pth')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","model.eval()  # Set the model to evaluation mode\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":745},"id":"sunuI12qdJc1","executionInfo":{"status":"error","timestamp":1725310994600,"user_tz":-330,"elapsed":31,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"df82a831-f78f-449d-e30b-5af14d54f0a6"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Error(s) in loading state_dict for TransformerEncoder:\n\tMissing key(s) in state_dict: \"embedding.weight\", \"pos_encoding.pe\", \"encoder_layers.0.0.w_q.weight\", \"encoder_layers.0.0.w_q.bias\", \"encoder_layers.0.0.w_k.weight\", \"encoder_layers.0.0.w_k.bias\", \"encoder_layers.0.0.w_v.weight\", \"encoder_layers.0.0.w_v.bias\", \"encoder_layers.0.0.w_o.weight\", \"encoder_layers.0.0.w_o.bias\", \"encoder_layers.0.1.fc1.weight\", \"encoder_layers.0.1.fc1.bias\", \"encoder_layers.0.1.fc2.weight\", \"encoder_layers.0.1.fc2.bias\", \"encoder_layers.0.1.layer_norm.weight\", \"encoder_layers.0.1.layer_norm.bias\", \"encoder_layers.1.0.w_q.weight\", \"encoder_layers.1.0.w_q.bias\", \"encoder_layers.1.0.w_k.weight\", \"encoder_layers.1.0.w_k.bias\", \"encoder_layers.1.0.w_v.weight\", \"encoder_layers.1.0.w_v.bias\", \"encoder_layers.1.0.w_o.weight\", \"encoder_layers.1.0.w_o.bias\", \"encoder_layers.1.1.fc1.weight\", \"encoder_layers.1.1.fc1.bias\", \"encoder_layers.1.1.fc2.weight\", \"encoder_layers.1.1.fc2.bias\", \"encoder_layers.1.1.layer_norm.weight\", \"encoder_layers.1.1.layer_norm.bias\", \"layer_norm.weight\", \"layer_norm.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.embedding.weight\", \"encoder.pos_encoding.pe\", \"encoder.encoder_layers.0.0.w_q.weight\", \"encoder.encoder_layers.0.0.w_q.bias\", \"encoder.encoder_layers.0.0.w_k.weight\", \"encoder.encoder_layers.0.0.w_k.bias\", \"encoder.encoder_layers.0.0.w_v.weight\", \"encoder.encoder_layers.0.0.w_v.bias\", \"encoder.encoder_layers.0.0.w_o.weight\", \"encoder.encoder_layers.0.0.w_o.bias\", \"encoder.encoder_layers.0.1.fc1.weight\", \"encoder.encoder_layers.0.1.fc1.bias\", \"encoder.encoder_layers.0.1.fc2.weight\", \"encoder.encoder_layers.0.1.fc2.bias\", \"encoder.encoder_layers.0.1.layer_norm.weight\", \"encoder.encoder_layers.0.1.layer_norm.bias\", \"encoder.encoder_layers.1.0.w_q.weight\", \"encoder.encoder_layers.1.0.w_q.bias\", \"encoder.encoder_layers.1.0.w_k.weight\", \"encoder.encoder_layers.1.0.w_k.bias\", \"encoder.encoder_layers.1.0.w_v.weight\", \"encoder.encoder_layers.1.0.w_v.bias\", \"encoder.encoder_layers.1.0.w_o.weight\", \"encoder.encoder_layers.1.0.w_o.bias\", \"encoder.encoder_layers.1.1.fc1.weight\", \"encoder.encoder_layers.1.1.fc1.bias\", \"encoder.encoder_layers.1.1.fc2.weight\", \"encoder.encoder_layers.1.1.fc2.bias\", \"encoder.encoder_layers.1.1.layer_norm.weight\", \"encoder.encoder_layers.1.1.layer_norm.bias\", \"encoder.layer_norm.weight\", \"encoder.layer_norm.bias\", \"decoder.embedding.weight\", \"decoder.pos_encoding.pe\", \"decoder.decoder_layers.0.0.w_q.weight\", \"decoder.decoder_layers.0.0.w_q.bias\", \"decoder.decoder_layers.0.0.w_k.weight\", \"decoder.decoder_layers.0.0.w_k.bias\", \"decoder.decoder_layers.0.0.w_v.weight\", \"decoder.decoder_layers.0.0.w_v.bias\", \"decoder.decoder_layers.0.0.w_o.weight\", \"decoder.decoder_layers.0.0.w_o.bias\", \"decoder.decoder_layers.0.1.w_q.weight\", \"decoder.decoder_layers.0.1.w_q.bias\", \"decoder.decoder_layers.0.1.w_k.weight\", \"decoder.decoder_layers.0.1.w_k.bias\", \"decoder.decoder_layers.0.1.w_v.weight\", \"decoder.decoder_layers.0.1.w_v.bias\", \"decoder.decoder_layers.0.1.w_o.weight\", \"decoder.decoder_layers.0.1.w_o.bias\", \"decoder.decoder_layers.0.2.fc1.weight\", \"decoder.decoder_layers.0.2.fc1.bias\", \"decoder.decoder_layers.0.2.fc2.weight\", \"decoder.decoder_layers.0.2.fc2.bias\", \"decoder.decoder_layers.0.2.layer_norm.weight\", \"decoder.decoder_layers.0.2.layer_norm.bias\", \"decoder.decoder_layers.1.0.w_q.weight\", \"decoder.decoder_layers.1.0.w_q.bias\", \"decoder.decoder_layers.1.0.w_k.weight\", \"decoder.decoder_layers.1.0.w_k.bias\", \"decoder.decoder_layers.1.0.w_v.weight\", \"decoder.decoder_layers.1.0.w_v.bias\", \"decoder.decoder_layers.1.0.w_o.weight\", \"decoder.decoder_layers.1.0.w_o.bias\", \"decoder.decoder_layers.1.1.w_q.weight\", \"decoder.decoder_layers.1.1.w_q.bias\", \"decoder.decoder_layers.1.1.w_k.weight\", \"decoder.decoder_layers.1.1.w_k.bias\", \"decoder.decoder_layers.1.1.w_v.weight\", \"decoder.decoder_layers.1.1.w_v.bias\", \"decoder.decoder_layers.1.1.w_o.weight\", \"decoder.decoder_layers.1.1.w_o.bias\", \"decoder.decoder_layers.1.2.fc1.weight\", \"decoder.decoder_layers.1.2.fc1.bias\", \"decoder.decoder_layers.1.2.fc2.weight\", \"decoder.decoder_layers.1.2.fc2.bias\", \"decoder.decoder_layers.1.2.layer_norm.weight\", \"decoder.decoder_layers.1.2.layer_norm.bias\", \"decoder.layer_norm.weight\", \"decoder.layer_norm.bias\", \"decoder.fc_out.weight\", \"decoder.fc_out.bias\". ","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-152-f97860b88696>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the model and optimizer state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transformer_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set the model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1604\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1605\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TransformerEncoder:\n\tMissing key(s) in state_dict: \"embedding.weight\", \"pos_encoding.pe\", \"encoder_layers.0.0.w_q.weight\", \"encoder_layers.0.0.w_q.bias\", \"encoder_layers.0.0.w_k.weight\", \"encoder_layers.0.0.w_k.bias\", \"encoder_layers.0.0.w_v.weight\", \"encoder_layers.0.0.w_v.bias\", \"encoder_layers.0.0.w_o.weight\", \"encoder_layers.0.0.w_o.bias\", \"encoder_layers.0.1.fc1.weight\", \"encoder_layers.0.1.fc1.bias\", \"encoder_layers.0.1.fc2.weight\", \"encoder_layers.0.1.fc2.bias\", \"encoder_layers.0.1.layer_norm.weight\", \"encoder_layers.0.1.layer_norm.bias\", \"encoder_layers.1.0.w_q.weight\", \"encoder_layers.1.0.w_q.bias\", \"encoder_layers.1.0.w_k.weight\", \"encoder_layers.1.0.w_k.bias\", \"encoder_layers.1.0.w_v.weight\", \"encoder_layers.1.0.w_v.bias\", \"encoder_layers.1.0.w_o.weight\", \"encoder_layers.1.0.w_o.bias\", \"encoder_layers.1.1.fc1.weight\", \"encoder_layers.1.1.fc1.bias\", \"encoder_layers.1.1.fc2.weight\", \"encoder_layers.1.1.fc2.bias\", \"encoder_layers.1.1.layer_norm.weight\", \"encoder_layers.1.1.layer_norm.bias\", \"layer_norm.weight\", \"layer_norm.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.embedding.weight\", \"encoder.pos_encoding.pe\", \"encoder.encoder_layers.0.0.w_q.weight\", \"encoder.encoder_layers.0.0.w_q.bias\", \"encoder.encoder_layers.0.0.w_k.weight\", \"encoder.encoder_layers.0.0.w_k.bias\", \"encoder.encoder_layers.0.0.w_v.weight\", \"encoder.encoder_layers.0.0.w_v.bias\", \"encoder.encoder_layers.0.0.w_o.weight\", \"encoder.encoder_layers.0.0.w_o.bias\", \"encoder.encoder_layers.0.1.fc1.weight\", \"encoder.encoder_layers.0.1.fc1.bias\", \"encoder.encoder_layers.0.1.fc2.weight\", \"encoder.encoder_layers.0.1.fc2.bias\", \"encoder.encoder_layers.0.1.layer_norm.weight\", \"encoder.encoder_layers.0.1.layer_norm.bias\", \"encoder.encoder_layers.1.0.w_q.weight\", \"encoder.encoder_layers.1.0.w_q.bias\", \"encoder.encoder_layers.1.0.w_k.weight\", \"encoder.encoder_layers.1.0.w_k.bias\", \"encoder.encoder_layers.1.0.w_v.weight\", \"encoder.encoder_layers.1.0.w_v.bias\", \"encoder.encoder_layers.1.0.w_o.weight\", \"encoder.encoder_layers.1.0.w_o.bias\", \"encoder.encoder_layers.1.1.fc1.weight\", \"encoder.encoder_layers.1.1.fc1.bias\", \"encoder.encoder_layers.1.1.fc2.weight\", \"encoder.encoder_layers.1.1.fc2.bias\", \"encoder.encoder_layers.1.1.layer_norm.weight\", \"encoder.encoder_layers.1.1.layer_norm.bias\", \"encoder.layer_norm.weight\", \"encoder.layer_norm.bias\", \"decoder.embedding.weight\", \"decoder.pos_encoding.pe\", \"decoder.decoder_layers.0.0.w_q.weight\", \"decoder.decoder_layers.0.0.w_q.bias\", \"decoder.decoder_layers.0.0.w_k.weight\", \"decoder.decoder_layers.0.0...."]}]},{"cell_type":"code","source":["def generate_sequence(encoder, decoder, src, max_len=11):\n","    encoder.eval()\n","    decoder.eval()\n","\n","    src = torch.tensor(src).unsqueeze(0)  # Add batch dimension\n","    tgt = torch.zeros((1, max_len), dtype=torch.long)  # Initialize target sequence with zeros\n","    print(\"Initial Target Sequence:\", tgt)\n","\n","    with torch.no_grad():\n","        # Encode the source sequence\n","        encoder_output = encoder(src)\n","\n","        for i in range(max_len):\n","            tgt_input = tgt[:, :i+1]  # All tokens up to position i (inclusive)\n","            print(\"Target Input Shape:\", tgt_input.shape)\n","\n","            # Forward pass through the decoder\n","            output = decoder(tgt_input, encoder_output)\n","            print(\"Model Output Shape:\", output.shape)\n","\n","            # Output should be of shape (batch_size, seq_len, vocab_size)\n","            if output.dim() == 3:\n","                if output.size(1) > 0:\n","                    next_token = output[:, -1, :].argmax(dim=-1)  # Get the most likely next token\n","                    tgt[:, i] = next_token\n","                else:\n","                    print(\"Output sequence is empty.\")\n","                    break\n","            else:\n","                print(f\"Unexpected output shape: {output.shape}\")\n","                break\n","\n","            if next_token.item() == PAD:\n","                break\n","\n","    return tgt.squeeze().tolist()\n","\n","# Generate a sequence using the loaded models\n","example_src = padded_token_ids_data[8]  # Use an example source sequence\n","print(f\"Example Source Sequence: {example_src}\")\n","generated_sequence = generate_sequence(encoder, decoder, example_src)\n","print(f\"Generated Sequence: {generated_sequence}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLoMODatdOiA","executionInfo":{"status":"ok","timestamp":1725311001597,"user_tz":-330,"elapsed":469,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"34f56a58-732c-4d4f-aa34-5d9eb21b3615"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Example Source Sequence: [37, 38, 39, 40, 41, 42, 0, 0, 0, 0, 0]\n","Initial Target Sequence: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n","Target Input Shape: torch.Size([1, 1])\n","Model Output Shape: torch.Size([1, 1, 51])\n","Target Input Shape: torch.Size([1, 2])\n","Model Output Shape: torch.Size([1, 2, 51])\n","Target Input Shape: torch.Size([1, 3])\n","Model Output Shape: torch.Size([1, 3, 51])\n","Target Input Shape: torch.Size([1, 4])\n","Model Output Shape: torch.Size([1, 4, 51])\n","Target Input Shape: torch.Size([1, 5])\n","Model Output Shape: torch.Size([1, 5, 51])\n","Target Input Shape: torch.Size([1, 6])\n","Model Output Shape: torch.Size([1, 6, 51])\n","Target Input Shape: torch.Size([1, 7])\n","Model Output Shape: torch.Size([1, 7, 51])\n","Target Input Shape: torch.Size([1, 8])\n","Model Output Shape: torch.Size([1, 8, 51])\n","Target Input Shape: torch.Size([1, 9])\n","Model Output Shape: torch.Size([1, 9, 51])\n","Target Input Shape: torch.Size([1, 10])\n","Model Output Shape: torch.Size([1, 10, 51])\n","Target Input Shape: torch.Size([1, 11])\n","Model Output Shape: torch.Size([1, 11, 51])\n","Generated Sequence: [22, 22, 43, 38, 38, 38, 38, 38, 38, 38, 38]\n"]}]},{"cell_type":"code","source":["vocab = {\n","    0: '<PAD>',\n","    1: '<UNK>',\n","    2: 'I',\n","    3: 'love',\n","    4: 'machine',\n","    5: 'learning',\n","    6: '.',\n","    7: 'Artificial',\n","    8: 'intelligence',\n","    9: 'is',\n","    10: 'the',\n","    11: 'future',\n","    12: 'Transformers',\n","    13: 'are',\n","    14: 'powerful',\n","    15: 'models',\n","    16: 'Natural',\n","    17: 'language',\n","    18: 'processing',\n","    19: 'fascinating',\n","    20: 'Deep',\n","    21: 'learning',\n","    22: 'enables',\n","    23: 'great',\n","    24: 'advances',\n","    25: 'in',\n","    26: 'AI',\n","    27: 'PyTorch',\n","    28: 'popular',\n","    29: 'deep',\n","    30: 'library',\n","    31: 'GPT',\n","    32: 'models',\n","    33: 'widely',\n","    34: 'used',\n","    35: 'NLP',\n","    36: 'Neural',\n","    37: 'networks',\n","    38: 'are',\n","    39: 'the',\n","    40: 'backbone',\n","    41: 'of',\n","    42: 'Embedding',\n","    43: 'layers',\n","    44: 'convert',\n","    45: 'tokens',\n","    46: 'to',\n","    47: 'vectors',\n","    48: 'Attention',\n","    49: 'mechanisms',\n","    50: 'help',\n","    51: 'focus',\n","    52: 'on',\n","    53: 'important',\n","    54: 'parts',\n","    55: 'input'\n","}\n","\n","def tokens_to_sentence(tokens, vocab):\n","    return ' '.join(vocab.get(token, '<UNK>') for token in tokens)\n","\n","# Convert to sentence\n","\n","sentence = tokens_to_sentence(generated_sequence, vocab)\n","print(f\"Generated Sentence: {sentence}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgdY5t1RdWg3","executionInfo":{"status":"ok","timestamp":1725311001976,"user_tz":-330,"elapsed":2,"user":{"displayName":"Hrithik Hadawale","userId":"13203035845462636557"}},"outputId":"c703615c-f4e9-49b9-c8db-83967c83c9d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated Sentence: enables enables layers are are are are are are are are\n"]}]},{"cell_type":"markdown","source":["Next steps depend on your goals and current project needs. Here are some potential directions:\n","\n","Fine-Tuning and Evaluation:\n","\n","Fine-Tune: If you’re working with a specific dataset or task, fine-tune the model to improve performance.\n","Evaluate: Use metrics like BLEU score for sequence generation tasks, or other relevant metrics for your specific application.\n","Model Optimization:\n","\n","Optimize Performance: Explore techniques to optimize the model for faster inference and reduced memory usage.\n","Quantization: Implement model quantization for deployment on edge devices or for efficiency.\n","Expand the Model:\n","\n","Additional Features: Implement additional features or improvements, such as more advanced positional encodings or attention mechanisms.\n","Experimentation: Try different architectures or hyperparameters to see if performance improves.\n","Integration:\n","\n","Deployment: Integrate the model into a production environment or application. Ensure it works well with real-world data and under different conditions.\n","User Interface: Develop a user interface for interacting with the model, such as a web or mobile application.\n","Documentation and Sharing:\n","\n","Document: Create comprehensive documentation for your model, including how to use it, its limitations, and its performance.\n","Share: Share your work with the community or stakeholders. You might consider publishing a paper, creating a project repository, or presenting your work.\n","Continual Learning:\n","\n","Keep Learning: Stay updated with the latest research and techniques in the field. Implement new ideas and improvements as you learn more."],"metadata":{"id":"0v73yo5p584U"}},{"cell_type":"code","source":[],"metadata":{"id":"ghvFI0c7gL2z"},"execution_count":null,"outputs":[]}]}