# Transformer from Scratch

This Jupyter notebook (`Transformer-scratch.ipynb`) contains an implementation of the Transformer model from scratch. The Transformer architecture, introduced in the paper "Attention Is All You Need", has become a cornerstone of modern natural language processing.

## Purpose

The main objectives of this notebook are:
1. To provide a detailed, step-by-step implementation of the Transformer model.
2. To help understand the inner workings of the Transformer architecture.
3. To demonstrate how to train and evaluate a Transformer model on a specific task.

## Dependencies

To run this notebook, you'll need the following libraries:
- PyTorch
- NumPy
- Tiktoken
- Torchtext
- Transformers


You can install these dependencies using pip:


Llama-3 architecture code updated.

Working on a Mixture of Experts (MoE) Layers.

Soon I will upload the code to github.
